{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f0e2fd",
   "metadata": {},
   "source": [
    "## Notebook 2: Solana Client Setup and Wallet Analysis\n",
    "### This notebook handles all Solana blockchain interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f2170a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies and setup\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import List, Dict, Tuple, Set, Iterable, Optional, Any, Callable\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import inspect\n",
    "from dataclasses import dataclass, asdict\n",
    "from database import DatabaseManager\n",
    "import sqlite3\n",
    "import psycopg2\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from dune_client.client import DuneClient\n",
    "from dune_client.query import QueryBase\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Solana specific imports\n",
    "from dotenv import load_dotenv\n",
    "from solana.rpc.async_api import AsyncClient\n",
    "from solana.rpc.commitment import Commitment\n",
    "from solana.rpc.types import TokenAccountOpts\n",
    "from solders.pubkey import Pubkey, Pubkey as PublicKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9271c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helius API Key loaded successfully: True\n",
      "Bird eye API Key loaded successfully: True\n",
      "Dune API Key loaded successfully: True\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"HELIUS_API_KEY\")\n",
    "BASE_URL = f\"https://mainnet.helius-rpc.com/?api-key={API_KEY}\"\n",
    "birdeye_key = os.getenv(\"BIRDEYE_API_KEY\")\n",
    "DUNE_API_KEY = os.getenv(\"DUNE_API_KEY\")\n",
    "query_id = 5668844\n",
    "\n",
    "# Check if the API key is loaded successfully\n",
    "print(\"Helius API Key loaded successfully:\", API_KEY is not None)\n",
    "print(\"Bird eye API Key loaded successfully:\", birdeye_key is not None)\n",
    "print(\"Dune API Key loaded successfully:\", DUNE_API_KEY is not None)\n",
    "\n",
    "BIRDEYE_URL = \"https://public-api.birdeye.so/defi/v2/tokens/new_listing?meme_platform_enabled=true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27bbd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time range (past 14 days, excluding today)\n",
    "start_time = int((datetime.now() - relativedelta(days=14)).timestamp())\n",
    "end_time = int((datetime.now() - relativedelta(days=1)).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96eb456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolanaAlphaClient:\n",
    "    \"\"\"Async Solana client for alpha detection and wallet analysis\"\"\"\n",
    "\n",
    "    def __init__(self, rpc_url: str = BASE_URL):\n",
    "        self.rpc_url = rpc_url\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    async def make_rpc_call(self, method: str, params: List[Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Async helper to make RPC calls\"\"\"\n",
    "        payload = {\n",
    "            \"jsonrpc\": \"2.0\",\n",
    "            \"id\": \"1\",\n",
    "            \"method\": method,\n",
    "            \"params\": params\n",
    "        }\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            try:\n",
    "                async with session.post(self.rpc_url, json=payload, headers=self.headers, timeout=10) as response:\n",
    "                    response.raise_for_status()\n",
    "                    return await response.json()\n",
    "            except aiohttp.ClientError as e:\n",
    "                print(f\"❌ RPC call error: {e}\")\n",
    "                return {\"error\": str(e)}\n",
    "\n",
    "    async def test_connection(self) -> bool:\n",
    "        \"\"\"Async test for Solana RPC health\"\"\"\n",
    "        response = await self.make_rpc_call(\"getHealth\", [])\n",
    "        if response.get(\"result\") == \"ok\":\n",
    "            print(\"✅ Solana RPC connection successful\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Solana RPC connection failed: {response.get('error', 'Unknown error')}\")\n",
    "            return False\n",
    "\n",
    "    async def get_account_info(self, wallet_address: str) -> Dict[str, Any]:\n",
    "        \"\"\"Async fetch for wallet account info\"\"\"\n",
    "        response = await self.make_rpc_call(\"getAccountInfo\", [wallet_address])\n",
    "        if \"error\" in response:\n",
    "            print(f\"❌ Error getting account info for {wallet_address}: {response['error']}\")\n",
    "            return {\"balance\": 0, \"exists\": False, \"error\": response[\"error\"]}\n",
    "\n",
    "        result = response.get(\"result\", {})\n",
    "        account_info = result.get(\"value\")\n",
    "\n",
    "        if not account_info:\n",
    "            return {\"balance\": 0, \"exists\": False}\n",
    "\n",
    "        balance_sol = account_info[\"lamports\"] / 1_000_000_000\n",
    "\n",
    "        return {\n",
    "            \"balance\": balance_sol,\n",
    "            \"exists\": True,\n",
    "            \"owner\": account_info[\"owner\"],\n",
    "            \"executable\": account_info.get(\"executable\", False),\n",
    "            \"lamports\": account_info[\"lamports\"]\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccee5bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Solana client setup...\n",
      "✅ Solana RPC connection successful\n",
      "✅ Connection test successful\n",
      "📊 Test wallet info: {'balance': 0.012267414, 'exists': True, 'owner': '11111111111111111111111111111111', 'executable': False, 'lamports': 12267414}\n"
     ]
    }
   ],
   "source": [
    "async def test_solana_setup():\n",
    "    \"\"\"Test the async Solana client setup\"\"\"\n",
    "    print(\"🧪 Testing Solana client setup...\")\n",
    "\n",
    "    client = SolanaAlphaClient(BASE_URL)\n",
    "\n",
    "    connection_ok = await client.test_connection()\n",
    "\n",
    "    if not connection_ok:\n",
    "        print(\"❌ Connection test failed\")\n",
    "    else:\n",
    "        print(\"✅ Connection test successful\")\n",
    "        test_wallet = \"Fiiu1ZnaEwVcvcTxazkR14A1Va6K6VbJfoEiNVMbfTw5\"\n",
    "        account_info = await client.get_account_info(test_wallet)\n",
    "        print(f\"📊 Test wallet info: {account_info}\")\n",
    "\n",
    "# Run the test\n",
    "await test_solana_setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18334ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionAnalyzer:\n",
    "    \"\"\"Analyze Solana transactions for wallet behavior patterns\"\"\"\n",
    "\n",
    "    def __init__(self, client: SolanaAlphaClient):\n",
    "        self.client = client\n",
    "\n",
    "    async def get_wallet_transactions(self, wallet_address: str, limit: int = 100) -> List[Dict[str, Optional[any]]]:\n",
    "        \"\"\"Fetch recent successful transactions for a wallet using RPC call\"\"\"\n",
    "        try:\n",
    "            params = [\n",
    "                wallet_address,\n",
    "                {\n",
    "                    \"limit\": limit,\n",
    "                    \"commitment\": \"confirmed\"\n",
    "                }\n",
    "            ]\n",
    "            response = await self.client.make_rpc_call(\"getSignaturesForAddress\", params)\n",
    "\n",
    "            if \"error\" in response:\n",
    "                print(f\"❌ RPC error for {wallet_address}: {response['error']}\")\n",
    "                return []\n",
    "\n",
    "            signature_list = response.get(\"result\", [])\n",
    "            if not signature_list:\n",
    "                print(f\"⚠️ No transactions found for {wallet_address[:8]}\")\n",
    "                return []\n",
    "\n",
    "            return self._parse_successful_transactions(signature_list, wallet_address)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to fetch transactions for {wallet_address}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _parse_successful_transactions(self, signature_list: List[Dict], wallet_address: str) -> List[Dict]:\n",
    "        \"\"\"Extract successful transaction data\"\"\"\n",
    "        transactions = [\n",
    "            {\n",
    "                \"signature\": sig[\"signature\"],\n",
    "                \"slot\": sig[\"slot\"],\n",
    "                \"block_time\": sig.get(\"blockTime\"),\n",
    "                \"timestamp\": datetime.fromtimestamp(sig[\"blockTime\"]) if sig.get(\"blockTime\") else None\n",
    "            }\n",
    "            for sig in signature_list if sig.get(\"err\") is None\n",
    "        ]\n",
    "\n",
    "        print(f\"📥 Parsed {len(transactions)} successful transactions for {wallet_address[:8]}\")\n",
    "        return transactions\n",
    "\n",
    "    async def analyze_wallet_performance(self, wallet_address: str, days: int = 30) -> Dict[str, any]:\n",
    "        \"\"\"Analyze wallet activity over a given time window\"\"\"\n",
    "        print(f\"🔍 Starting performance analysis for {wallet_address[:8]} over {days} days...\")\n",
    "\n",
    "        transactions = await self.get_wallet_transactions(wallet_address, limit=200)\n",
    "\n",
    "        if not transactions:\n",
    "            return self._empty_analysis(wallet_address, days)\n",
    "\n",
    "        recent_transactions = self._filter_recent_transactions(transactions, days)\n",
    "\n",
    "        return self._build_analysis_report(wallet_address, transactions, recent_transactions, days)\n",
    "\n",
    "    def _filter_recent_transactions(self, transactions: List[Dict], days: int) -> List[Dict]:\n",
    "        \"\"\"Filter transactions within the specified time window\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        return [tx for tx in transactions if tx[\"timestamp\"] and tx[\"timestamp\"] > cutoff]\n",
    "\n",
    "    def _build_analysis_report(\n",
    "        self,\n",
    "        wallet_address: str,\n",
    "        all_tx: List[Dict],\n",
    "        recent_tx: List[Dict],\n",
    "        days: int\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"Generate performance metrics\"\"\"\n",
    "        return {\n",
    "            \"wallet\": wallet_address,\n",
    "            \"total_transactions\": len(all_tx),\n",
    "            \"recent_transactions\": len(recent_tx),\n",
    "            \"analysis_period_days\": days,\n",
    "            \"first_transaction\": all_tx[-1][\"timestamp\"] if all_tx else None,\n",
    "            \"last_transaction\": all_tx[0][\"timestamp\"] if all_tx else None,\n",
    "            \"activity_score\": round(len(recent_tx) / days, 2),\n",
    "            \"signatures\": [tx[\"signature\"] for tx in recent_tx[:5]]\n",
    "        }\n",
    "\n",
    "    def _empty_analysis(self, wallet_address: str, days: int) -> Dict[str, any]:\n",
    "        \"\"\"Return default analysis when no transactions are found\"\"\"\n",
    "        return {\n",
    "            \"wallet\": wallet_address,\n",
    "            \"total_transactions\": 0,\n",
    "            \"recent_transactions\": 0,\n",
    "            \"analysis_period_days\": days,\n",
    "            \"error\": \"No transactions found\"\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26e5772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Testing get_wallet_transactions...\n",
      "📥 Parsed 48 successful transactions for Fiiu1Zna\n",
      "✅ Retrieved 48 transactions\n",
      "\n",
      "📊 Testing analyze_wallet_performance...\n",
      "🔍 Starting performance analysis for Fiiu1Zna over 30 days...\n",
      "📥 Parsed 198 successful transactions for Fiiu1Zna\n",
      "✅ Performance Analysis:\n",
      "  wallet: Fiiu1ZnaEwVcvcTxazkR14A1Va6K6VbJfoEiNVMbfTw5\n",
      "  total_transactions: 198\n",
      "  recent_transactions: 2\n",
      "  analysis_period_days: 30\n",
      "  first_transaction: 2024-12-20 21:25:14\n",
      "  last_transaction: 2025-08-09 10:17:36\n",
      "  activity_score: 0.07\n",
      "  signatures: ['5JsZS6ZW3QVzmDQH8kkAeRxAjZM9ojqZZco8mVjGEPmiPJafQRbibSDvwHzKFw3TU6AKD6HgroRDdS7XdUzqrXSU', 'haATLavQqPywf5foexKZkYfAnCotDBY7Hmfc2QBcnsgDULSDrPZKcQUgwBYjjpNs2HzimfTwKHpt4qPLguagQbk']\n"
     ]
    }
   ],
   "source": [
    "TEST_WALLET = \"Fiiu1ZnaEwVcvcTxazkR14A1Va6K6VbJfoEiNVMbfTw5\"\n",
    "async def run_transaction_tests():\n",
    "    client = SolanaAlphaClient(BASE_URL)\n",
    "    analyzer = TransactionAnalyzer(client)\n",
    "\n",
    "    print(\"\\n🔧 Testing get_wallet_transactions...\")\n",
    "    transactions = await analyzer.get_wallet_transactions(TEST_WALLET, limit=50)\n",
    "    print(f\"✅ Retrieved {len(transactions)} transactions\")\n",
    "\n",
    "    print(\"\\n📊 Testing analyze_wallet_performance...\")\n",
    "    performance = await analyzer.analyze_wallet_performance(TEST_WALLET, days=30)\n",
    "    print(\"✅ Performance Analysis:\")\n",
    "    for key, value in performance.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Run the test\n",
    "await run_transaction_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b045aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WalletPerformanceTracker:\n",
    "    \"\"\"Track and score wallet performance for alpha detection.\n",
    "\n",
    "    Notes:\n",
    "      - `analyzer` is expected to expose `analyze_wallet_performance(wallet_address)` which may be async or sync.\n",
    "      - `token_discovery` is expected to expose:\n",
    "           - async get_token_holders(token_address, ...) -> List[dict] where each dict has keys:\n",
    "             'wallet' (str), 'balance_raw' (int), optionally 'balance' (float), 'balance_formatted' (str)\n",
    "           - async analyze_token_distribution(token_address, ...) -> dict (optionally contains 'supply_raw' and 'decimals')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        analyzer: Any,\n",
    "        token_discovery: Any,\n",
    "        *,\n",
    "        concurrency: int = 10,\n",
    "        min_score: float = 70.0,\n",
    "        top_k_holders_per_token: Optional[int] = None,\n",
    "        min_balance_raw: int = 0,\n",
    "        min_holding_percentage: float = 0.0,\n",
    "        score_weights: Optional[Dict[str, float]] = None,\n",
    "        analyzer_timeout: float = 30.0,\n",
    "    ):\n",
    "        self.analyzer = analyzer\n",
    "        self.token_discovery = token_discovery\n",
    "        self.concurrency = max(1, int(concurrency))\n",
    "        self.min_score_default = float(min_score)\n",
    "        self.top_k_holders_per_token = top_k_holders_per_token\n",
    "        self.min_balance_raw = int(min_balance_raw)\n",
    "        self.min_holding_percentage = float(min_holding_percentage)\n",
    "        self.score_weights = score_weights or {\n",
    "            \"activity\": 30.0,\n",
    "            \"volume\": 40.0,\n",
    "            \"recency\": 30.0,\n",
    "        }\n",
    "        self.analyzer_timeout = float(analyzer_timeout)\n",
    "        self._wallet_analysis_cache: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    def calculate_wallet_score(self, wallet_analysis: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate a performance score for a wallet (0-100) using configurable weights.\"\"\"\n",
    "        if not wallet_analysis or wallet_analysis.get(\"error\"):\n",
    "            return 0.0\n",
    "\n",
    "        activity = float(wallet_analysis.get(\"activity_score\", 0.0))\n",
    "        total_tx = float(wallet_analysis.get(\"total_transactions\", 0))\n",
    "        recent_tx = int(wallet_analysis.get(\"recent_transactions\", 0))\n",
    "\n",
    "        activity_score = min(activity * self.score_weights[\"activity\"], self.score_weights[\"activity\"])\n",
    "        volume_score = min((total_tx / 10.0), self.score_weights[\"volume\"])\n",
    "        recency_score = self.score_weights[\"recency\"] if recent_tx > 0 else 0.0\n",
    "\n",
    "        total = activity_score + volume_score + recency_score\n",
    "        total = min(total, sum(self.score_weights.values()))\n",
    "        weight_sum = sum(self.score_weights.values())\n",
    "        score_0_100 = (total / weight_sum) * 100.0 if weight_sum > 0 else 0.0\n",
    "        return float(min(max(score_0_100, 0.0), 100.0))\n",
    "\n",
    "    async def _maybe_async_call(self, func: Callable, *args, **kwargs):\n",
    "        \"\"\"Call func; handle both async and sync functions. Return its result.\"\"\"\n",
    "        if inspect.iscoroutinefunction(func):\n",
    "            return await func(*args, **kwargs)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, lambda: func(*args, **kwargs))\n",
    "\n",
    "    async def _analyze_wallet_with_timeout(self, wallet_address: str) -> Dict[str, Any]:\n",
    "        \"\"\"Run analyzer.analyze_wallet_performance with timeout, using cache if available.\"\"\"\n",
    "        if wallet_address in self._wallet_analysis_cache:\n",
    "            return self._wallet_analysis_cache[wallet_address]\n",
    "\n",
    "        try:\n",
    "            coro = self._maybe_async_call(self.analyzer.analyze_wallet_performance, wallet_address)\n",
    "            result = await asyncio.wait_for(coro, timeout=self.analyzer_timeout)\n",
    "            self._wallet_analysis_cache[wallet_address] = result if isinstance(result, dict) else {\"result\": result}\n",
    "            return self._wallet_analysis_cache[wallet_address]\n",
    "        except asyncio.TimeoutError:\n",
    "            logger.warning(\"Analyzer timed out for wallet %s\", wallet_address)\n",
    "            result = {\"error\": \"analyzer_timeout\"}\n",
    "            self._wallet_analysis_cache[wallet_address] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Analyzer exception for wallet %s: %s\", wallet_address, e)\n",
    "            result = {\"error\": f\"analyzer_exception: {e}\"}\n",
    "            self._wallet_analysis_cache[wallet_address] = result\n",
    "            return result\n",
    "\n",
    "    async def discover_successful_wallets(\n",
    "        self,\n",
    "        token_list: List[str],\n",
    "        *,\n",
    "        min_score: Optional[float] = None,\n",
    "        concurrency: Optional[int] = None,\n",
    "        fetch_supply_for_percentage: bool = True,\n",
    "        holders_limit_per_token: Optional[int] = None,\n",
    "        skip_on_error_tokens: bool = True,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        min_score = float(min_score if min_score is not None else self.min_score_default)\n",
    "        concurrency = int(concurrency or self.concurrency)\n",
    "        holders_limit_per_token = (\n",
    "            holders_limit_per_token if holders_limit_per_token is not None else self.top_k_holders_per_token\n",
    "        )\n",
    "\n",
    "        sem = asyncio.Semaphore(concurrency)\n",
    "        discovered: List[Dict[str, Any]] = []\n",
    "\n",
    "        async def _analyze_single_holder(holder: Dict[str, Any], token_addr: str):\n",
    "            async with sem:\n",
    "                wallet_address = holder.get(\"wallet\") or holder.get(\"owner\") or holder.get(\"address\")\n",
    "                if not wallet_address:\n",
    "                    return None\n",
    "\n",
    "                try:\n",
    "                    raw = int(holder.get(\"balance_raw\", holder.get(\"balance\", 0) or 0))\n",
    "                except Exception:\n",
    "                    raw = 0\n",
    "                if raw < self.min_balance_raw:\n",
    "                    return None\n",
    "\n",
    "                pct = holder.get(\"percentage\")\n",
    "                if pct is not None and pct < self.min_holding_percentage:\n",
    "                    return None\n",
    "\n",
    "                analysis = await self._analyze_wallet_with_timeout(wallet_address)\n",
    "                score = self.calculate_wallet_score(analysis)\n",
    "\n",
    "                if score >= min_score:\n",
    "                    return {\n",
    "                        \"address\": wallet_address,\n",
    "                        \"score\": float(score),\n",
    "                        \"analysis\": analysis,\n",
    "                        \"discovered_via_token\": token_addr,\n",
    "                        \"holding_percentage\": float(pct) if pct is not None else None,\n",
    "                        \"balance_raw\": int(raw),\n",
    "                        \"balance\": holder.get(\"balance\"),\n",
    "                    }\n",
    "                return None\n",
    "\n",
    "        for token_addr in token_list:\n",
    "            logger.info(\"Starting token analysis for %s\", token_addr)\n",
    "            supply_raw = None\n",
    "            dist = None\n",
    "            try:\n",
    "                if fetch_supply_for_percentage and hasattr(self.token_discovery, \"analyze_token_distribution\"):\n",
    "                    dist = await self.token_discovery.analyze_token_distribution(token_addr, limit=1000, top_n=1)\n",
    "                    if dist and dist.get(\"found_on_chain\") and \"supply_raw\" in dist:\n",
    "                        supply_raw = int(dist.get(\"supply_raw\", 0))\n",
    "            except Exception:\n",
    "                logger.exception(\"Error fetching supply via analyze_token_distribution for %s\", token_addr)\n",
    "                if skip_on_error_tokens:\n",
    "                    continue\n",
    "\n",
    "            holders = None\n",
    "            try:\n",
    "                if holders_limit_per_token is not None and hasattr(self.token_discovery, \"get_token_holders\"):\n",
    "                    holders = await self.token_discovery.get_token_holders(\n",
    "                        token_addr,\n",
    "                        limit=1000,\n",
    "                        max_pages=None,\n",
    "                        decimals=None,\n",
    "                    )\n",
    "                    if holders_limit_per_token:\n",
    "                        holders = holders[: holders_limit_per_token]\n",
    "                else:\n",
    "                    if dist:\n",
    "                        holders = dist.get(\"holders_top_n\") or []\n",
    "                    else:\n",
    "                        holders = await self.token_discovery.get_token_holders(token_addr)\n",
    "            except Exception:\n",
    "                logger.exception(\"Error fetching holders for %s\", token_addr)\n",
    "                if skip_on_error_tokens:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "            if supply_raw and holders:\n",
    "                for h in holders:\n",
    "                    try:\n",
    "                        raw = int(h.get(\"balance_raw\", h.get(\"balance\", 0) or 0))\n",
    "                        h.setdefault(\"percentage\", (raw / supply_raw * 100.0) if supply_raw > 0 else 0.0)\n",
    "                    except Exception:\n",
    "                        h.setdefault(\"percentage\", None)\n",
    "\n",
    "            tasks = [asyncio.create_task(_analyze_single_holder(h, token_addr)) for h in holders]\n",
    "            if tasks:\n",
    "                results = await asyncio.gather(*tasks, return_exceptions=False)\n",
    "                for r in results:\n",
    "                    if r:\n",
    "                        discovered.append(r)\n",
    "\n",
    "        unique: Dict[str, Dict[str, Any]] = {}\n",
    "        for w in discovered:\n",
    "            addr = w[\"address\"]\n",
    "            if addr not in unique or w[\"score\"] > unique[addr][\"score\"]:\n",
    "                unique[addr] = w\n",
    "\n",
    "        sorted_wallets = sorted(unique.values(), key=lambda x: x[\"score\"], reverse=True)\n",
    "        logger.info(\"Discovered %d wallets with score >= %s\", len(sorted_wallets), min_score)\n",
    "        return sorted_wallets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23af0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HolderAggregator:\n",
    "    \"\"\"Robust holder aggregation service for Solana tokens.\"\"\"\n",
    "\n",
    "    def __init__(self, client: \"SolanaAlphaClient\"):\n",
    "        self.client = client\n",
    "\n",
    "    async def get_token_holders(\n",
    "        self,\n",
    "        token_mint: str,\n",
    "        *,\n",
    "        sleep_between: float = 0.15,\n",
    "        limit: int = 1000,\n",
    "        max_pages: Optional[int] = None,\n",
    "        decimals: Optional[int] = None,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Fetch token holders from Helius getTokenAccounts with pagination.\"\"\"\n",
    "        page = 1\n",
    "        owner_balances = defaultdict(int)\n",
    "        owner_token_account_counts = defaultdict(int)\n",
    "\n",
    "        while True:\n",
    "            payload_params = {\n",
    "                \"mint\": token_mint,\n",
    "                \"page\": page,\n",
    "                \"limit\": limit,\n",
    "                \"displayOptions\": {},\n",
    "            }\n",
    "\n",
    "            data = await self.client.make_rpc_call(\"getTokenAccounts\", payload_params)\n",
    "            token_accounts = data.get(\"result\", {}).get(\"token_accounts\", [])\n",
    "\n",
    "            if not token_accounts:\n",
    "                break\n",
    "\n",
    "            for ta in token_accounts:\n",
    "                owner = ta.get(\"owner\") or ta.get(\"address\")\n",
    "                amt_raw = ta.get(\"amount\", 0)\n",
    "\n",
    "                # Handle nested account shapes\n",
    "                if \"account\" in ta and isinstance(ta[\"account\"], dict):\n",
    "                    acct = ta[\"account\"]\n",
    "                    owner = owner or acct.get(\"owner\")\n",
    "                    amt_raw = acct.get(\"amount\", 0)\n",
    "\n",
    "                # Normalize amount\n",
    "                if isinstance(amt_raw, dict):\n",
    "                    amt_raw = int(float(amt_raw.get(\"amount\") or amt_raw.get(\"uiAmount\", 0)))\n",
    "                else:\n",
    "                    try:\n",
    "                        amt_raw = int(amt_raw)\n",
    "                    except Exception:\n",
    "                        amt_raw = int(float(amt_raw)) if amt_raw else 0\n",
    "\n",
    "                if owner:\n",
    "                    owner_balances[owner] += amt_raw\n",
    "                    owner_token_account_counts[owner] += 1\n",
    "\n",
    "            page += 1\n",
    "            if max_pages and page > max_pages:\n",
    "                break\n",
    "            await asyncio.sleep(sleep_between)\n",
    "\n",
    "        # Build holders list\n",
    "        holders = []\n",
    "        for owner, raw in owner_balances.items():\n",
    "            human_balance = raw / (10 ** decimals) if decimals else None\n",
    "            holders.append({\n",
    "                \"wallet\": owner,\n",
    "                \"balance_raw\": raw,\n",
    "                \"balance\": human_balance,\n",
    "                \"balance_formatted\": f\"{human_balance:,.{decimals}f}\" if human_balance is not None else str(raw),\n",
    "                \"num_token_accounts\": owner_token_account_counts[owner],\n",
    "            })\n",
    "\n",
    "        holders.sort(key=lambda x: x[\"balance_raw\"], reverse=True)\n",
    "        return holders\n",
    "\n",
    "    def analyze_holders(self, holders: List[Dict[str, Any]], top_n_for_concentration: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Return basic holder analytics:\n",
    "          - total_holders\n",
    "          - total_balance_raw\n",
    "          - concentration_metrics: top_n sum/raw & percentage, gini_like\n",
    "        This method is defensive and always fills the same keys for callers.\n",
    "        \"\"\"\n",
    "        out: Dict[str, Any] = {\"total_holders\": 0, \"total_balance_raw\": 0, \"concentration_metrics\": {}}\n",
    "        if not holders:\n",
    "            return out\n",
    "\n",
    "        total = sum(int(h.get(\"balance_raw\", 0)) for h in holders)\n",
    "        out[\"total_holders\"] = len(holders)\n",
    "        out[\"total_balance_raw\"] = int(total)\n",
    "\n",
    "        top_n = holders[:top_n_for_concentration]\n",
    "        top_sum = sum(int(h.get(\"balance_raw\", 0)) for h in top_n)\n",
    "        top_pct = (top_sum / total * 100.0) if total > 0 else 0.0\n",
    "\n",
    "        # Put both specific and convenience keys to match older tests/consumers\n",
    "        out[\"concentration_metrics\"][f\"top_{top_n_for_concentration}_sum_raw\"] = int(top_sum)\n",
    "        out[\"concentration_metrics\"][f\"top_{top_n_for_concentration}_percentage\"] = float(top_pct)\n",
    "        # convenience alias for top_10_percentage (backwards compat) when top_n == 10, otherwise keep a top_10 entry too\n",
    "        if top_n_for_concentration == 10:\n",
    "            out[\"concentration_metrics\"][\"top_10_percentage\"] = float(top_pct)\n",
    "        else:\n",
    "            # compute top 10 percentage as well if possible\n",
    "            top_10 = holders[:10]\n",
    "            top_10_sum = sum(int(h.get(\"balance_raw\", 0)) for h in top_10)\n",
    "            out[\"concentration_metrics\"][\"top_10_percentage\"] = float((top_10_sum / total * 100.0) if total > 0 else 0.0)\n",
    "\n",
    "        # gini-like measure (not normalized to classic Gini, but a useful inequality indicator)\n",
    "        vals = sorted([int(h.get(\"balance_raw\", 0)) for h in holders])\n",
    "        n = len(vals)\n",
    "        if n > 1 and total > 0:\n",
    "            cum = 0\n",
    "            for i, v in enumerate(vals, start=1):\n",
    "                cum += (2 * i - n - 1) * v\n",
    "            gini = cum / (n * total)\n",
    "            out[\"concentration_metrics\"][\"gini_like\"] = float(gini)\n",
    "        else:\n",
    "            out[\"concentration_metrics\"][\"gini_like\"] = 0.0\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aedfbaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holders: 13\n",
      "analysis: {'total_holders': 13, 'total_balance_raw': 84469108852, 'concentration_metrics': {'top_10_sum_raw': 84245728991, 'top_10_percentage': 99.73554845784938, 'gini_like': 0.7356593936164546}}\n"
     ]
    }
   ],
   "source": [
    "client = SolanaAlphaClient(BASE_URL)   \n",
    "holder_agg = HolderAggregator(client)\n",
    "summary = await holder_agg.get_token_holders(\"BusxEFRTayALb5nYBdXdy1iZGq9GgoqLMpRVGQB3FeYt\", max_pages=2)\n",
    "print(\"holders:\", len(summary))\n",
    "analysis = holder_agg.analyze_holders(summary, top_n_for_concentration=10)\n",
    "print(\"analysis:\", analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f9a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TradingStart:\n",
    "    mint: Optional[str]\n",
    "    block_time: Optional[int]\n",
    "    program_id: Optional[str]\n",
    "    detected_via: Optional[str] = None\n",
    "    extra: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class TokenDiscovery:\n",
    "    \"\"\"\n",
    "    TokenDiscovery fetches new token launches from:\n",
    "      - Birdeye (liquidity added today)\n",
    "      - Dune (tokens whose first trade was yesterday)\n",
    "\n",
    "    This version includes robust Dune caching.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: Optional[Any] = None,\n",
    "        *,\n",
    "        birdeye_api_key: Optional[str] = None,\n",
    "        dune_api_key: Optional[str] = None,\n",
    "        dune_query_id: Optional[int] = None,\n",
    "        dune_cache_file: str = \"./data/dune_recent.pkl\",\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.debug = bool(debug)\n",
    "\n",
    "        # Birdeye setup (unchanged)\n",
    "        self.birdeye_key = birdeye_api_key or globals().get(\"birdeye_key\")\n",
    "        self.birdeye_url = globals().get(\"BIRDEYE_URL\")\n",
    "\n",
    "        # Dune setup\n",
    "        self.dune_api_key = dune_api_key or os.getenv(\"DUNE_API_KEY\")\n",
    "        self.dune_query_id = dune_query_id\n",
    "        self.dune_client = DuneClient(self.dune_api_key) if self.dune_api_key else None\n",
    "\n",
    "        # Dune cache path\n",
    "        self.dune_cache_file = dune_cache_file\n",
    "\n",
    "        if self.debug:\n",
    "            if not self.birdeye_key:\n",
    "                print(\"⚠️ BIRDEYE_API_KEY not set\")\n",
    "            if not self.dune_api_key:\n",
    "                print(\"⚠️ DUNE_API_KEY not set\")\n",
    "            print(f\"TokenDiscovery initialized. Dune cache: {self.dune_cache_file}\")\n",
    "\n",
    "    # ---------- Small utility to unwrap dune client responses ----------\n",
    "    def _rows_from_dune_payload(self, payload: Any) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract a list-of-dicts 'rows' from dune_client.get_latest_result return value.\n",
    "        Handles ResultsResponse-like objects, dicts, or direct lists.\n",
    "        \"\"\"\n",
    "        # If it's None -> empty\n",
    "        if payload is None:\n",
    "            return []\n",
    "\n",
    "        # If it has attribute 'result' that contains 'rows' (ResultsResponse-like)\n",
    "        if hasattr(payload, \"result\"):\n",
    "            try:\n",
    "                result_obj = getattr(payload, \"result\")\n",
    "                # result_obj may be a dict-like or object; try both\n",
    "                if isinstance(result_obj, dict):\n",
    "                    rows = result_obj.get(\"rows\", [])\n",
    "                else:\n",
    "                    # object with attribute rows\n",
    "                    rows = getattr(result_obj, \"rows\", [])\n",
    "                if isinstance(rows, list):\n",
    "                    return rows\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # If payload is a dict\n",
    "        if isinstance(payload, dict):\n",
    "            # common shapes: {\"result\": {\"rows\": [...]}} or {\"rows\": [...]}\n",
    "            rows = payload.get(\"result\", {}).get(\"rows\") if payload.get(\"result\") else None\n",
    "            if rows is None:\n",
    "                rows = payload.get(\"rows\", None)\n",
    "            if isinstance(rows, list):\n",
    "                return rows\n",
    "            # Some payloads directly carry list\n",
    "            if isinstance(payload.get(\"data\", None), list):\n",
    "                return payload[\"data\"]\n",
    "\n",
    "        # If payload itself is a list of rows\n",
    "        if isinstance(payload, list):\n",
    "            return payload\n",
    "\n",
    "        # Fallback: try to get attribute 'rows' directly\n",
    "        if hasattr(payload, \"rows\"):\n",
    "            rows = getattr(payload, \"rows\")\n",
    "            if isinstance(rows, list):\n",
    "                return rows\n",
    "\n",
    "        # nothing matched\n",
    "        return []\n",
    "\n",
    "    # ---------- Dune: fetch latest rows (no caching) ----------\n",
    "    def fetch_dune_latest_rows(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fetch latest result for the configured query_id from Dune and return rows list.\n",
    "        Does not touch cache.\n",
    "        \"\"\"\n",
    "        if not self.dune_client or not self.dune_query_id:\n",
    "            raise RuntimeError(\"Dune client or query_id not configured.\")\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[Dune] fetching latest result for query {self.dune_query_id}...\")\n",
    "\n",
    "        payload = self.dune_client.get_latest_result(self.dune_query_id)\n",
    "        rows = self._rows_from_dune_payload(payload)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[Dune] extracted {len(rows)} rows from payload\")\n",
    "\n",
    "        return rows\n",
    "\n",
    "    # ---------- Dune: cached access, returns List[TradingStart] ----------\n",
    "    def get_tokens_launched_yesterday_cached(\n",
    "        self,\n",
    "        cache_max_age_days: int = 7\n",
    "    ) -> List[TradingStart]:\n",
    "        \"\"\"\n",
    "        Return tokens whose first trade was yesterday using cached Dune results.\n",
    "        Cache is stored at self.dune_cache_file. If cache doesn't contain yesterday's data,\n",
    "        fetch fresh from Dune and update cache.\n",
    "\n",
    "        Returns a list of TradingStart objects (so it can be concatenated with Birdeye results).\n",
    "        \"\"\"\n",
    "        cache_path = self.dune_cache_file\n",
    "\n",
    "        # helper to convert rows -> list[TradingStart] filtered to yesterday\n",
    "        def rows_to_trading_starts(rows: List[Dict[str, Any]]) -> List[TradingStart]:\n",
    "            if not rows:\n",
    "                return []\n",
    "            df = pd.DataFrame(rows)\n",
    "            # accept both column names from you earlier variants\n",
    "            candidate_date_col = None\n",
    "            for col in (\"first_buy_date\", \"first_buy_date_utc\", \"block_date\"):\n",
    "                if col in df.columns:\n",
    "                    candidate_date_col = col\n",
    "                    break\n",
    "            # minted column\n",
    "            candidate_mint_col = None\n",
    "            for col in (\"mint_address\", \"mint\", \"token_bought_mint_address\"):\n",
    "                if col in df.columns:\n",
    "                    candidate_mint_col = col\n",
    "                    break\n",
    "\n",
    "            if candidate_date_col is None or candidate_mint_col is None:\n",
    "                # if columns not found, return empty list\n",
    "                if self.debug:\n",
    "                    print(\"[Dune] expected columns not present in rows -> returning []\")\n",
    "                return []\n",
    "\n",
    "            df[candidate_date_col] = pd.to_datetime(df[candidate_date_col], errors=\"coerce\")\n",
    "            yesterday = (datetime.now(timezone.utc).date() - timedelta(days=1))\n",
    "            filtered = df[df[candidate_date_col].dt.date == yesterday]\n",
    "\n",
    "            out: List[TradingStart] = []\n",
    "            for _, row in filtered.iterrows():\n",
    "                ts = None\n",
    "                try:\n",
    "                    # convert to UTC timestamp\n",
    "                    dt = pd.to_datetime(row[candidate_date_col])\n",
    "                    if pd.isna(dt):\n",
    "                        continue\n",
    "                    # ensure tz-aware UTC\n",
    "                    if dt.tzinfo is None:\n",
    "                        dt = dt.tz_localize(\"UTC\")\n",
    "                    dt_utc = dt.tz_convert(\"UTC\")\n",
    "                    ts = int(dt_utc.timestamp())\n",
    "                except Exception:\n",
    "                    # fallback: skip row\n",
    "                    continue\n",
    "\n",
    "                out.append(\n",
    "                    TradingStart(\n",
    "                        mint=row[candidate_mint_col],\n",
    "                        block_time=ts,\n",
    "                        program_id=\"dune\",\n",
    "                        detected_via=\"dune\",\n",
    "                        extra={candidate_date_col: str(row[candidate_date_col])}\n",
    "                    )\n",
    "                )\n",
    "            return out\n",
    "\n",
    "        # 1) Try cache\n",
    "        if os.path.exists(cache_path):\n",
    "            try:\n",
    "                cache_obj = joblib.load(cache_path)\n",
    "                # cache_obj expected shape: {\"rows\": [...], \"fetched_at\": \"ISO\"}\n",
    "                if isinstance(cache_obj, dict) and \"rows\" in cache_obj and \"fetched_at\" in cache_obj:\n",
    "                    fetched_at = None\n",
    "                    try:\n",
    "                        fetched_at = datetime.fromisoformat(cache_obj[\"fetched_at\"])\n",
    "                        if fetched_at.tzinfo is None:\n",
    "                            fetched_at = fetched_at.replace(tzinfo=timezone.utc)\n",
    "                    except Exception:\n",
    "                        fetched_at = None\n",
    "\n",
    "                    rows = cache_obj.get(\"rows\", [])\n",
    "                    starts = rows_to_trading_starts(rows)\n",
    "\n",
    "                    # If cache already contains yesterday's rows, return them immediately\n",
    "                    if starts:\n",
    "                        if self.debug:\n",
    "                            print(f\"[Dune/cache] using cached data fetched_at={cache_obj.get('fetched_at')}, found {len(starts)} yesterday tokens\")\n",
    "                        return starts\n",
    "\n",
    "                    # If cache is recent (within cache_max_age_days) but doesn't have yesterday tokens,\n",
    "                    # treat as stale and continue to fetch fresh below\n",
    "                    if fetched_at:\n",
    "                        age_days = (datetime.now(timezone.utc) - fetched_at).days\n",
    "                        if age_days <= cache_max_age_days:\n",
    "                            if self.debug:\n",
    "                                print(f\"[Dune/cache] cached but no yesterday rows; cache age {age_days}d <= {cache_max_age_days}d -> will fetch fresh\")\n",
    "                        else:\n",
    "                            if self.debug:\n",
    "                                print(f\"[Dune/cache] cache older than {cache_max_age_days} days (age {age_days}d) -> fetching fresh\")\n",
    "                else:\n",
    "                    if self.debug:\n",
    "                        print(\"[Dune/cache] cache file format not recognized, ignoring\")\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"[Dune/cache] error reading cache file: {e} -- will fetch fresh\")\n",
    "\n",
    "        # 2) Fetch fresh rows from Dune\n",
    "        rows = []\n",
    "        try:\n",
    "            rows = self.fetch_dune_latest_rows()\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"[Dune] error fetching latest rows: {e}\")\n",
    "            # If fetch failed but cache existed above we already tried to return any useful cached starts.\n",
    "            return []\n",
    "\n",
    "        # Save cache (rows + timestamp ISO)\n",
    "        try:\n",
    "            joblib.dump({\"rows\": rows, \"fetched_at\": datetime.now(timezone.utc).isoformat()}, cache_path)\n",
    "            if self.debug:\n",
    "                print(f\"[Dune/cache] wrote {len(rows)} rows to cache {cache_path}\")\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"[Dune/cache] failed to write cache: {e}\")\n",
    "\n",
    "        # Convert to TradingStart objects filtered to yesterday\n",
    "        starts = rows_to_trading_starts(rows)\n",
    "        if self.debug:\n",
    "            print(f\"[Dune] found {len(starts)} yesterday tokens after fresh fetch\")\n",
    "        return starts\n",
    "\n",
    "    # ---------- Existing Birdeye methods (unchanged) ----------\n",
    "    async def _fetch_birdeye_items(self, limit: int = 200, timeout: int = 15) -> List[Dict[str, Any]]:\n",
    "        if not self.birdeye_key or not self.birdeye_url:\n",
    "            raise RuntimeError(\"Birdeye API key or URL not set.\")\n",
    "        url = f\"{self.birdeye_url}&limit={int(limit)}\" if \"?\" in self.birdeye_url else f\"{self.birdeye_url}?limit={int(limit)}\"\n",
    "        headers = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"x-chain\": \"solana\",\n",
    "            \"X-API-KEY\": self.birdeye_key,\n",
    "        }\n",
    "        async with aiohttp.ClientSession() as sess:\n",
    "            async with sess.get(url, headers=headers, timeout=timeout) as resp:\n",
    "                resp.raise_for_status()\n",
    "                data = await resp.json()\n",
    "        items = []\n",
    "        if isinstance(data, dict):\n",
    "            if \"data\" in data:\n",
    "                d = data[\"data\"]\n",
    "                if isinstance(d, dict) and \"items\" in d:\n",
    "                    items = d[\"items\"] or []\n",
    "                elif isinstance(d, list):\n",
    "                    items = d\n",
    "            elif \"items\" in data:\n",
    "                items = data[\"items\"] or []\n",
    "        elif isinstance(data, list):\n",
    "            items = data\n",
    "        return [it for it in items if isinstance(it, dict)]\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_liquidity_added_at(val: Any) -> Optional[int]:\n",
    "        if not val:\n",
    "            return None\n",
    "        if isinstance(val, (int, float)):\n",
    "            return int(val)\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(str(val))\n",
    "            if dt.tzinfo is None:\n",
    "                dt = dt.replace(tzinfo=timezone.utc)\n",
    "            return int(dt.astimezone(timezone.utc).timestamp())\n",
    "        except Exception:\n",
    "            try:\n",
    "                s = str(val).rstrip(\"Z\")\n",
    "                dt = datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n",
    "                dt = dt.replace(tzinfo=timezone.utc)\n",
    "                return int(dt.timestamp())\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _utc_day_bounds_for_date(dt: Optional[datetime] = None) -> Tuple[int, int]:\n",
    "        d = (dt or datetime.now(timezone.utc)).astimezone(timezone.utc)\n",
    "        start = datetime(d.year, d.month, d.day, 0, 0, 0, tzinfo=timezone.utc)\n",
    "        end = start + timedelta(days=1) - timedelta(seconds=1)\n",
    "        return int(start.timestamp()), int(end.timestamp())\n",
    "\n",
    "    async def get_tokens_with_liquidity_today(self, limit: int = 500) -> List[TradingStart]:\n",
    "        items = await self._fetch_birdeye_items(limit=limit)\n",
    "        start_ts, end_ts = self._utc_day_bounds_for_date(datetime.now(timezone.utc))\n",
    "        out: List[TradingStart] = []\n",
    "        for it in items:\n",
    "            la = it.get(\"liquidityAddedAt\") or it.get(\"liquidity_added_at\") or it.get(\"first_listed_at\")\n",
    "            ts = self._parse_liquidity_added_at(la)\n",
    "            if ts and start_ts <= ts <= end_ts:\n",
    "                out.append(\n",
    "                    TradingStart(\n",
    "                        mint=it.get(\"address\"),\n",
    "                        block_time=ts,\n",
    "                        program_id=\"birdeye\",\n",
    "                        detected_via=\"birdeye\",\n",
    "                        extra={\n",
    "                            \"symbol\": it.get(\"symbol\"),\n",
    "                            \"name\": it.get(\"name\"),\n",
    "                            \"decimals\": it.get(\"decimals\"),\n",
    "                            \"liquidity\": it.get(\"liquidity\"),\n",
    "                            \"logoURI\": it.get(\"logoURI\"),\n",
    "                            \"source\": it.get(\"source\"),\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2e2a5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenDiscovery initialized. Dune cache: ./data/dune_recent.pkl\n",
      "today: 20\n",
      "7ieoyr4sajqqJ7GmmuobyibnFgqgnWQdL5zheoVGHFB1 NVIDIA🔥 2025-08-26T20:35:19+00:00 82767.09169172078\n",
      "mQR6PW18n7yuHkHCB5258wdXuHnG1e1WEY2UWMxpump j*b 2025-08-26T20:35:18+00:00 6263.927567960085\n",
      "Fpkyd1UnUtGFEzPBf1C3QPzUtrn13HuSCGzREJtLpump money 2025-08-26T20:35:13+00:00 6311.124115433562\n",
      "4Wbbi8NN3ACFsKswkLWVkAes4BnFtq2NnqnXzH7JELLY JCAT 2025-08-26T20:35:12+00:00 0\n",
      "EGWnXZYCkhZbFHzLNbdBZGdeFDi89wZzSMuftfuCpump pablo 2025-08-26T20:35:06+00:00 6166.976176913179\n",
      "EtZGpKic1yePwPjm1Y3hmeDfX4JMV8m43TFsfBYNpump yah 2025-08-26T20:35:05+00:00 5507.77337300237\n",
      "BGeYeaDWUW8ApTnV1W5afBfeXnNQ2wuwCdQNGeK4pump JAWBREAKER 2025-08-26T20:35:04+00:00 5920.800655628733\n",
      "DRenw8frnHhsh68yQe4Bo5mUjZDVzFZkSKKDNnKLHYgr thing 2025-08-26T20:35:02+00:00 97967.16977615582\n",
      "AqL6UNjLUJKw23EiK4dBztXoTq9uWFTGJmHL2nz4qJSy TRUMP 2025-08-26T20:34:59+00:00 236.14911217644325\n",
      "GnvHgNbeFuhFGTCVz2f5Hqzkq8sMprsfsWXHFo8YjmLt NVIDIA🔥 2025-08-26T20:34:58+00:00 393.52704799547183\n",
      "5kiDyJD2SW1UQWA611PT52qviJi3MDMQaeFhFp1Udaos POON 2025-08-26T20:34:56+00:00 6121.899629891653\n",
      "8E54H17ERZgXfjEa4f75EAirjUNtgNVJX2RPFFScF7Qe Snowyfrog 2025-08-26T20:34:56+00:00 5626.1841821700755\n",
      "6DwFEF2fPAyLfe2qiiev54RchoorSQaftDahUzrLutwb wifjob 2025-08-26T20:34:54+00:00 6554.833272899318\n",
      "99Xibsx29c28dpbW3o9RZKrAKWF47Eacxk5Xfhigpump Hachiko 2025-08-26T20:34:52+00:00 6048.28039315723\n",
      "Ame2AgQT6t8UKCEznsPQ2jERLZ151cpyQ4Kc7ahvg3Pk Sony 2025-08-26T20:34:46+00:00 3936.731873932421\n",
      "eNiCPKBxUjj1hTA8yt6yS4zwrx59YSmQXGtWqqwdaos gi16z 2025-08-26T20:34:43+00:00 6422.228082121732\n",
      "82rih8gy9Wcu6SKzASZBgvW2bPUbBpQD8P1tAm8Ypump QUANT 2025-08-26T20:34:30+00:00 9093.22755668477\n",
      "75YHSHNrHbC2Cs5b5XTBenB9FWMA3E6YPHTFYxSopump BLACKKK 2025-08-26T20:34:29+00:00 7553.245059673837\n",
      "HLt7kyt7s7x1A76ZALJuhDPpUCLeHiLwVK5M8rdnpump PSP 2025-08-26T20:34:29+00:00 7461.665056108296\n",
      "3A92y9i9ypKmhnVn2FDmC1WLSXeAb14ep8QmbG6ypump SIRE 2025-08-26T20:34:28+00:00 6607.815436617969\n"
     ]
    }
   ],
   "source": [
    "# Instantiate\n",
    "td = TokenDiscovery(debug=True) \n",
    "\n",
    "# tokens with liquidity added today (UTC)\n",
    "today_tokens = await td.get_tokens_with_liquidity_today(limit=20)\n",
    "print(\"today:\", len(today_tokens))\n",
    "for t in today_tokens[:20]:\n",
    "    print(t.mint, t.extra.get(\"symbol\"), datetime.fromtimestamp(t.block_time, timezone.utc).isoformat(), t.extra.get(\"liquidity\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd224214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobLibTokenUpdater:\n",
    "    \"\"\"Token data storage using joblib for persistence.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = \"./data/token_data\", expiry_hours: int = 24, debug: bool = False):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.tokens_file = self.data_dir / \"tokens.pkl\"\n",
    "        self.expiry_hours = expiry_hours\n",
    "        self.debug = debug\n",
    "\n",
    "    async def save_trading_starts_async(self, trading_starts: List[TradingStart], skip_existing: bool = True) -> Dict[str, int]:\n",
    "        \"\"\"Save trading starts to joblib file.\"\"\"\n",
    "        if not trading_starts:\n",
    "            return {\"saved\": 0, \"skipped\": 0, \"errors\": 0}\n",
    "\n",
    "        # Load existing data\n",
    "        existing_tokens = []\n",
    "        if self.tokens_file.exists():\n",
    "            try:\n",
    "                existing_tokens = joblib.load(self.tokens_file)\n",
    "                if not isinstance(existing_tokens, list):\n",
    "                    existing_tokens = []\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"Error loading existing tokens: {e}\")\n",
    "                existing_tokens = []\n",
    "\n",
    "        # Convert existing to dict for fast lookup\n",
    "        existing_mints = {token.mint for token in existing_tokens if hasattr(token, 'mint')} if skip_existing else set()\n",
    "        \n",
    "        saved = 0\n",
    "        skipped = 0\n",
    "        errors = 0\n",
    "        \n",
    "        for start in trading_starts:\n",
    "            try:\n",
    "                if skip_existing and start.mint in existing_mints:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                existing_tokens.append(start)\n",
    "                saved += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"Error saving token {start.mint}: {e}\")\n",
    "                errors += 1\n",
    "\n",
    "        # Save updated list\n",
    "        try:\n",
    "            joblib.dump(existing_tokens, self.tokens_file)\n",
    "            if self.debug:\n",
    "                print(f\"Saved {len(existing_tokens)} total tokens to {self.tokens_file}\")\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"Error saving tokens file: {e}\")\n",
    "            errors += len(trading_starts)\n",
    "\n",
    "        return {\"saved\": saved, \"skipped\": skipped, \"errors\": errors}\n",
    "\n",
    "    async def cleanup_old_tokens_async(self) -> int:\n",
    "        \"\"\"Remove tokens older than expiry_hours.\"\"\"\n",
    "        if not self.tokens_file.exists():\n",
    "            return 0\n",
    "\n",
    "        try:\n",
    "            tokens = joblib.load(self.tokens_file)\n",
    "            if not isinstance(tokens, list):\n",
    "                return 0\n",
    "\n",
    "            cutoff_time = time.time() - (self.expiry_hours * 3600)\n",
    "            filtered_tokens = [\n",
    "                token for token in tokens \n",
    "                if hasattr(token, 'block_time') and token.block_time and token.block_time > cutoff_time\n",
    "            ]\n",
    "\n",
    "            deleted_count = len(tokens) - len(filtered_tokens)\n",
    "            \n",
    "            if deleted_count > 0:\n",
    "                joblib.dump(filtered_tokens, self.tokens_file)\n",
    "                if self.debug:\n",
    "                    print(f\"Cleaned up {deleted_count} old tokens\")\n",
    "\n",
    "            return deleted_count\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"Error during cleanup: {e}\")\n",
    "            return 0\n",
    "\n",
    "    async def get_tracked_tokens_async(self, limit: Optional[int] = None) -> List[TradingStart]:\n",
    "        \"\"\"Get all tracked tokens.\"\"\"\n",
    "        if not self.tokens_file.exists():\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            tokens = joblib.load(self.tokens_file)\n",
    "            if not isinstance(tokens, list):\n",
    "                return []\n",
    "\n",
    "            # Sort by block_time descending (newest first)\n",
    "            sorted_tokens = sorted(\n",
    "                tokens,\n",
    "                key=lambda x: x.block_time if hasattr(x, 'block_time') and x.block_time else 0,\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            if limit:\n",
    "                sorted_tokens = sorted_tokens[:limit]\n",
    "\n",
    "            return sorted_tokens\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"Error loading tokens: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cfcb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution function\n",
    "async def main():\n",
    "    \"\"\"Main function to run token discovery and storage.\"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    client = SolanaAlphaClient(BASE_URL)\n",
    "    \n",
    "    # Test connection\n",
    "    connection_ok = await client.test_connection()\n",
    "    if not connection_ok:\n",
    "        print(\"Failed to connect to Solana RPC\")\n",
    "        return\n",
    "\n",
    "    # Initialize token updater\n",
    "    updater = JobLibTokenUpdater(\n",
    "        data_dir=\"./data/token_data\",\n",
    "        expiry_hours=24,\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    # Initialize TokenDiscovery\n",
    "    td = TokenDiscovery(\n",
    "        client=client,\n",
    "        birdeye_api_key=birdeye_key,\n",
    "        dune_api_key=DUNE_API_KEY,\n",
    "        dune_query_id=query_id,\n",
    "        dune_cache_file=\"./data/dune_recent.pkl\",\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 1) Get yesterday's tokens from Dune (cached)\n",
    "        print(\"Fetching tokens from Dune (yesterday)...\")\n",
    "        dune_starts = td.get_tokens_launched_yesterday_cached()\n",
    "        print(f\"Fetched from Dune: {len(dune_starts)} tokens\")\n",
    "\n",
    "        # 2) Get today's tokens from Birdeye\n",
    "        print(\"Fetching tokens from Birdeye (today)...\")\n",
    "        today_starts = await td.get_tokens_with_liquidity_today(limit=20)\n",
    "        print(f\"Fetched from Birdeye: {len(today_starts)} tokens\")\n",
    "\n",
    "        # 3) Save both to storage\n",
    "        combined = dune_starts + today_starts\n",
    "        saved_stats = await updater.save_trading_starts_async(combined, skip_existing=True)\n",
    "        print(f\"Save results: {saved_stats}\")\n",
    "\n",
    "        # 4) Cleanup old tokens\n",
    "        deleted_count = await updater.cleanup_old_tokens_async()\n",
    "        print(f\"Cleaned up {deleted_count} old tokens\")\n",
    "\n",
    "        # 5) Show current tracked tokens\n",
    "        stored_tokens = await updater.get_tracked_tokens_async(limit=10)\n",
    "        print(f\"\\nCurrently tracking {len(stored_tokens)} tokens (showing first 10):\")\n",
    "        for token in stored_tokens[:10]:\n",
    "            dt_str = datetime.fromtimestamp(token.block_time, timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\") if token.block_time else \"Unknown\"\n",
    "            symbol = token.extra.get(\"symbol\", \"Unknown\") if token.extra else \"Unknown\"\n",
    "            print(f\"  {token.mint} | {symbol} | {dt_str} | via {token.detected_via}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "801d8d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Solana RPC connection successful\n",
      "TokenDiscovery initialized. Dune cache: ./data/dune_recent.pkl\n",
      "Fetching tokens from Dune (yesterday)...\n",
      "[Dune/cache] cached but no yesterday rows; cache age 0d <= 7d -> will fetch fresh\n",
      "[Dune] fetching latest result for query 5668844...\n",
      "[Dune] extracted 3 rows from payload\n",
      "[Dune/cache] wrote 3 rows to cache ./data/dune_recent.pkl\n",
      "[Dune] found 3 yesterday tokens after fresh fetch\n",
      "Fetched from Dune: 3 tokens\n",
      "Fetching tokens from Birdeye (today)...\n",
      "Fetched from Birdeye: 20 tokens\n",
      "Saved 63 total tokens to data\\token_data\\tokens.pkl\n",
      "Save results: {'saved': 23, 'skipped': 0, 'errors': 0}\n",
      "Cleaned up 23 old tokens\n",
      "Cleaned up 23 old tokens\n",
      "\n",
      "Currently tracking 10 tokens (showing first 10):\n",
      "  6Egn3FVPA6WpANr2uWoehdxstgyyLDFDpDc3yJUMgray | STINK | 2025-08-26 20:35:58 | via birdeye\n",
      "  68pRHfBuged76wNnDGC49JnBcXBXGLJTjf47ix6Nqgn9 | TGH | 2025-08-26 20:35:57 | via birdeye\n",
      "  4LUEVB7SDPdFnA7QNdPBBNkwk51tjVspDwShUhgyuv3a | CLIPPY AI  | 2025-08-26 20:35:55 | via birdeye\n",
      "  2f4AKj6dv1jRohhBzXHk7tnk93fqucapiKRmsyQHdutx | None | 2025-08-26 20:35:54 | via birdeye\n",
      "  Cf4pyqoRmonfMyZvVb7RYH2Q36eS2kq2GCRAACVRpump | 404 | 2025-08-26 20:35:47 | via birdeye\n",
      "  64FBDs2PwjeCo1ooTmsrUNsBnHDBzDTyvv2yg77Hpump | quant | 2025-08-26 20:35:36 | via birdeye\n",
      "  B6NYraUR2KvT4ToQn8SPDTL1mVkHGyrqHxUK3yiopump | Nanobanana | 2025-08-26 20:35:36 | via birdeye\n",
      "  26cxjNGpzJWccmffHEEPHBU5DGa3urDDexEtHK9c1sBt | BAKI | 2025-08-26 20:35:33 | via birdeye\n",
      "  2vHtpDShPJXhf1gFRr9mmq996ivXdtUtRkJZxiUsrray | NEPE | 2025-08-26 20:35:31 | via birdeye\n",
      "  Gk6S4gMp5NjNxL3byuy4hMVW1urAtswwzD2F65vsdrV8 | PUMP | 2025-08-26 20:35:21 | via birdeye\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "degen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
