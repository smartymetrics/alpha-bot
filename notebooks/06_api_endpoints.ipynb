{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767cb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced token monitor with persistent scheduling and hierarchical overlap scoring.\n",
    "Now using CoinGecko Pro API for new token discovery.\n",
    "Features:\n",
    " - SchedulingStore: Persistent scheduling state using joblib\n",
    " - Enhanced overlap grading with CRITICAL/HIGH/MEDIUM/LOW classifications\n",
    " - Concentration-based grading\n",
    " - Startup recovery logic\n",
    " - Persistent timestamp tracking to avoid refetching\n",
    " - 24-hour token expiration\n",
    "\"\"\"\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import math\n",
    "\n",
    "# NOTE: keep dune_client import guarded\n",
    "try:\n",
    "    from dune_client.client import DuneClient\n",
    "except Exception:\n",
    "    DuneClient = None\n",
    "\n",
    "# -----------------------\n",
    "# Sanitizer utilities\n",
    "# -----------------------\n",
    "def _sanitize_dict(d: Dict[Any, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Recursively sanitize a dictionary so all keys are strings (replace None with \"null\"),\n",
    "    and nested dicts/lists are processed. This prevents joblib/pickle errors like\n",
    "    \"Cannot serialize non-str key None\".\n",
    "    \"\"\"\n",
    "    clean: Dict[str, Any] = {}\n",
    "    for k, v in d.items():\n",
    "        key = \"null\" if k is None else str(k)\n",
    "        if isinstance(v, dict):\n",
    "            v = _sanitize_dict(v)\n",
    "        elif isinstance(v, list):\n",
    "            v = [_sanitize_dict(x) if isinstance(x, dict) else x for x in v]\n",
    "        clean[key] = v\n",
    "    return clean\n",
    "\n",
    "def _sanitize_maybe(obj: Any) -> Any:\n",
    "    \"\"\"Sanitize objects that may contain dicts/lists/TradingStart.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return _sanitize_dict(obj)\n",
    "    if isinstance(obj, list):\n",
    "        return [_sanitize_maybe(x) for x in obj]\n",
    "    if isinstance(obj, TradingStart):\n",
    "        d = asdict(obj)\n",
    "        d[\"extra\"] = _sanitize_dict(d.get(\"extra\") or {})\n",
    "        return d\n",
    "    return obj\n",
    "\n",
    "# Re-usable lightweight Solana RPC client\n",
    "class SolanaAlphaClient:\n",
    "    def __init__(self, rpc_url: str):\n",
    "        self.rpc_url = rpc_url\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    async def make_rpc_call(self, method: str, params: List[Any]) -> Dict[str, Any]:\n",
    "        payload = {\"jsonrpc\": \"2.0\", \"id\": \"1\", \"method\": method, \"params\": params}\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            try:\n",
    "                async with session.post(self.rpc_url, json=payload, headers=self.headers, timeout=20) as resp:\n",
    "                    resp.raise_for_status()\n",
    "                    return await resp.json()\n",
    "            except Exception as e:\n",
    "                return {\"error\": str(e)}\n",
    "\n",
    "    async def test_connection(self) -> bool:\n",
    "        r = await self.make_rpc_call(\"getHealth\", [])\n",
    "        return r.get(\"result\") == \"ok\"\n",
    "\n",
    "@dataclass\n",
    "class TradingStart:\n",
    "    mint: Optional[str] = None\n",
    "    block_time: Optional[int] = None\n",
    "    program_id: Optional[str] = None\n",
    "    detected_via: Optional[str] = None\n",
    "    extra: Optional[Dict[str, Any]] = None\n",
    "    fdv_usd: Optional[float] = None\n",
    "    volume_usd: Optional[float] = None\n",
    "    source_dex: Optional[str] = None\n",
    "    price_change_percentage: Optional[float] = None\n",
    "\n",
    "class TokenDiscovery:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: Optional[Any] = None,\n",
    "        *,\n",
    "        coingecko_pro_api_key: Optional[str] = None,\n",
    "        dune_api_key: Optional[str] = None,\n",
    "        dune_query_id: Optional[int] = None,\n",
    "        dune_cache_file: str = \"./data/dune_recent.pkl\",\n",
    "        timestamp_cache_file: str = \"./data/last_timestamp.pkl\",\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.debug = bool(debug)\n",
    "        # CoinGecko\n",
    "        self.coingecko_pro_api_key = coingecko_pro_api_key or os.environ.get(\"GECKO_API\")\n",
    "        self.coingecko_url = \"https://pro-api.coingecko.com/api/v3/onchain/networks/solana/new_pools\"\n",
    "        self.last_processed_timestamp = self._load_last_timestamp(timestamp_cache_file)\n",
    "        self.timestamp_cache_file = timestamp_cache_file\n",
    "        # Dune\n",
    "        self.dune_api_key = dune_api_key or os.environ.get(\"DUNE_API_KEY\")\n",
    "        self.dune_query_id = dune_query_id\n",
    "        if DuneClient and self.dune_api_key:\n",
    "            try:\n",
    "                self.dune_client = DuneClient(self.dune_api_key)\n",
    "            except Exception:\n",
    "                self.dune_client = None\n",
    "        else:\n",
    "            self.dune_client = None\n",
    "        self.dune_cache_file = dune_cache_file\n",
    "        if self.debug:\n",
    "            print(\"TokenDiscovery initialized with CoinGecko Pro\")\n",
    "\n",
    "    def _load_last_timestamp(self, cache_file: str) -> Optional[int]:\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                return joblib.load(cache_file)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return None\n",
    "\n",
    "    def _save_last_timestamp(self):\n",
    "        try:\n",
    "            joblib.dump(self.last_processed_timestamp, self.timestamp_cache_file)\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"Failed to save last timestamp: {e}\")\n",
    "\n",
    "    # ---------------- Dune helpers ----------------\n",
    "    def _rows_from_dune_payload(self, payload: Any) -> List[Dict[str, Any]]:\n",
    "        if payload is None:\n",
    "            return []\n",
    "        if hasattr(payload, \"result\"):\n",
    "            try:\n",
    "                r = getattr(payload, \"result\")\n",
    "                if isinstance(r, dict) and \"rows\" in r and isinstance(r[\"rows\"], list):\n",
    "                    return r[\"rows\"]\n",
    "                if hasattr(r, \"rows\"):\n",
    "                    return list(getattr(r, \"rows\") or [])\n",
    "            except Exception:\n",
    "                pass\n",
    "        if isinstance(payload, dict):\n",
    "            if \"result\" in payload and isinstance(payload[\"result\"], dict) and \"rows\" in payload[\"result\"]:\n",
    "                return payload[\"result\"][\"rows\"]\n",
    "            if \"rows\" in payload and isinstance(payload[\"rows\"], list):\n",
    "                return payload[\"rows\"]\n",
    "            if \"data\" in payload and isinstance(payload[\"data\"], list):\n",
    "                return payload[\"data\"]\n",
    "        if isinstance(payload, list):\n",
    "            return payload\n",
    "        if hasattr(payload, \"rows\"):\n",
    "            r = getattr(payload, \"rows\")\n",
    "            if isinstance(r, list):\n",
    "                return r\n",
    "        return []\n",
    "\n",
    "    def fetch_dune_latest_rows(self) -> List[Dict[str, Any]]:\n",
    "        if not self.dune_client or not self.dune_query_id:\n",
    "            raise RuntimeError(\"Dune client or query_id not configured\")\n",
    "        if self.debug:\n",
    "            print(f\"[Dune] fetching latest result for query {self.dune_query_id}\")\n",
    "        payload = self.dune_client.get_latest_result(self.dune_query_id)\n",
    "        rows = self._rows_from_dune_payload(payload)\n",
    "        if self.debug:\n",
    "            print(f\"[Dune] extracted {len(rows)} rows\")\n",
    "        return rows\n",
    "\n",
    "    def get_tokens_launched_yesterday_cached(self, cache_max_age_days: int = 7) -> List[TradingStart]:\n",
    "        cache_path = self.dune_cache_file\n",
    "\n",
    "        def rows_to_trading_starts(rows: List[Dict[str, Any]], target_yesterday: datetime.date) -> List[TradingStart]:\n",
    "            if not rows:\n",
    "                return []\n",
    "            df = pd.DataFrame(rows)\n",
    "            date_col = None\n",
    "            mint_col = None\n",
    "            for c in (\"first_buy_date\", \"first_buy_date_utc\", \"block_date\", \"first_trade_date\"):\n",
    "                if c in df.columns:\n",
    "                    date_col = c\n",
    "                    break\n",
    "            for c in (\"mint_address\", \"mint\", \"token_bought_mint_address\"):\n",
    "                if c in df.columns:\n",
    "                    mint_col = c\n",
    "                    break\n",
    "            if date_col is None or mint_col is None:\n",
    "                return []\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "            filtered = df[df[date_col].dt.date == target_yesterday]\n",
    "            out = []\n",
    "            for _, row in filtered.iterrows():\n",
    "                try:\n",
    "                    dt = pd.to_datetime(row[date_col])\n",
    "                    if pd.isna(dt):\n",
    "                        continue\n",
    "                    if dt.tzinfo is None:\n",
    "                        dt = dt.tz_localize(\"UTC\")\n",
    "                    ts = int(dt.tz_convert(\"UTC\").timestamp())\n",
    "                except Exception:\n",
    "                    continue\n",
    "                out.append(\n",
    "                    TradingStart(mint=row[mint_col], block_time=ts, program_id=\"dune\", detected_via=\"dune\", extra={date_col: str(row[date_col])})\n",
    "                )\n",
    "            return out\n",
    "\n",
    "        current_yesterday = (datetime.now(timezone.utc).date() - timedelta(days=1))\n",
    "        need_fetch = True\n",
    "        if os.path.exists(cache_path):\n",
    "            try:\n",
    "                cache_obj = joblib.load(cache_path)\n",
    "                if isinstance(cache_obj, dict) and \"rows\" in cache_obj and \"fetched_at\" in cache_obj:\n",
    "                    fetched_at = None\n",
    "                    cached_yesterday = None\n",
    "                    try:\n",
    "                        fetched_at = datetime.fromisoformat(cache_obj[\"fetched_at\"])\n",
    "                        if fetched_at.tzinfo is None:\n",
    "                            fetched_at = fetched_at.replace(tzinfo=timezone.utc)\n",
    "                        cached_yesterday = (fetched_at.date() - timedelta(days=1))\n",
    "                    except Exception:\n",
    "                        fetched_at = None\n",
    "                        cached_yesterday = None\n",
    "                    rows = cache_obj.get(\"rows\", [])\n",
    "                    if cached_yesterday == current_yesterday and fetched_at:\n",
    "                        starts = rows_to_trading_starts(rows, current_yesterday)\n",
    "                        if starts:\n",
    "                            if self.debug:\n",
    "                                print(f\"[Dune/cache] using cached data for yesterday={current_yesterday}, found {len(starts)} tokens\")\n",
    "                            need_fetch = False\n",
    "                            return starts\n",
    "                        elif self.debug:\n",
    "                            print(f\"[Dune/cache] cached data for yesterday={current_yesterday} but no tokens found\")\n",
    "                    else:\n",
    "                        if self.debug:\n",
    "                            if cached_yesterday != current_yesterday:\n",
    "                                print(f\"[Dune/cache] cached yesterday={cached_yesterday} != current yesterday={current_yesterday} -> need fresh data\")\n",
    "                            else:\n",
    "                                print(\"[Dune/cache] cache date calculation failed -> need fresh data\")\n",
    "                    if fetched_at:\n",
    "                        age_days = (datetime.now(timezone.utc) - fetched_at).days\n",
    "                        if age_days > cache_max_age_days:\n",
    "                            if self.debug:\n",
    "                                print(f\"[Dune/cache] cache age {age_days} days > max {cache_max_age_days} days -> need fresh data\")\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"[Dune/cache] load failed: {e}\")\n",
    "        if need_fetch:\n",
    "            try:\n",
    "                rows = self.fetch_dune_latest_rows()\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"[Dune] fetch failure: {e}\")\n",
    "                return []\n",
    "            try:\n",
    "                cache_obj = {\n",
    "                    \"rows\": rows,\n",
    "                    \"fetched_at\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"target_yesterday\": current_yesterday.isoformat()\n",
    "                }\n",
    "                joblib.dump(cache_obj, cache_path)\n",
    "                if self.debug:\n",
    "                    print(f\"[Dune/cache] cached fresh data for yesterday={current_yesterday}\")\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"[Dune/cache] write failed: {e}\")\n",
    "            starts = rows_to_trading_starts(rows, current_yesterday)\n",
    "            if self.debug:\n",
    "                print(f\"[Dune] found {len(starts)} tokens for yesterday={current_yesterday} after fresh fetch\")\n",
    "            return starts\n",
    "        return []\n",
    "\n",
    "    # ---------------- CoinGecko ----------------\n",
    "    async def _fetch_coingecko_new_pools(self, limit: int = 500, timeout: int = 15) -> List[Dict[str, Any]]:\n",
    "        headers = {\"accept\": \"application/json\"}\n",
    "        if self.coingecko_pro_api_key:\n",
    "            headers[\"x-cg-pro-api-key\"] = self.coingecko_pro_api_key\n",
    "\n",
    "        all_pools = []\n",
    "        page = 1\n",
    "        now = int(datetime.now(timezone.utc).timestamp())\n",
    "        cutoff = now - 24 * 3600\n",
    "        effective_cutoff = max(self.last_processed_timestamp or 0, cutoff)\n",
    "\n",
    "        while True:\n",
    "            url = f\"{self.coingecko_url}?page={page}\"\n",
    "            async with aiohttp.ClientSession() as sess:\n",
    "                async with sess.get(url, headers=headers, timeout=timeout) as resp:\n",
    "                    resp.raise_for_status()\n",
    "                    data = await resp.json()\n",
    "                    pools = data.get(\"data\", [])\n",
    "                    if not pools:\n",
    "                        break\n",
    "\n",
    "                    stop_paging = False\n",
    "                    for pool in pools:\n",
    "                        block_time = self._parse_pool_created_at(pool[\"attributes\"][\"pool_created_at\"])\n",
    "                        if not block_time:\n",
    "                            continue\n",
    "\n",
    "                        if block_time < effective_cutoff:\n",
    "                            stop_paging = True\n",
    "                            break\n",
    "\n",
    "                        all_pools.append(pool)\n",
    "\n",
    "                    if stop_paging or len(all_pools) >= limit:\n",
    "                        break\n",
    "\n",
    "                    page += 1\n",
    "\n",
    "        return all_pools\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_pool_created_at(val: Any) -> Optional[int]:\n",
    "        if not val:\n",
    "            return None\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(str(val))\n",
    "            if dt.tzinfo is None:\n",
    "                dt = dt.replace(tzinfo=timezone.utc)\n",
    "            return int(dt.timestamp())\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _utc_day_bounds_for_date(dt: Optional[datetime] = None) -> Tuple[int, int]:\n",
    "        d = (dt or datetime.now(timezone.utc)).astimezone(timezone.utc)\n",
    "        start = datetime(d.year, d.month, d.day, 0, 0, 0, tzinfo=timezone.utc)\n",
    "        end = start + timedelta(days=1) - timedelta(seconds=1)\n",
    "        return int(start.timestamp()), int(end.timestamp())\n",
    "\n",
    "    def _parse_coingecko_pool(self, pool: Dict[str, Any]) -> TradingStart:\n",
    "        attributes = pool[\"attributes\"]\n",
    "        base_token = pool[\"relationships\"][\"base_token\"][\"data\"]\n",
    "        mint = base_token[\"id\"].replace(\"eth_\", \"\").replace(\"solana_\", \"\")  # Handle both ETH and Solana\n",
    "        block_time = self._parse_pool_created_at(attributes[\"pool_created_at\"])\n",
    "        return TradingStart(\n",
    "            mint=mint,\n",
    "            block_time=block_time,\n",
    "            program_id=\"coingecko\",\n",
    "            detected_via=\"coingecko\",\n",
    "            extra={\n",
    "                \"name\": attributes[\"name\"].split(\" / \")[0],\n",
    "                \"fdv_usd\": attributes[\"fdv_usd\"],\n",
    "                \"market_cap_usd\": attributes.get(\"market_cap_usd\") or attributes[\"fdv_usd\"],\n",
    "                \"volume_usd\": attributes[\"volume_usd\"][\"h24\"],\n",
    "                \"source_dex\": pool[\"relationships\"][\"dex\"][\"data\"][\"id\"],\n",
    "                \"price_change_percentage\": attributes[\"price_change_percentage\"][\"h24\"],\n",
    "            },\n",
    "            fdv_usd=attributes[\"fdv_usd\"],\n",
    "            volume_usd=attributes[\"volume_usd\"][\"h24\"],\n",
    "            source_dex=pool[\"relationships\"][\"dex\"][\"data\"][\"id\"],\n",
    "            price_change_percentage=attributes[\"price_change_percentage\"][\"h24\"],\n",
    "        )\n",
    "\n",
    "    async def get_tokens_created_today(self, limit: int = 500) -> List[TradingStart]:\n",
    "        pools = await self._fetch_coingecko_new_pools(limit=limit)\n",
    "        out = []\n",
    "        now = int(datetime.now(timezone.utc).timestamp())\n",
    "        cutoff = now - 24 * 3600  # only include pools launched in last 24 hours\n",
    "\n",
    "        for pool in pools:\n",
    "            block_time = self._parse_pool_created_at(pool[\"attributes\"][\"pool_created_at\"])\n",
    "            if not block_time:\n",
    "                continue\n",
    "\n",
    "            # ✅ filter: only pools created in last 24 hours\n",
    "            if block_time < cutoff:\n",
    "                continue\n",
    "\n",
    "            ts = self._parse_coingecko_pool(pool)\n",
    "            out.append(ts)\n",
    "\n",
    "            # update last processed timestamp\n",
    "            if self.last_processed_timestamp is None or block_time > self.last_processed_timestamp:\n",
    "                self.last_processed_timestamp = block_time\n",
    "\n",
    "        if out:\n",
    "            self._save_last_timestamp()\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[CoinGecko] {len(out)} tokens launched in last 24h\")\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HolderAggregator:\n",
    "    def __init__(self, client: SolanaAlphaClient):\n",
    "        self.client = client\n",
    "\n",
    "    async def get_token_holders(self, token_mint: str, *, sleep_between: float = 0.15, limit: int = 1000, max_pages: Optional[int] = None, decimals: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "        page = 1\n",
    "        owner_balances = defaultdict(int)\n",
    "        owner_token_account_counts = defaultdict(int)\n",
    "        while True:\n",
    "            payload_params = {\"mint\": token_mint, \"page\": page, \"limit\": limit, \"displayOptions\": {}}\n",
    "            data = await self.client.make_rpc_call(\"getTokenAccounts\", payload_params)\n",
    "            token_accounts = data.get(\"result\", {}).get(\"token_accounts\", [])\n",
    "            if not token_accounts:\n",
    "                break\n",
    "            for ta in token_accounts:\n",
    "                owner = ta.get(\"owner\") or ta.get(\"address\")\n",
    "                amt_raw = ta.get(\"amount\", 0)\n",
    "                if \"account\" in ta and isinstance(ta[\"account\"], dict):\n",
    "                    acct = ta[\"account\"]\n",
    "                    owner = owner or acct.get(\"owner\")\n",
    "                    amt_raw = acct.get(\"amount\", 0)\n",
    "                if isinstance(amt_raw, dict):\n",
    "                    amt_raw = int(float(amt_raw.get(\"amount\") or amt_raw.get(\"uiAmount\", 0)))\n",
    "                else:\n",
    "                    try:\n",
    "                        amt_raw = int(amt_raw)\n",
    "                    except Exception:\n",
    "                        amt_raw = int(float(amt_raw)) if amt_raw else 0\n",
    "                if owner:\n",
    "                    owner_balances[owner] += amt_raw\n",
    "                    owner_token_account_counts[owner] += 1\n",
    "            page += 1\n",
    "            if max_pages and page > max_pages:\n",
    "                break\n",
    "            await asyncio.sleep(sleep_between)\n",
    "        holders = []\n",
    "        for owner, raw in owner_balances.items():\n",
    "            human_balance = raw / (10 ** decimals) if decimals else None\n",
    "            holders.append({\"wallet\": owner, \"balance_raw\": raw, \"balance\": human_balance, \"balance_formatted\": (f\"{human_balance:,.{decimals}f}\" if human_balance is not None and decimals is not None else str(raw)), \"num_token_accounts\": owner_token_account_counts[owner]})\n",
    "        holders.sort(key=lambda x: x[\"balance_raw\"], reverse=True)\n",
    "        return holders\n",
    "    \n",
    "def _normalize(obj: Any) -> Any:\n",
    "    if isinstance(obj, dict):\n",
    "        return {(\"null\" if k is None else str(k)): _normalize(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [_normalize(v) for v in obj]\n",
    "    elif isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):\n",
    "        return None\n",
    "    elif isinstance(obj, TradingStart):\n",
    "        d = asdict(obj)\n",
    "        d[\"extra\"] = _normalize(d.get(\"extra\") or {})\n",
    "        return _normalize(d)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "class JobLibTokenUpdater:\n",
    "    def __init__(self, data_dir: str = \"./data/token_data\", expiry_hours: int = 24, debug: bool = False):\n",
    "        self.data_dir = os.path.abspath(data_dir)\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        self.tokens_file = os.path.join(self.data_dir, \"tokens.pkl\")\n",
    "        self.expiry_hours = expiry_hours\n",
    "        self.debug = debug\n",
    "\n",
    "    def _load_tokens(self) -> List[Any]:\n",
    "        if os.path.exists(self.tokens_file):\n",
    "            try:\n",
    "                data = joblib.load(self.tokens_file)\n",
    "                if isinstance(data, list):\n",
    "                    return data\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(\"JobLibTokenUpdater: load error\", e)\n",
    "        return []\n",
    "\n",
    "    def _save_tokens(self, tokens: List[Any]):\n",
    "        try:\n",
    "            safe_tokens = [_normalize(t) for t in tokens]\n",
    "            joblib.dump(safe_tokens, self.tokens_file)\n",
    "        except Exception as e:\n",
    "            print(\"JobLibTokenUpdater: save error\", e)\n",
    "            traceback.print_exc()\n",
    "\n",
    "            # Write a debug snapshot so we can inspect the bad data\n",
    "            debug_path = self.tokens_file + \".debug.json\"\n",
    "            try:\n",
    "                import json\n",
    "                with open(debug_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump([asdict(t) if isinstance(t, TradingStart) else str(t) for t in tokens], f, indent=2, default=str)\n",
    "                print(f\"Saved debug snapshot to {debug_path}\")\n",
    "            except Exception as ee:\n",
    "                print(\"Also failed to dump debug snapshot:\", ee)\n",
    "\n",
    "\n",
    "    async def save_trading_starts_async(self, trading_starts: List[TradingStart], skip_existing: bool = True) -> Dict[str, int]:\n",
    "        existing = self._load_tokens()\n",
    "        # Existing may be list of dicts or TradingStart (if older saves). Normalize to dicts for checking.\n",
    "        existing_mints: Set[str] = set()\n",
    "        for t in existing:\n",
    "            if isinstance(t, dict):\n",
    "                m = t.get(\"mint\")\n",
    "            elif isinstance(t, TradingStart):\n",
    "                m = t.mint\n",
    "            else:\n",
    "                m = None\n",
    "            if m:\n",
    "                existing_mints.add(m)\n",
    "\n",
    "        saved = 0\n",
    "        skipped = 0\n",
    "        errors = 0\n",
    "        for s in trading_starts:\n",
    "            try:\n",
    "                if skip_existing and s.mint in existing_mints:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                existing.append(s)\n",
    "                saved += 1\n",
    "            except Exception:\n",
    "                errors += 1\n",
    "        self._save_tokens(existing)\n",
    "        if self.debug:\n",
    "            print(f\"JobLibTokenUpdater: saved={saved} skipped={skipped} errors={errors} total_now={len(existing)}\")\n",
    "        return {\"saved\": saved, \"skipped\": skipped, \"errors\": errors}\n",
    "\n",
    "    async def cleanup_old_tokens_async(self) -> int:\n",
    "        tokens = self._load_tokens()\n",
    "        if not tokens:\n",
    "            return 0\n",
    "        now = datetime.now(timezone.utc)\n",
    "        cutoff = int((now - timedelta(hours=self.expiry_hours)).timestamp())\n",
    "        kept: List[Any] = []\n",
    "        for t in tokens:\n",
    "            ts = 0\n",
    "            if isinstance(t, dict):\n",
    "                ts = int(t.get(\"block_time\", 0) or 0)\n",
    "            elif isinstance(t, TradingStart):\n",
    "                ts = int(t.block_time or 0)\n",
    "            if ts > cutoff:\n",
    "                kept.append(t)\n",
    "        deleted = len(tokens) - len(kept)\n",
    "        if deleted:\n",
    "            self._save_tokens(kept)\n",
    "            if self.debug:\n",
    "                print(f\"JobLibTokenUpdater: cleaned {deleted} tokens older than {self.expiry_hours} hours\")\n",
    "        return deleted\n",
    "\n",
    "    async def get_tracked_tokens_async(self, limit: Optional[int] = None) -> List[TradingStart]:\n",
    "        tokens = self._load_tokens()\n",
    "        # convert dicts to TradingStart\n",
    "        norm: List[TradingStart] = []\n",
    "        for t in tokens:\n",
    "            if isinstance(t, TradingStart):\n",
    "                norm.append(t)\n",
    "            elif isinstance(t, dict):\n",
    "                try:\n",
    "                    norm.append(TradingStart(**t))\n",
    "                except Exception:\n",
    "                    # if dict contains extra unexpected keys, pop them\n",
    "                    allowed = {\"mint\",\"block_time\",\"program_id\",\"detected_via\",\"extra\",\"fdv_usd\",\"volume_usd\",\"source_dex\",\"price_change_percentage\"}\n",
    "                    clean = {k:v for k,v in t.items() if k in allowed}\n",
    "                    norm.append(TradingStart(**clean))\n",
    "        norm.sort(key=lambda x: x.block_time or 0, reverse=True)\n",
    "        if limit:\n",
    "            norm = norm[:limit]\n",
    "        return norm\n",
    "\n",
    "class DuneHolderCache:\n",
    "    def __init__(self, cache_file: str = \"./data/dune_holders.pkl\", cache_max_days: int = 7, debug: bool = False):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache_max_days = cache_max_days\n",
    "        self.debug = debug\n",
    "        os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)\n",
    "\n",
    "    def _load_cache(self) -> Dict[str, Any]:\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                return joblib.load(self.cache_file)\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(\"DuneHolderCache: load failed\", e)\n",
    "        return {}\n",
    "\n",
    "    def _save_cache(self, obj: Dict[str, Any]):\n",
    "        try:\n",
    "            joblib.dump(_sanitize_maybe(obj), self.cache_file)\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(\"DuneHolderCache: save failed\", e)\n",
    "\n",
    "    async def build_cache(self, token_discovery: TokenDiscovery, holder_agg: HolderAggregator, top_n_per_token: int = 50) -> Dict[str, Set[str]]:\n",
    "        starts = token_discovery.get_tokens_launched_yesterday_cached()\n",
    "        if self.debug:\n",
    "            print(f\"DuneHolderCache: found {len(starts)} dune tokens\")\n",
    "        mapping: Dict[str, Set[str]] = {}\n",
    "        for s in starts:\n",
    "            if not s.mint:\n",
    "                continue\n",
    "            try:\n",
    "                holders = await holder_agg.get_token_holders(s.mint, limit=1000, max_pages=2, decimals=None)\n",
    "                top_wallets = {h[\"wallet\"] for h in holders[:top_n_per_token]}\n",
    "                mapping[s.mint] = top_wallets\n",
    "                if self.debug:\n",
    "                    print(f\"DuneHolderCache: token {s.mint} -> {len(top_wallets)} top holders\")\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"DuneHolderCache: error fetching holders for {s.mint}: {e}\")\n",
    "                mapping[s.mint] = set()\n",
    "        cache_obj = {\"mapping\": mapping, \"fetched_at\": datetime.now(timezone.utc).isoformat()}\n",
    "        self._save_cache(cache_obj)\n",
    "        return mapping\n",
    "\n",
    "    def load_mapping(self) -> Tuple[Dict[str, Set[str]], Optional[datetime]]:\n",
    "        obj = self._load_cache()\n",
    "        if not obj:\n",
    "            return {}, None\n",
    "        mapping = obj.get(\"mapping\", {})\n",
    "        fetched_at = None\n",
    "        try:\n",
    "            fetched_at = datetime.fromisoformat(obj.get(\"fetched_at\"))\n",
    "            if fetched_at.tzinfo is None:\n",
    "                fetched_at = fetched_at.replace(tzinfo=timezone.utc)\n",
    "        except Exception:\n",
    "            fetched_at = None\n",
    "        return mapping, fetched_at\n",
    "\n",
    "class OverlapStore:\n",
    "    def __init__(self, filepath: str = \"./data/overlap_results.pkl\", debug: bool = False):\n",
    "        self.filepath = filepath\n",
    "        self.debug = debug\n",
    "        os.makedirs(os.path.dirname(self.filepath), exist_ok=True)\n",
    "\n",
    "    def load(self) -> Dict[str, Any]:\n",
    "        if os.path.exists(self.filepath):\n",
    "            try:\n",
    "                return joblib.load(self.filepath)\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(\"OverlapStore: load failed\", e)\n",
    "        return {}\n",
    "\n",
    "    def save(self, obj: Dict[str, Any]):\n",
    "        try:\n",
    "            joblib.dump(_sanitize_maybe(obj), self.filepath)\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(\"OverlapStore: save failed\", e)\n",
    "\n",
    "class SchedulingStore:\n",
    "    def __init__(self, filepath: str = \"./data/scheduling_state.pkl\", debug: bool = False):\n",
    "        self.filepath = filepath\n",
    "        self.debug = debug\n",
    "        os.makedirs(os.path.dirname(self.filepath), exist_ok=True)\n",
    "\n",
    "    def load(self) -> Dict[str, Any]:\n",
    "        if os.path.exists(self.filepath):\n",
    "            try:\n",
    "                return joblib.load(self.filepath)\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(\"SchedulingStore: load failed\", e)\n",
    "        return {}\n",
    "\n",
    "    def save(self, obj: Dict[str, Any]):\n",
    "        try:\n",
    "            joblib.dump(_sanitize_maybe(obj), self.filepath)\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(\"SchedulingStore: save failed\", e)\n",
    "\n",
    "    def update_token_state(self, token_mint: str, state_update: Dict[str, Any]):\n",
    "        current_state = self.load()\n",
    "        if token_mint not in current_state:\n",
    "            current_state[token_mint] = {}\n",
    "        current_state[token_mint].update(state_update)\n",
    "        self.save(current_state)\n",
    "\n",
    "    def get_token_state(self, token_mint: str) -> Dict[str, Any]:\n",
    "        current_state = self.load()\n",
    "        return current_state.get(token_mint, {})\n",
    "\n",
    "    def cleanup_old_states(self, cutoff_timestamp: int = None):\n",
    "        current_state = self.load()\n",
    "        now = datetime.now(timezone.utc)\n",
    "        cutoff = cutoff_timestamp or int((now - timedelta(hours=24)).timestamp())\n",
    "        cleaned_state = {}\n",
    "        for token_mint, state in current_state.items():\n",
    "            launch_time = state.get(\"launch_time\", 0)\n",
    "            if launch_time > cutoff:\n",
    "                cleaned_state[token_mint] = state\n",
    "        removed_count = len(current_state) - len(cleaned_state)\n",
    "        if removed_count > 0:\n",
    "            self.save(cleaned_state)\n",
    "            if self.debug:\n",
    "                print(f\"SchedulingStore: cleaned {removed_count} old scheduling states\")\n",
    "        return removed_count\n",
    "\n",
    "def calculate_overlap_grade(overlap_count: int, overlap_percentage: float, concentration: float, total_new_holders: int, total_winner_wallets: int) -> str:\n",
    "    if (overlap_percentage >= 50 and overlap_count >= 100) or \\\n",
    "       (overlap_percentage >= 60 and overlap_count >= 50) or \\\n",
    "       (concentration >= 30 and overlap_count >= 75) or \\\n",
    "       (concentration >= 40 and overlap_count >= 50):\n",
    "        return \"CRITICAL\"\n",
    "    elif (overlap_percentage >= 30 and overlap_count >= 50) or \\\n",
    "         (overlap_percentage >= 40 and overlap_count >= 25) or \\\n",
    "         (concentration >= 20 and overlap_count >= 40) or \\\n",
    "         (concentration >= 25 and overlap_count >= 30):\n",
    "        return \"HIGH\"\n",
    "    elif (overlap_percentage >= 15 and overlap_count >= 25) or \\\n",
    "         (overlap_percentage >= 20 and overlap_count >= 15) or \\\n",
    "         (concentration >= 10 and overlap_count >= 20) or \\\n",
    "         (concentration >= 15 and overlap_count >= 15):\n",
    "        return \"MEDIUM\"\n",
    "    elif (overlap_percentage >= 5 and overlap_count >= 10) or \\\n",
    "         (overlap_count >= 5) or \\\n",
    "         (concentration >= 5 and overlap_count >= 8) or \\\n",
    "         (concentration >= 8 and overlap_count >= 5):\n",
    "        return \"LOW\"\n",
    "    else:\n",
    "        return \"NONE\"\n",
    "\n",
    "class Monitor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sol_client: SolanaAlphaClient,\n",
    "        token_discovery: TokenDiscovery,\n",
    "        holder_agg: HolderAggregator,\n",
    "        updater: JobLibTokenUpdater,\n",
    "        dune_cache: DuneHolderCache,\n",
    "        overlap_store: OverlapStore,\n",
    "        scheduling_store: SchedulingStore,\n",
    "        *,\n",
    "        coingecko_poll_interval_seconds: int = 30,\n",
    "        initial_check_delay_seconds: int = 2 * 3600,\n",
    "        repeat_interval_seconds: int = 6 * 3600,\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.sol_client = sol_client\n",
    "        self.token_discovery = token_discovery\n",
    "        self.holder_agg = holder_agg\n",
    "        self.updater = updater\n",
    "        self.dune_cache = dune_cache\n",
    "        self.overlap_store = overlap_store\n",
    "        self.scheduling_store = scheduling_store\n",
    "        self.coingecko_poll_interval_seconds = coingecko_poll_interval_seconds\n",
    "        self.initial_check_delay_seconds = initial_check_delay_seconds\n",
    "        self.repeat_interval_seconds = repeat_interval_seconds\n",
    "        self.debug = debug\n",
    "        self._scheduled: Set[str] = set()\n",
    "        self.last_cleanup = 0\n",
    "\n",
    "    async def ensure_dune_holders(self):\n",
    "        mapping, fetched_at = self.dune_cache.load_mapping()\n",
    "        need_build = True\n",
    "        if fetched_at:\n",
    "            age_hours = (datetime.now(timezone.utc) - fetched_at).total_seconds() / 3600\n",
    "            if age_hours <= 24:\n",
    "                need_build = False\n",
    "        if need_build:\n",
    "            if self.debug:\n",
    "                print(\"Monitor: (re)building dune holders cache\")\n",
    "            await self.dune_cache.build_cache(self.token_discovery, self.holder_agg)\n",
    "\n",
    "    async def startup_recovery(self):\n",
    "        if self.debug:\n",
    "            print(\"Monitor: performing startup recovery\")\n",
    "        scheduling_state = self.scheduling_store.load()\n",
    "        current_time = int(datetime.now(timezone.utc).timestamp())\n",
    "        cutoff_time = current_time - (24 * 3600)\n",
    "        self.scheduling_store.cleanup_old_states(cutoff_time)\n",
    "        await self.updater.cleanup_old_tokens_async()\n",
    "        recovery_tasks = []\n",
    "        for token_mint, state in scheduling_state.items():\n",
    "            if token_mint in self._scheduled:\n",
    "                continue\n",
    "            launch_time = state.get(\"launch_time\", 0)\n",
    "            if current_time - launch_time > 24 * 3600:\n",
    "                continue\n",
    "            status = state.get(\"status\", \"unknown\")\n",
    "            if status == \"pending_first\":\n",
    "                first_check_time = launch_time + self.initial_check_delay_seconds\n",
    "                delay = max(0, first_check_time - current_time)\n",
    "                if self.debug:\n",
    "                    print(f\"Recovery: scheduling first check for {token_mint} in {delay}s\")\n",
    "                task = asyncio.create_task(self._schedule_first_check_only(token_mint, delay))\n",
    "                recovery_tasks.append(task)\n",
    "            elif status == \"active\":\n",
    "                next_scheduled = state.get(\"next_scheduled_check\", 0)\n",
    "                delay = max(0, next_scheduled - current_time)\n",
    "                if delay <= 300:\n",
    "                    delay = 0\n",
    "                if self.debug:\n",
    "                    print(f\"Recovery: scheduling repeat check for {token_mint} in {delay}s\")\n",
    "                task = asyncio.create_task(self._schedule_repeat_check_only(token_mint, delay))\n",
    "                recovery_tasks.append(task)\n",
    "            self._scheduled.add(token_mint)\n",
    "        if recovery_tasks:\n",
    "            if self.debug:\n",
    "                print(f\"Monitor: started {len(recovery_tasks)} recovery tasks\")\n",
    "\n",
    "    async def _schedule_first_check_only(self, token_mint: str, delay_seconds: float):\n",
    "        if delay_seconds > 0:\n",
    "            await asyncio.sleep(delay_seconds)\n",
    "        tokens = await self.updater.get_tracked_tokens_async()\n",
    "        token_start = None\n",
    "        for t in tokens:\n",
    "            if t.mint == token_mint:\n",
    "                token_start = t\n",
    "                break\n",
    "        if not token_start:\n",
    "            if self.debug:\n",
    "                print(f\"_schedule_first_check_only: token {token_mint} not found in tracked tokens\")\n",
    "            return\n",
    "        try:\n",
    "            res = await self.check_holders_overlap(token_start)\n",
    "            obj = self.overlap_store.load() or {}\n",
    "            obj.setdefault(token_start.mint, []).append({\n",
    "                \"ts\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"result\": res,\n",
    "                \"check_type\": \"first_check\"\n",
    "            })\n",
    "            self.overlap_store.save(obj)\n",
    "            current_time = int(datetime.now(timezone.utc).timestamp())\n",
    "            next_check = current_time + self.repeat_interval_seconds\n",
    "            self.scheduling_store.update_token_state(token_mint, {\n",
    "                \"status\": \"active\",\n",
    "                \"last_completed_check\": current_time,\n",
    "                \"next_scheduled_check\": next_check,\n",
    "                \"total_checks_completed\": 1\n",
    "            })\n",
    "            asyncio.create_task(self._schedule_repeat_checks_for_token(token_start, next_check))\n",
    "            if self.debug:\n",
    "                print(f\"_schedule_first_check_only: completed first check for {token_mint}, grade: {res.get('grade', 'N/A')}\")\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"_schedule_first_check_only: error for {token_mint}: {e}\")\n",
    "            self.scheduling_store.update_token_state(token_mint, {\n",
    "                \"status\": \"failed\",\n",
    "                \"last_error\": str(e),\n",
    "                \"last_error_at\": datetime.now(timezone.utc).isoformat()\n",
    "            })\n",
    "\n",
    "    async def _schedule_repeat_check_only(self, token_mint: str, delay_seconds: float):\n",
    "        if delay_seconds > 0:\n",
    "            await asyncio.sleep(delay_seconds)\n",
    "        tokens = await self.updater.get_tracked_tokens_async()\n",
    "        token_start = None\n",
    "        for t in tokens:\n",
    "            if t.mint == token_mint:\n",
    "                token_start = t\n",
    "                break\n",
    "        if not token_start:\n",
    "            if self.debug:\n",
    "                print(f\"_schedule_repeat_only: token {token_mint} not found in tracked tokens\")\n",
    "            return\n",
    "        current_time = int(datetime.now(timezone.utc).timestamp())\n",
    "        launch_time = token_start.block_time or current_time\n",
    "        if current_time - launch_time > 24 * 3600:\n",
    "            if self.debug:\n",
    "                print(f\"_schedule_repeat_check_only: token {token_mint} past 24h -> marking completed\")\n",
    "            self.scheduling_store.update_token_state(token_mint, {\n",
    "                \"status\": \"completed\",\n",
    "                \"completed_at\": datetime.now(timezone.utc).isoformat()\n",
    "            })\n",
    "            return\n",
    "        try:\n",
    "            res = await self.check_holders_overlap(token_start)\n",
    "            obj = self.overlap_store.load() or {}\n",
    "            obj.setdefault(token_start.mint, []).append({\n",
    "                \"ts\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"result\": res,\n",
    "                \"check_type\": \"repeat_check\"\n",
    "            })\n",
    "            self.overlap_store.save(obj)\n",
    "            next_check = current_time + self.repeat_interval_seconds\n",
    "            state = self.scheduling_store.get_token_state(token_mint)\n",
    "            check_count = state.get(\"total_checks_completed\", 0) + 1\n",
    "            self.scheduling_store.update_token_state(token_mint, {\n",
    "                \"last_completed_check\": current_time,\n",
    "                \"next_scheduled_check\": next_check,\n",
    "                \"total_checks_completed\": check_count\n",
    "            })\n",
    "            asyncio.create_task(self._schedule_repeat_checks_for_token(token_start, next_check))\n",
    "            if self.debug:\n",
    "                print(f\"_schedule_repeat_check_only: completed repeat check #{check_count} for {token_mint}, grade: {res.get('grade', 'N/A')}\")\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"_schedule_repeat_check_only: error for {token_mint}: {e}\")\n",
    "\n",
    "    async def _schedule_repeat_checks_for_token(self, start: TradingStart, first_check_at: int):\n",
    "        current_time = int(datetime.now(timezone.utc).timestamp())\n",
    "        launch_time = start.block_time or current_time\n",
    "        stop_after = launch_time + 24 * 3600\n",
    "        check_time = first_check_at\n",
    "        while check_time < stop_after:\n",
    "            delay = max(0, check_time - int(datetime.now(timezone.utc).timestamp()))\n",
    "            if delay > 0:\n",
    "                await asyncio.sleep(delay)\n",
    "            now = int(datetime.now(timezone.utc).timestamp())\n",
    "            if now >= stop_after:\n",
    "                if self.debug:\n",
    "                    print(f\"_schedule_repeat_checks: token {start.mint} past 24h -> stopping\")\n",
    "                self.scheduling_store.update_token_state(start.mint, {\n",
    "                    \"status\": \"completed\",\n",
    "                    \"completed_at\": datetime.now(timezone.utc).isoformat()\n",
    "                })\n",
    "                break\n",
    "            try:\n",
    "                res = await self.check_holders_overlap(start)\n",
    "                obj = self.overlap_store.load() or {}\n",
    "                obj.setdefault(start.mint, []).append({\n",
    "                    \"ts\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"result\": res,\n",
    "                    \"check_type\": \"repeat_check\"\n",
    "                })\n",
    "                self.overlap_store.save(obj)\n",
    "                state = self.scheduling_store.get_token_state(start.mint)\n",
    "                check_count = state.get(\"total_checks_completed\", 0) + 1\n",
    "                next_check_time = now + self.repeat_interval_seconds\n",
    "                self.scheduling_store.update_token_state(start.mint, {\n",
    "                    \"last_completed_check\": now,\n",
    "                    \"next_scheduled_check\": next_check_time,\n",
    "                    \"total_checks_completed\": check_count\n",
    "                })\n",
    "                if self.debug:\n",
    "                    print(f\"_schedule_repeat_checks: completed check #{check_count} for {start.mint}, grade: {res.get('grade', 'N/A')}\")\n",
    "                check_time = next_check_time\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"_schedule_repeat_checks: error for {start.mint}: {e}\")\n",
    "                check_time = now + self.repeat_interval_seconds\n",
    "\n",
    "    async def poll_coingecko_loop(self):\n",
    "        if self.debug:\n",
    "            print(\"Monitor: starting CoinGecko poll loop\")\n",
    "        await self.startup_recovery()\n",
    "        while True:\n",
    "            try:\n",
    "                starts = await self.token_discovery.get_tokens_created_today(limit=500)\n",
    "                if self.debug:\n",
    "                    print(f\"Monitor: CoinGecko returned {len(starts)} tokens\")\n",
    "                new_tokens_scheduled = 0\n",
    "                for s in starts:\n",
    "                    if not s.mint:\n",
    "                        continue\n",
    "                    if s.mint in self._scheduled:\n",
    "                        continue\n",
    "                    existing_state = self.scheduling_store.get_token_state(s.mint)\n",
    "                    if existing_state:\n",
    "                        if self.debug:\n",
    "                            print(f\"Monitor: token {s.mint} already has scheduling state, skipping\")\n",
    "                        continue\n",
    "                    asyncio.create_task(self._schedule_overlap_checks_for_token(s))\n",
    "                    self._scheduled.add(s.mint)\n",
    "                    new_tokens_scheduled += 1\n",
    "                    current_time = int(datetime.now(timezone.utc).timestamp())\n",
    "                    self.scheduling_store.update_token_state(s.mint, {\n",
    "                        \"launch_time\": s.block_time or current_time,\n",
    "                        \"first_check_at\": (s.block_time or current_time) + self.initial_check_delay_seconds,\n",
    "                        \"status\": \"pending_first\",\n",
    "                        \"created_at\": datetime.now(timezone.utc).isoformat(),\n",
    "                        \"total_checks_completed\": 0\n",
    "                    })\n",
    "                if new_tokens_scheduled > 0 and self.debug:\n",
    "                    print(f\"Monitor: scheduled overlap checks for {new_tokens_scheduled} new tokens\")\n",
    "                try:\n",
    "                    await self.updater.save_trading_starts_async(starts, skip_existing=True)\n",
    "                except Exception as e:\n",
    "                    if self.debug:\n",
    "                        print(\"Monitor: updater save error\", e)\n",
    "                current_time = time.time()\n",
    "                if current_time - self.last_cleanup > 3600:\n",
    "                    await self.updater.cleanup_old_tokens_async()\n",
    "                    self.scheduling_store.cleanup_old_states()\n",
    "                    self.last_cleanup = current_time\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(\"Monitor: CoinGecko poll error\", e)\n",
    "                    traceback.print_exc()\n",
    "            await asyncio.sleep(self.coingecko_poll_interval_seconds)\n",
    "\n",
    "    async def _schedule_overlap_checks_for_token(self, start: TradingStart):\n",
    "        now_ts = int(datetime.now(timezone.utc).timestamp())\n",
    "        block_ts = int(start.block_time or now_ts)\n",
    "        first_run_at = block_ts + self.initial_check_delay_seconds\n",
    "        to_sleep = max(0, first_run_at - now_ts)\n",
    "        if self.debug:\n",
    "            print(f\"_schedule: token={start.mint} will first run in {to_sleep}s (at {datetime.fromtimestamp(first_run_at, timezone.utc)})\")\n",
    "        await asyncio.sleep(to_sleep)\n",
    "        self.scheduling_store.update_token_state(start.mint, {\"status\": \"running_first_check\"})\n",
    "        stop_after = block_ts + 24 * 3600\n",
    "        check_count = 0\n",
    "        while True:\n",
    "            now_ts2 = int(datetime.now(timezone.utc).timestamp())\n",
    "            if now_ts2 > stop_after:\n",
    "                if self.debug:\n",
    "                    print(f\"_schedule: token={start.mint} past 24h -> stopping scheduled checks\")\n",
    "                self.scheduling_store.update_token_state(start.mint, {\n",
    "                    \"status\": \"completed\",\n",
    "                    \"completed_at\": datetime.now(timezone.utc).isoformat()\n",
    "                })\n",
    "                break\n",
    "            try:\n",
    "                res = await self.check_holders_overlap(start)\n",
    "                check_count += 1\n",
    "                obj = self.overlap_store.load() or {}\n",
    "                obj.setdefault(start.mint, []).append({\n",
    "                    \"ts\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"result\": res,\n",
    "                    \"check_type\": \"first_check\" if check_count == 1 else \"repeat_check\"\n",
    "                })\n",
    "                self.overlap_store.save(obj)\n",
    "                next_check_time = now_ts2 + self.repeat_interval_seconds\n",
    "                self.scheduling_store.update_token_state(start.mint, {\n",
    "                    \"status\": \"active\",\n",
    "                    \"last_completed_check\": now_ts2,\n",
    "                    \"next_scheduled_check\": next_check_time,\n",
    "                    \"total_checks_completed\": check_count\n",
    "                })\n",
    "                if self.debug:\n",
    "                    print(f\"_schedule: completed check #{check_count} for {start.mint}, grade: {res.get('grade', 'N/A')}, overlap: {res.get('overlap_count', 0)} wallets\")\n",
    "            except Exception as e:\n",
    "                if self.debug:\n",
    "                    print(f\"_schedule: overlap check error for {start.mint}: {e}\")\n",
    "                self.scheduling_store.update_token_state(start.mint, {\n",
    "                    \"last_error\": str(e),\n",
    "                    \"last_error_at\": datetime.now(timezone.utc).isoformat()\n",
    "                })\n",
    "            await asyncio.sleep(self.repeat_interval_seconds)\n",
    "\n",
    "    async def check_holders_overlap(self, start: TradingStart, top_k_holders: int = 200) -> Dict[str, Any]:\n",
    "        if self.debug:\n",
    "            print(f\"check_holders_overlap: computing for {start.mint}\")\n",
    "        await self.ensure_dune_holders()\n",
    "        mapping, _ = self.dune_cache.load_mapping()\n",
    "        union_yesterday_wallets: Set[str] = set()\n",
    "        for holders in mapping.values():\n",
    "            union_yesterday_wallets.update(holders)\n",
    "        if self.debug:\n",
    "            print(f\"check_holders_overlap: union of yesterday winners has {len(union_yesterday_wallets)} unique wallets\")\n",
    "        try:\n",
    "            holders_list = await self.holder_agg.get_token_holders(start.mint, limit=1000, max_pages=2, decimals=None)\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"check_holders_overlap: failed to fetch holders for {start.mint}: {e}\")\n",
    "            return {\"error\": \"fetch_holders_failed\", \"error_details\": str(e)}\n",
    "        top_holders = [h.get(\"wallet\") for h in holders_list[:top_k_holders] if h.get(\"wallet\")]\n",
    "        top_set = set(top_holders)\n",
    "        overlap = top_set.intersection(union_yesterday_wallets)\n",
    "        overlap_count = len(overlap)\n",
    "        top_count = len(top_set)\n",
    "        total_winner_wallets = len(union_yesterday_wallets)\n",
    "        overlap_pct = (overlap_count / top_count * 100.0) if top_count > 0 else 0.0\n",
    "        concentration = (overlap_count / total_winner_wallets * 100.0) if total_winner_wallets > 0 else 0.0\n",
    "        grade = calculate_overlap_grade(overlap_count, overlap_pct, concentration, top_count, total_winner_wallets)\n",
    "        summary = {\n",
    "            \"token\": start.mint,\n",
    "            \"checked_at\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"top_holders_checked\": top_count,\n",
    "            \"overlap_count\": overlap_count,\n",
    "            \"overlap_percentage\": round(overlap_pct, 2),\n",
    "            \"concentration\": round(concentration, 2),\n",
    "            \"total_winner_wallets\": total_winner_wallets,\n",
    "            \"grade\": grade,\n",
    "            \"sample_overlap\": list(overlap)[:20],\n",
    "            \"detected_via\": start.detected_via,\n",
    "            \"block_time\": start.block_time,\n",
    "            \"token_metadata\": {\n",
    "                k: v for k, v in {\n",
    "                    \"name\": start.extra.get(\"name\") if start.extra else None,\n",
    "                    \"fdv_usd\": start.fdv_usd,\n",
    "                    \"volume_usd\": start.volume_usd,\n",
    "                    \"source_dex\": start.source_dex,\n",
    "                    \"price_change_percentage\": start.price_change_percentage,\n",
    "                }.items() if v is not None\n",
    "            }\n",
    "        }\n",
    "        if self.debug:\n",
    "            print(f\"check_holders_overlap: {start.mint} overlap {overlap_count}/{top_count} ({overlap_pct:.2f}%) concentration {concentration:.2f}% grade={grade}\")\n",
    "        return summary\n",
    "\n",
    "async def main_loop():\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    HELIUS_API_KEY = os.environ.get(\"HELIUS_API_KEY\")\n",
    "    COINGECKO_PRO_API_KEY = os.environ.get(\"GECKO_API\")\n",
    "    # COINGECKO_PRO_API_KEY=\n",
    "    DUNE_API_KEY = os.environ.get(\"DUNE_API_KEY\")\n",
    "    DUNE_QUERY_ID = int(os.environ.get(\"DUNE_QUERY_ID\") or 5668844)\n",
    "    BASE_URL = f\"https://mainnet.helius-rpc.com/?api-key={HELIUS_API_KEY}\"\n",
    "    sol_client = SolanaAlphaClient(BASE_URL)\n",
    "    ok = await sol_client.test_connection()\n",
    "    print(\"Solana RPC ok:\", ok)\n",
    "    td = TokenDiscovery(\n",
    "        client=sol_client,\n",
    "        coingecko_pro_api_key=COINGECKO_PRO_API_KEY,\n",
    "        dune_api_key=DUNE_API_KEY,\n",
    "        dune_query_id=DUNE_QUERY_ID,\n",
    "        dune_cache_file=\"./data/dune_recent.pkl\",\n",
    "        timestamp_cache_file=\"./data/last_timestamp.pkl\",\n",
    "        debug=True\n",
    "    )\n",
    "    holder_agg = HolderAggregator(sol_client)\n",
    "    updater = JobLibTokenUpdater(data_dir=\"./data/token_data\", expiry_hours=24, debug=True)\n",
    "    dune_cache = DuneHolderCache(cache_file=\"./data/dune_holders.pkl\", cache_max_days=7, debug=True)\n",
    "    overlap_store = OverlapStore(filepath=\"./data/overlap_results.pkl\", debug=True)\n",
    "    scheduling_store = SchedulingStore(filepath=\"./data/scheduling_state.pkl\", debug=True)\n",
    "    monitor = Monitor(\n",
    "        sol_client=sol_client,\n",
    "        token_discovery=td,\n",
    "        holder_agg=holder_agg,\n",
    "        updater=updater,\n",
    "        dune_cache=dune_cache,\n",
    "        overlap_store=overlap_store,\n",
    "        scheduling_store=scheduling_store,\n",
    "        coingecko_poll_interval_seconds=30,\n",
    "        initial_check_delay_seconds=2 * 3600,\n",
    "        repeat_interval_seconds=6 * 3600,\n",
    "        debug=True,\n",
    "    )\n",
    "    await monitor.ensure_dune_holders()\n",
    "    await monitor.poll_coingecko_loop()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         asyncio.run(main_loop())\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"Interrupted, exiting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(main_loop())\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted, exiting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e50bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_fetch():\n",
    "    td = TokenDiscovery(coingecko_pro_api_key=os.environ[\"GECKO_API\"], debug=True)\n",
    "    pools = await td._fetch_coingecko_new_pools(limit=20)\n",
    "    print(\"Raw pools fetched:\", len(pools))\n",
    "    for p in pools[:3]:  # show first 3\n",
    "        print(json.dumps(p, indent=2)[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e0aab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenDiscovery initialized with CoinGecko Pro\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m test_fetch()\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mtest_fetch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_fetch\u001b[39m():\n\u001b[0;32m      2\u001b[0m     td \u001b[38;5;241m=\u001b[39m TokenDiscovery(coingecko_pro_api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGECKO_API\u001b[39m\u001b[38;5;124m\"\u001b[39m], debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m     pools \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m td\u001b[38;5;241m.\u001b[39m_fetch_coingecko_new_pools(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw pools fetched:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(pools))\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pools[:\u001b[38;5;241m3\u001b[39m]:  \u001b[38;5;66;03m# show first 3\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 299\u001b[0m, in \u001b[0;36mTokenDiscovery._fetch_coingecko_new_pools\u001b[1;34m(self, limit, timeout)\u001b[0m\n\u001b[0;32m    297\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoingecko_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m sess:\n\u001b[1;32m--> 299\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m sess\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout) \u001b[38;5;28;01mas\u001b[39;00m resp:\n\u001b[0;32m    300\u001b[0m         resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    301\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\aiohttp\\client.py:1488\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[1;32m-> 1488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp: _RetType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\aiohttp\\client.py:770\u001b[0m, in \u001b[0;36mClientSession._request\u001b[1;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size, middlewares)\u001b[0m\n\u001b[0;32m    767\u001b[0m     handler \u001b[38;5;241m=\u001b[39m _connect_and_send_request\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 770\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m handler(req)\n\u001b[0;32m    771\u001b[0m \u001b[38;5;66;03m# Client connector errors should not be retried\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[0;32m    773\u001b[0m     ConnectionTimeoutError,\n\u001b[0;32m    774\u001b[0m     ClientConnectorError,\n\u001b[0;32m    775\u001b[0m     ClientConnectorCertificateError,\n\u001b[0;32m    776\u001b[0m     ClientConnectorSSLError,\n\u001b[0;32m    777\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\aiohttp\\client.py:748\u001b[0m, in \u001b[0;36mClientSession._request.<locals>._connect_and_send_request\u001b[1;34m(req)\u001b[0m\n\u001b[0;32m    746\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m req\u001b[38;5;241m.\u001b[39msend(conn)\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 748\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstart(conn)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    750\u001b[0m     resp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\aiohttp\\client_reqrep.py:532\u001b[0m, in \u001b[0;36mClientResponse.start\u001b[1;34m(self, connection)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\n\u001b[1;32m--> 532\u001b[0m     message, payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m http\u001b[38;5;241m.\u001b[39mHttpProcessingError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[0;32m    535\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[0;32m    536\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    539\u001b[0m         headers\u001b[38;5;241m=\u001b[39mexc\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    540\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\aiohttp\\streams.py:672\u001b[0m, in \u001b[0;36mDataQueue.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_future()\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 672\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio\u001b[38;5;241m.\u001b[39mCancelledError, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = await test_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39815bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "degen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
