{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d211a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSupabase Dataset Retrieval and DataFrame Creation Notebook\\n==========================================================\\nThis notebook retrieves token snapshot datasets from Supabase storage\\nand creates a pandas DataFrame with one row per token containing daily data.\\n\\nRequirements:\\npip install supabase pandas python-dotenv requests\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Supabase Dataset Retrieval and DataFrame Creation Notebook\n",
    "==========================================================\n",
    "This notebook retrieves token snapshot datasets from Supabase storage\n",
    "and creates a pandas DataFrame with one row per token containing daily data.\n",
    "\n",
    "Requirements:\n",
    "pip install supabase pandas python-dotenv requests\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# # Supabase Token Dataset Loader\n",
    "# \n",
    "# This notebook:\n",
    "# 1. Connects to Supabase storage\n",
    "# 2. Lists all dataset files from the analytics/snapshots folder\n",
    "# 3. Downloads and parses JSON snapshot files\n",
    "# 4. Creates a pandas DataFrame with one row per token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6833ce5",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71ea3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf124fe",
   "metadata": {},
   "source": [
    "\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e20568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to Supabase: https://ldraroaloinsesjoayxc.supabase.co\n"
     ]
    }
   ],
   "source": [
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "BUCKET_NAME = os.getenv(\"SUPABASE_BUCKET\", \"monitor-data\")\n",
    "DATASET_DIR_REMOTE = os.getenv(\"DATASET_DIR_REMOTE\", \"datasets\")\n",
    "\n",
    "if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "    raise RuntimeError(\"âŒ Missing SUPABASE_URL or SUPABASE_KEY in environment variables\")\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "print(f\"âœ… Connected to Supabase: {SUPABASE_URL}\")\n",
    "\n",
    "global _file_cache_headers\n",
    "_file_cache_headers: Dict[str, Dict[str, str]] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435f687",
   "metadata": {},
   "source": [
    "\n",
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3fb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download function using signed URLs\n",
    "def download_json_from_supabase(remote_path: str, local_dataset_root: str = \"./dataset\") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Download and parse a JSON file from Supabase using a signed URL,\n",
    "    with conditional GET (ETag/If-Modified-Since) caching.\n",
    "    \n",
    "    Saves to and loads from a local 'dataset' folder to avoid refetching.\n",
    "\n",
    "    Args:\n",
    "        remote_path: Path to file in Supabase storage \n",
    "                     (e.g., 'mint_discovery/2025-01-15/token.json')\n",
    "        local_dataset_root: The local root directory to save/load cached files.\n",
    "    \n",
    "    Returns:\n",
    "        Parsed JSON as dict, or None if error.\n",
    "    \"\"\"\n",
    "    global _file_cache_headers\n",
    "\n",
    "    # 1. Determine local save path\n",
    "    save_path = os.path.join(local_dataset_root, remote_path)\n",
    "    file_content_bytes: Optional[bytes] = None\n",
    "\n",
    "    try:\n",
    "        # 2. Generate a signed URL\n",
    "        signed_url_response = supabase.storage.from_(BUCKET_NAME).create_signed_url(remote_path, 60)\n",
    "        signed_url = signed_url_response.get('signedURL')\n",
    "        \n",
    "        if not signed_url:\n",
    "            print(f\"âŒ Could not generate signed URL for '{remote_path}'\")\n",
    "            # If we can't get a URL, check the local cache as a fallback\n",
    "            if os.path.exists(save_path):\n",
    "                print(f\"Loading from local cache as fallback: '{save_path}'\")\n",
    "                with open(save_path, \"rb\") as f:\n",
    "                    file_content_bytes = f.read()\n",
    "            else:\n",
    "                return None  # No URL, no local file\n",
    "        \n",
    "        if not file_content_bytes:\n",
    "            # 3. Prepare headers for conditional GET\n",
    "            headers = {}\n",
    "            cached_headers = _file_cache_headers.get(remote_path, {})\n",
    "            if cached_headers.get('Last-Modified'):\n",
    "                headers['If-Modified-Since'] = cached_headers['Last-Modified']\n",
    "            if cached_headers.get('ETag'):\n",
    "                headers['If-None-Match'] = cached_headers['ETag']\n",
    "\n",
    "            # 4. Perform the HTTP request\n",
    "            response = requests.get(signed_url, headers=headers, timeout=15)\n",
    "\n",
    "            # 5. Handle the response\n",
    "            if response.status_code == 304:\n",
    "                # 304 Not Modified\n",
    "                print(f\"File '{remote_path}': No change detected (304 Not Modified).\")\n",
    "                if os.path.exists(save_path):\n",
    "                    print(f\"Loading from local cache: '{save_path}'\")\n",
    "                    with open(save_path, \"rb\") as f:\n",
    "                        file_content_bytes = f.read()\n",
    "                else:\n",
    "                    # File not modified, but local copy is missing. Force re-download.\n",
    "                    print(f\"File '{remote_path}' not modified, but local file '{save_path}' missing. Forcing re-download.\")\n",
    "                    headers.pop('If-Modified-Since', None)\n",
    "                    headers.pop('If-None-Match', None)\n",
    "                    _file_cache_headers.pop(remote_path, None)\n",
    "                    response = requests.get(signed_url, headers=headers, timeout=15)\n",
    "                    # Allow to fall through to 200 logic if it's 200 now\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                # 200 OK: File is new or has been updated\n",
    "                print(f\"File '{remote_path}': File updated â€” new data loaded.\")\n",
    "                file_content_bytes = response.content\n",
    "\n",
    "                # Update our cache with the new 'Last-Modified' and 'ETag' headers\n",
    "                new_last_modified = response.headers.get('Last-Modified')\n",
    "                new_etag = response.headers.get('ETag')\n",
    "                \n",
    "                new_headers_to_cache = {}\n",
    "                if new_last_modified:\n",
    "                    new_headers_to_cache['Last-Modified'] = new_last_modified\n",
    "                if new_etag:\n",
    "                    new_headers_to_cache['ETag'] = new_etag\n",
    "                    \n",
    "                if new_headers_to_cache:\n",
    "                    _file_cache_headers[remote_path] = new_headers_to_cache\n",
    "                    # print(f\"File '{remote_path}': Updated cache headers.\") # Optional: for more verbose logging\n",
    "\n",
    "                # Save the new file content locally\n",
    "                os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(file_content_bytes)\n",
    "                print(f\"Downloaded and saved '{remote_path}' -> '{save_path}'\")\n",
    "\n",
    "            elif response.status_code != 304: # if it wasn't 200 or 304\n",
    "                # Handle other errors (404 Not Found, 403 Forbidden, 500, etc.)\n",
    "                print(f\"File '{remote_path}': Error fetching file. Status: {response.status_code}, Response: {response.text[:100]}...\")\n",
    "                if os.path.exists(save_path):\n",
    "                    print(f\"Loading from local cache as fallback: '{save_path}'\")\n",
    "                    with open(save_path, \"rb\") as f:\n",
    "                        file_content_bytes = f.read()\n",
    "                else:\n",
    "                    return None # No file, no cache.\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"File '{remote_path}': Network or request error. {e}\")\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"Loading from local cache as fallback: '{save_path}'\")\n",
    "            with open(save_path, \"rb\") as f:\n",
    "                file_content_bytes = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"File '{remote_path}': An unexpected error occurred. {e}\")\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"Loading from local cache as fallback: '{save_path}'\")\n",
    "            with open(save_path, \"rb\") as f:\n",
    "                file_content_bytes = f.read()\n",
    "\n",
    "    # 6. If we have bytes (from download or cache), parse them\n",
    "    if file_content_bytes:\n",
    "        try:\n",
    "            return json.loads(file_content_bytes.decode('utf-8'))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ Failed to parse JSON from '{remote_path}' (local: '{save_path}'): {e}\")\n",
    "            return None\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"âŒ Failed to decode UTF-8 from '{remote_path}' (local: '{save_path}'): {e}\")\n",
    "            return None\n",
    "    \n",
    "    # If we reach here, all attempts failed\n",
    "    print(f\"âŒ Failed to retrieve or load '{remote_path}'\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a724590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List files function\n",
    "def list_dataset_files(pipeline: str = None, date_folder: str = None, limit: int = 1000) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    List all dataset files in Supabase storage.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Filter by pipeline (e.g., 'mint_discovery', 'mint_alpha')\n",
    "        date_folder: Filter by date folder (e.g., '2025-01-15')\n",
    "        limit: Maximum number of files to return\n",
    "    \n",
    "    Returns:\n",
    "        List of file metadata dicts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pipeline and date_folder:\n",
    "            folder = f\"{DATASET_DIR_REMOTE}/{pipeline}/{date_folder}\"\n",
    "        elif pipeline:\n",
    "            folder = f\"{DATASET_DIR_REMOTE}/{pipeline}\"\n",
    "        else:\n",
    "            folder = DATASET_DIR_REMOTE\n",
    "        \n",
    "        files = supabase.storage.from_(BUCKET_NAME).list(\n",
    "            folder,\n",
    "            {\"limit\": limit, \"sortBy\": {\"column\": \"created_at\", \"order\": \"desc\"}}\n",
    "        )\n",
    "        \n",
    "        return files if files else []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error listing files in {folder}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e94b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_dataset_files(pipeline: str = None, date_folder: str = None, include_expired: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively list all dataset JSON files across all date folders.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Filter by pipeline ('discovery' or 'alpha'). If None, gets both.\n",
    "        date_folder: Filter by specific date (e.g., '2025-01-15'). If None, gets all dates.\n",
    "        include_expired: Whether to include files from 'expired_no_label' folder. Default False.\n",
    "    \n",
    "    Returns:\n",
    "        List of full file paths\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    \n",
    "    try:\n",
    "        # Define pipelines: 'discovery' and 'alpha' folders\n",
    "        if pipeline:\n",
    "            pipelines = [pipeline]\n",
    "        else:\n",
    "            pipelines = ['discovery', 'alpha']\n",
    "        \n",
    "        print(f\"ðŸ“‚ Scanning pipelines: {pipelines}\")\n",
    "        \n",
    "        # For each pipeline, list date folders\n",
    "        for pipe in pipelines:\n",
    "            pipeline_path = f\"{DATASET_DIR_REMOTE}/{pipe}\"\n",
    "            \n",
    "            try:\n",
    "                # List all folders in this pipeline (these are the daily folders)\n",
    "                date_folders = supabase.storage.from_(BUCKET_NAME).list(pipeline_path)\n",
    "                \n",
    "                print(f\"\\n  ðŸ“… Pipeline '{pipe}' - Found {len(date_folders)} folders\")\n",
    "                \n",
    "                # Filter date folders if specified\n",
    "                if date_folder:\n",
    "                    date_folders = [f for f in date_folders if f.get('name') == date_folder]\n",
    "                    print(f\"     Filtered to date: {date_folder}\")\n",
    "                \n",
    "                for date_item in date_folders:\n",
    "                    folder_name = date_item.get('name')\n",
    "                    if not folder_name:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip 'expired_no_label' folder unless explicitly requested\n",
    "                    if folder_name == 'expired_no_label' and not include_expired:\n",
    "                        print(f\"    â­ï¸  Skipping: {folder_name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    date_path = f\"{pipeline_path}/{folder_name}\"\n",
    "                    \n",
    "                    try:\n",
    "                        # List JSON files in this date folder\n",
    "                        files = supabase.storage.from_(BUCKET_NAME).list(date_path)\n",
    "                        \n",
    "                        # Filter for JSON files only (files won't have 'id' field or will have metadata)\n",
    "                        json_files = []\n",
    "                        for f in files:\n",
    "                            fname = f.get('name', '')\n",
    "                            if fname.endswith('.json'):\n",
    "                                # This is a file, not a folder\n",
    "                                json_files.append(f)\n",
    "                        \n",
    "                        print(f\"    ðŸ“ {folder_name}: {len(json_files)} JSON files\")\n",
    "                        \n",
    "                        for file in json_files:\n",
    "                            full_path = f\"{date_path}/{file['name']}\"\n",
    "                            all_files.append(full_path)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"    âŒ Error listing files in '{date_path}': {e}\")\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error scanning pipeline '{pipe}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nâœ… Total files found: {len(all_files)}\")\n",
    "        \n",
    "        # Show sample of files found\n",
    "        if all_files:\n",
    "            print(\"\\nðŸ“„ Sample files:\")\n",
    "            for i, file_path in enumerate(all_files[:3]):\n",
    "                print(f\"  {i+1}. {file_path}\")\n",
    "            if len(all_files) > 3:\n",
    "                print(f\"  ... and {len(all_files) - 3} more\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸  No files found. Showing folder structure for debugging:\")\n",
    "            try:\n",
    "                # Try to list what's actually in the datasets folder\n",
    "                for pipe in pipelines:\n",
    "                    pipeline_path = f\"{DATASET_DIR_REMOTE}/{pipe}\"\n",
    "                    pipeline_contents = supabase.storage.from_(BUCKET_NAME).list(pipeline_path)\n",
    "                    print(f\"\\nðŸ“‚ '{pipeline_path}' contains {len(pipeline_contents)} items:\")\n",
    "                    for item in pipeline_contents[:10]:  # Show first 10\n",
    "                        item_name = item.get('name')\n",
    "                        print(f\"  - {item_name}\")\n",
    "                        # Try to peek inside each item\n",
    "                        try:\n",
    "                            folder_path = f\"{pipeline_path}/{item_name}\"\n",
    "                            folder_contents = supabase.storage.from_(BUCKET_NAME).list(folder_path)\n",
    "                            json_count = len([f for f in folder_contents if f.get('name', '').endswith('.json')])\n",
    "                            total_count = len(folder_contents)\n",
    "                            print(f\"    â†’ Contains {total_count} items ({json_count} JSON files)\")\n",
    "                            if json_count > 0:\n",
    "                                print(f\"    â†’ Sample: {folder_contents[0].get('name', 'N/A')}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"    â†’ Error: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Debug failed: {e}\")\n",
    "        \n",
    "        return all_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error listing dataset files: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fd098",
   "metadata": {},
   "source": [
    "\n",
    "#### Data Retrieval and Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e44b9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "def flatten_snapshot_to_row(snapshot: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Flatten a snapshot JSON into a single row dict for DataFrame with derived features.\n",
    "    \n",
    "    This corrected function robustly handles data inconsistencies found in different\n",
    "    snapshot versions (e.g., 'old.json' vs. 'new.json') by falling back to raw\n",
    "    input sources if data is missing from the 'features' block.\n",
    "    \n",
    "    Args:\n",
    "        snapshot: Snapshot dict loaded from JSON\n",
    "    \n",
    "    Returns:\n",
    "        Flattened dict with all relevant fields and derived features\n",
    "    \"\"\"\n",
    "    \n",
    "    row = {}\n",
    "    \n",
    "    # Snapshot metadata\n",
    "    row['snapshot_id'] = snapshot.get('snapshot_id')\n",
    "    row['generated_at_utc'] = snapshot.get('generated_at_utc')\n",
    "\n",
    "    # --- Inputs (Base Data Sources) ---\n",
    "    inputs = snapshot.get('inputs', {})\n",
    "    signal_data = inputs.get('signal_data', {})\n",
    "    signal_data_result = signal_data.get('result', {})\n",
    "    security_data = signal_data_result.get('security', {})\n",
    "    rugcheck_data = security_data.get('rugcheck', {})\n",
    "    dexscreener_data = signal_data_result.get('dexscreener', {})\n",
    "    holiday_check = inputs.get('holiday_check', {})\n",
    "    \n",
    "    # Get rugcheck_raw for additional features (Corrected Path)\n",
    "    # This block is a sibling to 'signal_data' in the 'inputs'\n",
    "    rugcheck_raw_data = inputs.get('rugcheck_raw', {})\n",
    "    \n",
    "    # Get dexscreener_raw for market feature fallbacks\n",
    "    dexscreener_raw = inputs.get('dexscreener_raw', {})\n",
    "    dex_pairs = dexscreener_raw.get('pairs')\n",
    "    primary_dex_pair = dex_pairs[0] if dex_pairs and isinstance(dex_pairs, list) and len(dex_pairs) > 0 else {}\n",
    "    primary_dex_liquidity = primary_dex_pair.get('liquidity', {})\n",
    "    primary_dex_volume = primary_dex_pair.get('volume', {})\n",
    "    primary_dex_price_change = primary_dex_pair.get('priceChange', {})\n",
    "\n",
    "    # Inputs - signal_data\n",
    "    row['input_signal_data_ts'] = signal_data.get('ts')\n",
    "    row['input_signal_data_check_type'] = signal_data.get('check_type')\n",
    "    row['input_signal_data_security'] = signal_data.get('security')\n",
    "\n",
    "    # Inputs - signal_data.result\n",
    "    row['input_signal_data_result_mint'] = signal_data_result.get('mint')\n",
    "    row['input_signal_data_result_checked_at'] = signal_data_result.get('checked_at')\n",
    "    row['input_signal_data_result_overlap_count'] = signal_data_result.get('overlap_count')\n",
    "    row['input_signal_data_result_overlap_percentage'] = signal_data_result.get('overlap_percentage')\n",
    "    row['input_signal_data_result_concentration'] = signal_data_result.get('concentration')\n",
    "    row['input_signal_data_result_weighted_concentration'] = signal_data_result.get('weighted_concentration')\n",
    "    row['input_signal_data_result_total_winner_wallets'] = signal_data_result.get('total_winner_wallets')\n",
    "    row['input_signal_data_result_grade'] = signal_data_result.get('grade')\n",
    "    row['input_signal_data_result_needs_monitoring'] = signal_data_result.get('needs_monitoring')\n",
    "    row['input_signal_data_result_skip_reason'] = signal_data_result.get('skip_reason')\n",
    "\n",
    "    # Inputs - signal_data.result.security\n",
    "    row['input_security_rugcheck_passed'] = security_data.get('rugcheck_passed')\n",
    "    row['input_security_probation'] = security_data.get('probation')\n",
    "    row['input_security_probation_reason'] = security_data.get('probation_reason')\n",
    "\n",
    "    # Inputs - signal_data.result.security.rugcheck\n",
    "    row['input_rugcheck_rugged'] = rugcheck_data.get('rugged')\n",
    "    row['input_rugcheck_top1_holder_pct'] = rugcheck_data.get('top1_holder_pct')\n",
    "    row['input_rugcheck_holder_count'] = rugcheck_data.get('holder_count')\n",
    "    row['input_rugcheck_has_authorities'] = rugcheck_data.get('has_authorities')\n",
    "    row['input_rugcheck_creator_balance'] = rugcheck_data.get('creator_balance')\n",
    "    row['input_rugcheck_freeze_authority'] = rugcheck_data.get('freeze_authority')\n",
    "    row['input_rugcheck_mint_authority'] = rugcheck_data.get('mint_authority')\n",
    "    row['input_rugcheck_transfer_fee_pct'] = rugcheck_data.get('transfer_fee_pct')\n",
    "    row['input_rugcheck_lp_locked_pct'] = rugcheck_data.get('lp_locked_pct')\n",
    "    row['input_rugcheck_total_liquidity_usd'] = rugcheck_data.get('total_liquidity_usd')\n",
    "\n",
    "    # Inputs - signal_data.result.dexscreener\n",
    "    row['input_dexscreener_current_price_usd'] = dexscreener_data.get('current_price_usd')\n",
    "\n",
    "    # Inputs - holiday_check\n",
    "    row['input_holiday_check_is_holiday'] = holiday_check.get('is_holiday')\n",
    "\n",
    "    # --- Features (Robust Population with Fallbacks) ---\n",
    "    features = snapshot.get('features', {})\n",
    "    row['mint'] = features.get('mint', row['input_signal_data_result_mint'])\n",
    "    row['signal_source'] = features.get('signal_source')\n",
    "    row['grade'] = features.get('grade', row['input_signal_data_result_grade'])\n",
    "    row['checked_at_utc'] = features.get('checked_at_utc', row['input_signal_data_result_checked_at'])\n",
    "    row['checked_at_timestamp'] = features.get('checked_at_timestamp')\n",
    "    \n",
    "    # Time features\n",
    "    row['time_of_day_utc'] = features.get('time_of_day_utc')\n",
    "    row['day_of_week_utc'] = features.get('day_of_week_utc')\n",
    "    row['is_weekend_utc'] = features.get('is_weekend_utc')\n",
    "    row['is_public_holiday_any'] = features.get('is_public_holiday_any', row['input_holiday_check_is_holiday'])\n",
    "    \n",
    "    # Market features (with fallbacks to inputs.dexscreener_raw)\n",
    "    row['price_usd'] = features.get('price_usd')\n",
    "    if row['price_usd'] is None:\n",
    "        # Fallback to dexscreener_raw, then signal_data.dexscreener\n",
    "        row['price_usd'] = primary_dex_pair.get('priceUsd')\n",
    "        if row['price_usd'] is None:\n",
    "             row['price_usd'] = dexscreener_data.get('current_price_usd')\n",
    "        \n",
    "    row['fdv_usd'] = features.get('fdv_usd')\n",
    "    if row['fdv_usd'] is None:\n",
    "        row['fdv_usd'] = primary_dex_pair.get('fdv')\n",
    "\n",
    "    row['liquidity_usd'] = features.get('liquidity_usd')\n",
    "    if row['liquidity_usd'] is None:\n",
    "        row['liquidity_usd'] = primary_dex_liquidity.get('usd')\n",
    "        if row['liquidity_usd'] is None:\n",
    "            row['liquidity_usd'] = row['input_rugcheck_total_liquidity_usd']\n",
    "\n",
    "    row['volume_h24_usd'] = features.get('volume_h24_usd')\n",
    "    if row['volume_h24_usd'] is None:\n",
    "        row['volume_h24_usd'] = primary_dex_volume.get('h24')\n",
    "\n",
    "    row['price_change_h24_pct'] = features.get('price_change_h24_pct')\n",
    "    if row['price_change_h24_pct'] is None:\n",
    "        row['price_change_h24_pct'] = primary_dex_price_change.get('h24')\n",
    "\n",
    "    pair_created_at_ms = primary_dex_pair.get('pairCreatedAt')\n",
    "    pair_created_at_ts_from_dex = pair_created_at_ms / 1000 if pair_created_at_ms else None\n",
    "    row['pair_created_at_timestamp'] = features.get('pair_created_at_timestamp', pair_created_at_ts_from_dex)\n",
    "    \n",
    "    # Security features (with fallbacks to inputs.rugcheck_raw)\n",
    "    row['rugcheck_risk_level'] = features.get('rugcheck_risk_level') # No easy fallback\n",
    "    row['is_rugged'] = features.get('is_rugged', rugcheck_data.get('rugged'))\n",
    "    \n",
    "    row['has_mint_authority'] = features.get('has_mint_authority')\n",
    "    if row['has_mint_authority'] is None:\n",
    "        # Check both the simple input and the raw input\n",
    "        raw_mint_auth = rugcheck_raw_data.get('mintAuthority', row['input_rugcheck_mint_authority'])\n",
    "        row['has_mint_authority'] = raw_mint_auth is not None\n",
    "        \n",
    "    row['has_freeze_authority'] = features.get('has_freeze_authority')\n",
    "    if row['has_freeze_authority'] is None:\n",
    "        raw_freeze_auth = rugcheck_raw_data.get('freezeAuthority', row['input_rugcheck_freeze_authority'])\n",
    "        row['has_freeze_authority'] = raw_freeze_auth is not None\n",
    "\n",
    "    token_supply_from_raw = rugcheck_raw_data.get('token', {}).get('supply', 0)\n",
    "    creator_balance_from_raw = rugcheck_raw_data.get('creatorBalance', row['input_rugcheck_creator_balance'])\n",
    "    creator_balance_pct_from_raw = (creator_balance_from_raw / token_supply_from_raw) * 100 if token_supply_from_raw and token_supply_from_raw > 0 else 0\n",
    "    row['creator_balance_pct'] = features.get('creator_balance_pct', creator_balance_pct_from_raw)\n",
    "\n",
    "    top_holders_from_raw = rugcheck_raw_data.get('topHolders', [])\n",
    "    top_10_pct_from_raw = sum(h.get('pct', 0) for h in top_holders_from_raw[:10]) if top_holders_from_raw else 0\n",
    "    row['top_10_holders_pct'] = features.get('top_10_holders_pct', top_10_pct_from_raw)\n",
    "\n",
    "    lp_locked_pct_from_inputs = row['input_rugcheck_lp_locked_pct']\n",
    "    is_lp_locked_95_plus_from_inputs = lp_locked_pct_from_inputs >= 95 if lp_locked_pct_from_inputs is not None else None\n",
    "    row['is_lp_locked_95_plus'] = features.get('is_lp_locked_95_plus', is_lp_locked_95_plus_from_inputs)\n",
    "\n",
    "    total_lp_locked_usd_from_raw = rugcheck_raw_data.get('total_lp_usd') # From new.json structure\n",
    "    if total_lp_locked_usd_from_raw is None: # Fallback for old.json structure\n",
    "        markets_from_raw = rugcheck_raw_data.get('markets', [])\n",
    "        if markets_from_raw:\n",
    "            total_lp_locked_usd_from_raw = sum(m.get('lp', {}).get('lpLockedUSD', 0) for m in markets_from_raw)\n",
    "    row['total_lp_locked_usd'] = features.get('total_lp_locked_usd', total_lp_locked_usd_from_raw)\n",
    "    \n",
    "    # Derived features (with fallbacks)\n",
    "    token_age_at_signal_seconds_from_inputs = None\n",
    "    if row['checked_at_timestamp'] and row['pair_created_at_timestamp']:\n",
    "        token_age_at_signal_seconds_from_inputs = row['checked_at_timestamp'] - row['pair_created_at_timestamp']\n",
    "    row['token_age_at_signal_seconds'] = features.get('token_age_at_signal_seconds', token_age_at_signal_seconds_from_inputs)\n",
    "    \n",
    "    # --- Finalization ---\n",
    "    finalization = snapshot.get('finalization', {})\n",
    "    row['token_age_hours_at_signal'] = finalization.get('token_age_hours_at_signal')\n",
    "    row['is_new_token'] = finalization.get('is_new_token')\n",
    "    # Corrected: Fallback for different key names\n",
    "    row['finalize_window_hours'] = finalization.get('finalize_window_hours', finalization.get('tracking_duration_hours'))\n",
    "    row['finalize_deadline'] = finalization.get('finalize_deadline')\n",
    "    row['check_interval_minutes'] = finalization.get('check_interval_minutes')\n",
    "    row['next_check_at'] = finalization.get('next_check_at')\n",
    "    row['check_count'] = finalization.get('check_count')\n",
    "    row['finalization_status'] = finalization.get('finalization_status')\n",
    "    row['claimed_by'] = finalization.get('claimed_by')\n",
    "    row['claimed_at'] = finalization.get('claimed_at')\n",
    "    row['finalized_at'] = finalization.get('finalized_at')\n",
    "    \n",
    "    # --- Label ---\n",
    "    label = snapshot.get('label')\n",
    "    if label:\n",
    "        row['label_mint'] = label.get('mint')\n",
    "        row['label_signal_type'] = label.get('signal_type')\n",
    "        row['label_symbol'] = label.get('symbol')\n",
    "        row['label_name'] = label.get('name')\n",
    "        row['label_entry_price'] = label.get('entry_price')\n",
    "        row['label_entry_time'] = label.get('entry_time')\n",
    "        row['label_token_age_hours'] = label.get('token_age_hours')\n",
    "        row['label_pair_created_at'] = label.get('pair_created_at')\n",
    "        row['label_tracking_interval_seconds'] = label.get('tracking_interval_seconds')\n",
    "        row['label_tracking_duration_hours'] = label.get('tracking_duration_hours')\n",
    "        row['label_tracking_end_time'] = label.get('tracking_end_time')\n",
    "        row['label_current_price'] = label.get('current_price')\n",
    "        row['label_current_roi'] = label.get('current_roi')\n",
    "        row['label_ath_price'] = label.get('ath_price')\n",
    "        row['label_ath_roi'] = label.get('ath_roi')\n",
    "        row['label_ath_time'] = label.get('ath_time')\n",
    "        row['label_status'] = label.get('status')\n",
    "        row['label_hit_50_percent'] = label.get('hit_50_percent')\n",
    "        row['label_hit_50_percent_time'] = label.get('hit_50_percent_time')\n",
    "        row['label_time_to_ath_minutes'] = label.get('time_to_ath_minutes')\n",
    "        row['label_time_to_50_percent_minutes'] = label.get('time_to_50_percent_minutes')\n",
    "        row['label_last_price_check'] = label.get('last_price_check')\n",
    "        row['label_last_successful_price'] = label.get('last_successful_price')\n",
    "        row['label_consecutive_failures'] = label.get('consecutive_failures')\n",
    "        row['label_retry_start_time'] = label.get('retry_start_time')\n",
    "        row['label_final_price'] = label.get('final_price')\n",
    "        row['label_final_roi'] = label.get('final_roi') # Corrected: Was 'label_final_roi'\n",
    "        row['label_tracking_completed_at'] = label.get('tracking_completed_at')\n",
    "        row['label_win_pct'] = label.get('win_pct')\n",
    "        row['label_max_gain_pct'] = label.get('max_gain_pct')\n",
    "        row['label_max_drawdown_pct'] = label.get('max_drawdown_pct')\n",
    "        row['label_final_pnl_pct'] = label.get('final_pnl_pct')\n",
    "        row['label_duration_hours'] = label.get('duration_hours')\n",
    "    else:\n",
    "        # Set all label fields to None if label is missing\n",
    "        label_keys = [\n",
    "            'label_mint', 'label_signal_type', 'label_symbol', 'label_name',\n",
    "            'label_entry_price', 'label_entry_time', 'label_token_age_hours',\n",
    "            'label_pair_created_at', 'label_tracking_interval_seconds',\n",
    "            'label_tracking_duration_hours', 'label_tracking_end_time',\n",
    "            'label_current_price', 'label_current_roi', 'label_ath_price',\n",
    "            'label_ath_roi', 'label_ath_time', 'label_status',\n",
    "            'label_hit_50_percent', 'label_hit_50_percent_time',\n",
    "            'label_time_to_ath_minutes', 'label_time_to_50_percent_minutes',\n",
    "            'label_last_price_check', 'label_last_successful_price',\n",
    "            'label_consecutive_failures', 'label_retry_start_time',\n",
    "            'label_final_price', 'label_final_roi', 'label_tracking_completed_at',\n",
    "            'label_win_pct', 'label_max_gain_pct', 'label_max_drawdown_pct',\n",
    "            'label_final_pnl_pct', 'label_duration_hours'\n",
    "        ]\n",
    "        for key in label_keys:\n",
    "            row[key] = None\n",
    "    \n",
    "    # ========== ADDITIONAL FEATURES FROM RUGCHECK_RAW ==========\n",
    "    # These features pull from rugcheck_raw_data, which was defined robustly\n",
    "    \n",
    "    # Rugcheck score\n",
    "    row['rugcheck_score'] = rugcheck_raw_data.get('score')\n",
    "    row['rugcheck_score_normalized'] = rugcheck_raw_data.get('score_normalised')\n",
    "    \n",
    "    # Risk flags\n",
    "    risks = rugcheck_raw_data.get('risks', [])\n",
    "    row['risk_flag_count'] = len(risks)\n",
    "    row['has_critical_risk'] = any(r.get('level') == 'error' for r in risks) if risks else False\n",
    "    row['has_warning_risk'] = any(r.get('level') == 'warn' for r in risks) if risks else False\n",
    "    \n",
    "    # Market data from rugcheck_raw\n",
    "    markets = rugcheck_raw_data.get('markets', [])\n",
    "    row['number_of_markets'] = len(markets)\n",
    "    \n",
    "    # Check for specific market types\n",
    "    row['has_pump_fun_market'] = any(m.get('marketType') == 'pump_fun_amm' for m in markets)\n",
    "    row['has_meteora_market'] = any('meteora' in m.get('marketType', '') for m in markets)\n",
    "    row['has_raydium_market'] = any('raydium' in m.get('marketType', '') for m in markets)\n",
    "    \n",
    "    # LP Provider count\n",
    "    row['total_lp_providers'] = rugcheck_raw_data.get('totalLPProviders', 0)\n",
    "    \n",
    "    # Top holders data\n",
    "    # top_holders_from_raw already defined\n",
    "    if len(top_holders_from_raw) >= 3:\n",
    "        row['top_3_holders_pct'] = sum(h.get('pct', 0) for h in top_holders_from_raw[:3])\n",
    "    else:\n",
    "        row['top_3_holders_pct'] = None\n",
    "    \n",
    "    # Probation meta\n",
    "    probation_meta = rugcheck_raw_data.get('probation_meta', {})\n",
    "    row['probation_top_n_pct'] = probation_meta.get('top_n_pct')\n",
    "    row['probation_threshold_pct'] = probation_meta.get('threshold_pct')\n",
    "    \n",
    "    # Insider networks\n",
    "    insider_networks = rugcheck_raw_data.get('insiderNetworks') or [] # Corrected: Handle None\n",
    "    row['total_insider_networks'] = len(insider_networks)\n",
    "    \n",
    "    if insider_networks:\n",
    "        row['largest_insider_network_size'] = max(n.get('size', 0) for n in insider_networks) if insider_networks else 0\n",
    "        row['total_insider_token_amount'] = sum(n.get('tokenAmount', 0) for n in insider_networks)\n",
    "        row['total_insider_active_accounts'] = sum(n.get('activeAccounts', 0) for n in insider_networks)\n",
    "        \n",
    "        # Count network types\n",
    "        transfer_networks = [n for n in insider_networks if n.get('type') == 'transfer']\n",
    "        trade_networks = [n for n in insider_networks if n.get('type') == 'trade']\n",
    "        row['transfer_network_count'] = len(transfer_networks)\n",
    "        row['trade_network_count'] = len(trade_networks)\n",
    "        row['has_transfer_network'] = len(transfer_networks) > 0\n",
    "    else:\n",
    "        row['largest_insider_network_size'] = 0\n",
    "        row['total_insider_token_amount'] = 0\n",
    "        row['total_insider_active_accounts'] = 0\n",
    "        row['transfer_network_count'] = 0\n",
    "        row['trade_network_count'] = 0\n",
    "        row['has_transfer_network'] = False\n",
    "    \n",
    "    # Graph insiders detected\n",
    "    row['graph_insiders_detected'] = rugcheck_raw_data.get('graphInsidersDetected', 0)\n",
    "    \n",
    "    # Launchpad info\n",
    "    launchpad = rugcheck_raw_data.get('launchpad', {})\n",
    "    row['launchpad_platform'] = launchpad.get('platform')\n",
    "    row['is_pump_fun_launch'] = launchpad.get('platform') == 'pump_fun'\n",
    "    \n",
    "    # Token supply\n",
    "    token_info = rugcheck_raw_data.get('token', {})\n",
    "    row['token_supply'] = token_info.get('supply')\n",
    "    row['token_decimals'] = token_info.get('decimals')\n",
    "    \n",
    "    # ========== DERIVED FEATURES ==========\n",
    "    # This section now pulls from the robust 'row' dictionary\n",
    "    \n",
    "    # Safe division helper\n",
    "    def safe_divide(a, b, default=None):\n",
    "        if a is None or b is None or b == 0:\n",
    "            return default\n",
    "        return a / b\n",
    "    \n",
    "    # Smart Money & Overlap Metrics\n",
    "    overlap_count = row['input_signal_data_result_overlap_count'] or 0\n",
    "    weighted_concentration = row['input_signal_data_result_weighted_concentration'] or 0\n",
    "    concentration = row['input_signal_data_result_concentration'] or 0\n",
    "    # Corrected: Use 'or 1' to prevent division by zero\n",
    "    total_winner_wallets = row['input_signal_data_result_total_winner_wallets'] or 1\n",
    "    holder_count = row['input_rugcheck_holder_count'] or 1\n",
    "    \n",
    "    row['overlap_quality_score'] = safe_divide(\n",
    "        overlap_count * weighted_concentration, \n",
    "        total_winner_wallets,\n",
    "        0\n",
    "    )\n",
    "    row['winner_wallet_density'] = safe_divide(overlap_count, holder_count, 0)\n",
    "    row['smart_money_concentration_ratio'] = safe_divide(\n",
    "        weighted_concentration, \n",
    "        concentration if concentration > 0 else 1,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Insider Risk Metrics\n",
    "    token_supply = row.get('token_supply') or 0\n",
    "    if token_supply > 0:\n",
    "        row['insider_token_percentage'] = (row['total_insider_token_amount'] / token_supply) * 100\n",
    "    else:\n",
    "        row['insider_token_percentage'] = 0\n",
    "    \n",
    "    # Liquidity Metrics\n",
    "    total_liquidity = row['liquidity_usd'] or 0\n",
    "    row['liquidity_per_holder'] = safe_divide(total_liquidity, holder_count, 0)\n",
    "    \n",
    "    fdv_usd = row['fdv_usd'] or 0\n",
    "    row['liquidity_to_mcap_ratio'] = safe_divide(total_liquidity, fdv_usd, 0)\n",
    "    \n",
    "    # Volume Metrics\n",
    "    volume_h24 = row['volume_h24_usd'] or 0\n",
    "    row['volume_to_liquidity_ratio'] = safe_divide(volume_h24, total_liquidity, 0)\n",
    "    row['turnover_ratio'] = safe_divide(volume_h24, fdv_usd, 0)\n",
    "    \n",
    "    # Holder Concentration\n",
    "    top1_pct = row['input_rugcheck_top1_holder_pct'] or 0\n",
    "    top10_pct = row['top_10_holders_pct'] or 0\n",
    "    row['whale_concentration_score'] = (top1_pct * 3) + (top10_pct - top1_pct) if top10_pct >= top1_pct else top1_pct * 3\n",
    "    \n",
    "    # Calculate non-AMM top holder percentage (excluding AMM pools)\n",
    "    if top_holders_from_raw:\n",
    "        known_accounts = rugcheck_raw_data.get('knownAccounts', {})\n",
    "        non_amm_holders = [h for h in top_holders_from_raw if known_accounts.get(h.get('owner', ''), {}).get('type') != 'AMM']\n",
    "        if non_amm_holders:\n",
    "            row['non_amm_top1_holder_pct'] = non_amm_holders[0].get('pct', 0)\n",
    "        else:\n",
    "            row['non_amm_top1_holder_pct'] = 0\n",
    "    else:\n",
    "        row['non_amm_top1_holder_pct'] = 0\n",
    "    \n",
    "    # Token Age & Timing\n",
    "    checked_at_ts = row['checked_at_timestamp'] or 0\n",
    "    pair_created_ts = row['pair_created_at_timestamp'] or 0\n",
    "    \n",
    "    if checked_at_ts and pair_created_ts and checked_at_ts >= pair_created_ts:\n",
    "        time_since_creation_seconds = checked_at_ts - pair_created_ts\n",
    "        row['time_since_pair_creation_minutes'] = time_since_creation_seconds / 60\n",
    "        row['time_since_pair_creation_hours'] = time_since_creation_seconds / 3600\n",
    "    else:\n",
    "        row['time_since_pair_creation_minutes'] = None\n",
    "        row['time_since_pair_creation_hours'] = None\n",
    "    \n",
    "    # Token age categories\n",
    "    token_age_hours = row['token_age_hours_at_signal']\n",
    "    if token_age_hours is not None:\n",
    "        if token_age_hours < 1:\n",
    "            row['token_age_category'] = 'ultra_fresh'\n",
    "        elif token_age_hours < 6:\n",
    "            row['token_age_category'] = 'fresh'\n",
    "        elif token_age_hours < 24:\n",
    "            row['token_age_category'] = 'young'\n",
    "        else:\n",
    "            row['token_age_category'] = 'established'\n",
    "            \n",
    "        row['is_launch_hour_1'] = token_age_hours < 1\n",
    "        row['is_launch_day_1'] = token_age_hours < 24\n",
    "    else:\n",
    "        row['token_age_category'] = None\n",
    "        row['is_launch_hour_1'] = None\n",
    "        row['is_launch_day_1'] = None\n",
    "    \n",
    "    # Price Action Metrics\n",
    "    price_change_24h = row['price_change_h24_pct'] or 0\n",
    "    row['price_velocity'] = safe_divide(price_change_24h, 24, 0)\n",
    "    row['price_impact_score'] = safe_divide(fdv_usd, total_liquidity, 0)\n",
    "    \n",
    "    # Risk Scores\n",
    "    has_mint_auth = row['has_mint_authority']\n",
    "    has_freeze_auth = row['has_freeze_authority']\n",
    "    transfer_fee = row['input_rugcheck_transfer_fee_pct'] or 0\n",
    "    \n",
    "    row['authority_risk_score'] = (\n",
    "        (50 if has_mint_auth else 0) + \n",
    "        (50 if has_freeze_auth else 0) + \n",
    "        (transfer_fee * 100 if transfer_fee else 0)\n",
    "    )\n",
    "    \n",
    "    # Creator risk flags\n",
    "    creator_balance_pct = row['creator_balance_pct'] or 0\n",
    "    row['creator_balance_high_risk'] = creator_balance_pct > 5\n",
    "    row['creator_dumped'] = (creator_balance_pct == 0) and (token_age_hours is not None and token_age_hours < 24)\n",
    "    \n",
    "    # Concentration risk flags\n",
    "    row['suspicious_concentration'] = (top1_pct > 20) and (holder_count < 100)\n",
    "    \n",
    "    # Composite Pump & Dump Risk Score (weighted 0-100 scale)\n",
    "    pump_dump_components = []\n",
    "    \n",
    "    # Insider network size (0-30 points)\n",
    "    if row['largest_insider_network_size'] > 100:\n",
    "        pump_dump_components.append(30)\n",
    "    elif row['largest_insider_network_size'] > 50:\n",
    "        pump_dump_components.append(20)\n",
    "    elif row['largest_insider_network_size'] > 20:\n",
    "        pump_dump_components.append(10)\n",
    "    else:\n",
    "        pump_dump_components.append(0)\n",
    "    \n",
    "    # Holder concentration (0-25 points)\n",
    "    if top1_pct > 30:\n",
    "        pump_dump_components.append(25)\n",
    "    elif top1_pct > 20:\n",
    "        pump_dump_components.append(15)\n",
    "    elif top1_pct > 10:\n",
    "        pump_dump_components.append(8)\n",
    "    else:\n",
    "        pump_dump_components.append(0)\n",
    "    \n",
    "    # Liquidity lock (0-20 points, inverted - lower lock = higher risk)\n",
    "    lp_locked_pct = row['input_rugcheck_lp_locked_pct'] or 0\n",
    "    if lp_locked_pct < 50:\n",
    "        pump_dump_components.append(20)\n",
    "    elif lp_locked_pct < 80:\n",
    "        pump_dump_components.append(12)\n",
    "    elif lp_locked_pct < 95:\n",
    "        pump_dump_components.append(5)\n",
    "    else:\n",
    "        pump_dump_components.append(0)\n",
    "    \n",
    "    # Authority presence (0-15 points)\n",
    "    pump_dump_components.append(min(row['authority_risk_score'] / 100 * 15, 15))\n",
    "    \n",
    "    # Creator behavior (0-10 points)\n",
    "    if row['creator_dumped']:\n",
    "        pump_dump_components.append(10)\n",
    "    elif creator_balance_pct > 10:\n",
    "        pump_dump_components.append(7)\n",
    "    elif creator_balance_pct > 5:\n",
    "        pump_dump_components.append(4)\n",
    "    else:\n",
    "        pump_dump_components.append(0)\n",
    "    \n",
    "    row['pump_dump_risk_score'] = sum(pump_dump_components)\n",
    "    \n",
    "    # Sustainability Score (inverse of risk, 0-100 scale)\n",
    "    sustainability_components = []\n",
    "    \n",
    "    # LP locked (25 points)\n",
    "    if lp_locked_pct >= 95:\n",
    "        sustainability_components.append(25)\n",
    "    elif lp_locked_pct >= 80:\n",
    "        sustainability_components.append(15)\n",
    "    elif lp_locked_pct >= 50:\n",
    "        sustainability_components.append(8)\n",
    "    else:\n",
    "        sustainability_components.append(0)\n",
    "    \n",
    "    # No authorities (25 points)\n",
    "    if not has_mint_auth and not has_freeze_auth and transfer_fee == 0:\n",
    "        sustainability_components.append(25)\n",
    "    elif not has_mint_auth and not has_freeze_auth:\n",
    "        sustainability_components.append(20)\n",
    "    else:\n",
    "        sustainability_components.append(0)\n",
    "    \n",
    "    # Holder count (20 points)\n",
    "    if holder_count > 1000:\n",
    "        sustainability_components.append(20)\n",
    "    elif holder_count > 500:\n",
    "        sustainability_components.append(15)\n",
    "    elif holder_count > 200:\n",
    "        sustainability_components.append(10)\n",
    "    elif holder_count > 100:\n",
    "        sustainability_components.append(5)\n",
    "    else:\n",
    "        sustainability_components.append(0)\n",
    "    \n",
    "    # Volume/liquidity ratio (15 points)\n",
    "    vol_liq_ratio = row.get('volume_to_liquidity_ratio', 0)\n",
    "    if vol_liq_ratio is not None and 0.1 <= vol_liq_ratio <= 2.0:\n",
    "        sustainability_components.append(15)\n",
    "    elif vol_liq_ratio is not None and 0.05 <= vol_liq_ratio <= 5.0:\n",
    "        sustainability_components.append(10)\n",
    "    else:\n",
    "        sustainability_components.append(0)\n",
    "    \n",
    "    # Low insider risk (15 points)\n",
    "    if row['total_insider_networks'] == 0:\n",
    "        sustainability_components.append(15)\n",
    "    elif row['largest_insider_network_size'] < 20:\n",
    "        sustainability_components.append(10)\n",
    "    elif row['largest_insider_network_size'] < 50:\n",
    "        sustainability_components.append(5)\n",
    "    else:\n",
    "        sustainability_components.append(0)\n",
    "    \n",
    "    row['sustainability_score'] = sum(sustainability_components)\n",
    "    \n",
    "    # Temporal Context Features\n",
    "    hour = row['time_of_day_utc']\n",
    "    if hour is not None:\n",
    "        if 0 <= hour < 7:\n",
    "            row['hour_category'] = 'dead_hours'\n",
    "        elif 7 <= hour < 15:\n",
    "            row['hour_category'] = 'asia_hours'\n",
    "        elif 15 <= hour < 19:\n",
    "            row['hour_category'] = 'eu_hours'\n",
    "        else: # 19 <= hour < 24\n",
    "            row['hour_category'] = 'us_hours'\n",
    "            \n",
    "        row['is_crypto_prime_time'] = 14 <= hour <= 22\n",
    "    else:\n",
    "        row['hour_category'] = None\n",
    "        row['is_crypto_prime_time'] = None\n",
    "    \n",
    "    day_of_week = row['day_of_week_utc']\n",
    "    if day_of_week is not None:\n",
    "        row['is_first_day_of_week'] = day_of_week == 0  # Monday\n",
    "        row['is_last_day_of_week'] = day_of_week == 4  # Friday\n",
    "    else:\n",
    "        row['is_first_day_of_week'] = None\n",
    "        row['is_last_day_of_week'] = None\n",
    "    \n",
    "    # Average holding size\n",
    "    if token_supply and holder_count:\n",
    "        row['average_holding_size'] = token_supply / holder_count\n",
    "    else:\n",
    "        row['average_holding_size'] = None\n",
    "    \n",
    "    # Market structure metrics\n",
    "    if markets:\n",
    "        liquidity_values = []\n",
    "        for market in markets:\n",
    "            lp = market.get('lp', {})\n",
    "            # Use 'lpLockedUSD' as it's more reliable than total_liquidity_usd\n",
    "            lp_usd = lp.get('lpLockedUSD')\n",
    "            if lp_usd is None:\n",
    "                # Fallback for non-locked/other LP types\n",
    "                lp_usd = (lp.get('baseUSD', 0) or 0) + (lp.get('quoteUSD', 0) or 0)\n",
    "\n",
    "            if lp_usd and lp_usd > 0:\n",
    "                liquidity_values.append(lp_usd)\n",
    "        \n",
    "        total_market_liquidity = sum(liquidity_values)\n",
    "        if total_market_liquidity > 0:\n",
    "            row['primary_market_liquidity_pct'] = (max(liquidity_values) / total_market_liquidity) * 100\n",
    "            if len(liquidity_values) > 1:\n",
    "                row['liquidity_fragmentation_index'] = np.std(liquidity_values)\n",
    "            else:\n",
    "                row['liquidity_fragmentation_index'] = 0\n",
    "        else:\n",
    "            row['primary_market_liquidity_pct'] = None\n",
    "            row['liquidity_fragmentation_index'] = None\n",
    "    else:\n",
    "        row['primary_market_liquidity_pct'] = None\n",
    "        row['liquidity_fragmentation_index'] = None\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14910c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load datasets into DataFrame\n",
    "def load_datasets_to_dataframe(pipeline: str = None, date_folder: str = None, limit: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all dataset files from Supabase and create a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Filter by pipeline ('discovery' or 'alpha'). None = both.\n",
    "        date_folder: Filter by specific date folder (e.g., '2025-01-15'). None = all dates.\n",
    "        limit: Maximum number of files to process (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with one row per token snapshot\n",
    "    \"\"\"\n",
    "    print(\"ðŸ” Listing dataset files...\")\n",
    "    file_paths = list_all_dataset_files(pipeline=pipeline, date_folder=date_folder)\n",
    "    \n",
    "    if limit:\n",
    "        file_paths = file_paths[:limit]\n",
    "        print(f\"âš ï¸ Limited to {limit} files\")\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(\"âŒ No dataset files found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nðŸ“¥ Downloading and processing {len(file_paths)} files...\")\n",
    "    \n",
    "    rows = []\n",
    "    errors = 0\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Progress: {i + 1}/{len(file_paths)} files processed...\")\n",
    "        \n",
    "        snapshot = download_json_from_supabase(file_path)\n",
    "        \n",
    "        if snapshot:\n",
    "            try:\n",
    "                row = flatten_snapshot_to_row(snapshot)\n",
    "                rows.append(row)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error processing {file_path}: {e}\")\n",
    "                errors += 1\n",
    "        else:\n",
    "            errors += 1\n",
    "    \n",
    "    print(f\"\\nâœ… Successfully processed {len(rows)} snapshots ({errors} errors)\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Convert timestamps to datetime\n",
    "    if 'checked_at_utc' in df.columns:\n",
    "        df['checked_at_utc'] = pd.to_datetime(df['checked_at_utc'])\n",
    "    if 'generated_at_utc' in df.columns:\n",
    "        df['generated_at_utc'] = pd.to_datetime(df['generated_at_utc'])\n",
    "    if 'finalized_at' in df.columns:\n",
    "        df['finalized_at'] = pd.to_datetime(df['finalized_at'])\n",
    "    if 'label_tracking_completed_at' in df.columns:\n",
    "        df['label_tracking_completed_at'] = pd.to_datetime(df['label_tracking_completed_at'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac702739",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "662a8506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Listing dataset files...\n",
      "ðŸ“‚ Scanning pipelines: ['discovery', 'alpha']\n",
      "\n",
      "  ðŸ“… Pipeline 'discovery' - Found 1 folders\n",
      "    â­ï¸  Skipping: expired_no_label\n",
      "\n",
      "  ðŸ“… Pipeline 'alpha' - Found 6 folders\n",
      "    ðŸ“ 2025-11-03: 2 JSON files\n",
      "    ðŸ“ 2025-11-05: 1 JSON files\n",
      "    ðŸ“ 2025-11-06: 21 JSON files\n",
      "    ðŸ“ 2025-11-07: 24 JSON files\n",
      "    ðŸ“ 2025-11-08: 9 JSON files\n",
      "    â­ï¸  Skipping: expired_no_label\n",
      "\n",
      "âœ… Total files found: 57\n",
      "\n",
      "ðŸ“„ Sample files:\n",
      "  1. datasets/alpha/2025-11-03/2wTr7CbhVx85JeJjJ4aVsMFJKcvLwDJyfyK3SKd6pump_2025-11-03T14-48-04.181771_00-00.json\n",
      "  2. datasets/alpha/2025-11-03/5yVT7keCgESd6S523y6EwiVfJ3JoRj8f28JkZt2ipump_2025-11-03T15-03-11.343913_00-00.json\n",
      "  3. datasets/alpha/2025-11-05/4bcfWqNrGgvEy8NA3kWeCbZr46Qpd175hWbUXtkopump_2025-11-05T04-14-22.642681_00-00.json\n",
      "  ... and 54 more\n",
      "\n",
      "ðŸ“¥ Downloading and processing 57 files...\n",
      "File 'datasets/alpha/2025-11-03/2wTr7CbhVx85JeJjJ4aVsMFJKcvLwDJyfyK3SKd6pump_2025-11-03T14-48-04.181771_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-03/2wTr7CbhVx85JeJjJ4aVsMFJKcvLwDJyfyK3SKd6pump_2025-11-03T14-48-04.181771_00-00.json' -> './dataset\\datasets/alpha/2025-11-03/2wTr7CbhVx85JeJjJ4aVsMFJKcvLwDJyfyK3SKd6pump_2025-11-03T14-48-04.181771_00-00.json'\n",
      "File 'datasets/alpha/2025-11-03/5yVT7keCgESd6S523y6EwiVfJ3JoRj8f28JkZt2ipump_2025-11-03T15-03-11.343913_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-03/5yVT7keCgESd6S523y6EwiVfJ3JoRj8f28JkZt2ipump_2025-11-03T15-03-11.343913_00-00.json' -> './dataset\\datasets/alpha/2025-11-03/5yVT7keCgESd6S523y6EwiVfJ3JoRj8f28JkZt2ipump_2025-11-03T15-03-11.343913_00-00.json'\n",
      "File 'datasets/alpha/2025-11-05/4bcfWqNrGgvEy8NA3kWeCbZr46Qpd175hWbUXtkopump_2025-11-05T04-14-22.642681_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-05/4bcfWqNrGgvEy8NA3kWeCbZr46Qpd175hWbUXtkopump_2025-11-05T04-14-22.642681_00-00.json' -> './dataset\\datasets/alpha/2025-11-05/4bcfWqNrGgvEy8NA3kWeCbZr46Qpd175hWbUXtkopump_2025-11-05T04-14-22.642681_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/3MHQYPkMKoCk5Zt2RDtN8siaRBKuH2CQroLYsFGNpump_2025-11-06T23-17-42.108451_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/3MHQYPkMKoCk5Zt2RDtN8siaRBKuH2CQroLYsFGNpump_2025-11-06T23-17-42.108451_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/3MHQYPkMKoCk5Zt2RDtN8siaRBKuH2CQroLYsFGNpump_2025-11-06T23-17-42.108451_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-06T21-48-33.225354_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-06T21-48-33.225354_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-06T21-48-33.225354_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-06T23-17-07.525776_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-06T23-17-07.525776_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-06T23-17-07.525776_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/48Yd7G8UixJwBT4qF8C6TM9QgNHvcZPgG3dPn9fkpump_2025-11-06T23-09-42.479425_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/48Yd7G8UixJwBT4qF8C6TM9QgNHvcZPgG3dPn9fkpump_2025-11-06T23-09-42.479425_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/48Yd7G8UixJwBT4qF8C6TM9QgNHvcZPgG3dPn9fkpump_2025-11-06T23-09-42.479425_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-06T21-48-33.140534_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-06T21-48-33.140534_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-06T21-48-33.140534_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-06T23-12-04.655367_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-06T23-12-04.655367_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-06T23-12-04.655367_00-00.json'\n",
      "Progress: 10/57 files processed...\n",
      "File 'datasets/alpha/2025-11-06/7Mk8rfgdCZ2CF1tCoEcMy9uCFV3MkdfkdfyHmaU1pump_2025-11-06T06-12-19.581250_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/7Mk8rfgdCZ2CF1tCoEcMy9uCFV3MkdfkdfyHmaU1pump_2025-11-06T06-12-19.581250_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/7Mk8rfgdCZ2CF1tCoEcMy9uCFV3MkdfkdfyHmaU1pump_2025-11-06T06-12-19.581250_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-06T23-16-25.354242_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-06T23-16-25.354242_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-06T23-16-25.354242_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/BuhyqqURetnUuouatgLXWikrbUG93ShcxCJk27a7pump_2025-11-06T20-28-56.300273_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/BuhyqqURetnUuouatgLXWikrbUG93ShcxCJk27a7pump_2025-11-06T20-28-56.300273_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/BuhyqqURetnUuouatgLXWikrbUG93ShcxCJk27a7pump_2025-11-06T20-28-56.300273_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/BuhyqqURetnUuouatgLXWikrbUG93ShcxCJk27a7pump_2025-11-06T23-11-17.030907_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/BuhyqqURetnUuouatgLXWikrbUG93ShcxCJk27a7pump_2025-11-06T23-11-17.030907_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/BuhyqqURetnUuouatgLXWikrbUG93ShcxCJk27a7pump_2025-11-06T23-11-17.030907_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/CaHrnvS8y4qjosyeArb8PYgVUkK8Y6UXUdYg4BhPpump_2025-11-06T20-36-04.428389_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/CaHrnvS8y4qjosyeArb8PYgVUkK8Y6UXUdYg4BhPpump_2025-11-06T20-36-04.428389_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/CaHrnvS8y4qjosyeArb8PYgVUkK8Y6UXUdYg4BhPpump_2025-11-06T20-36-04.428389_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-06T21-51-10.239309_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-06T21-51-10.239309_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-06T21-51-10.239309_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T20-38-45.964075_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T20-38-45.964075_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T20-38-45.964075_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T21-49-44.525610_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T21-49-44.525610_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T21-49-44.525610_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T23-12-04.654344_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T23-12-04.654344_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T23-12-04.654344_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-06T20-30-56.853399_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-06T20-30-56.853399_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-06T20-30-56.853399_00-00.json'\n",
      "Progress: 20/57 files processed...\n",
      "File 'datasets/alpha/2025-11-06/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-06T23-11-17.037540_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-06T23-11-17.037540_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-06T23-11-17.037540_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-06T21-51-10.239402_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-06T21-51-10.239402_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-06T21-51-10.239402_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-06T23-15-10.049618_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-06T23-15-10.049618_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-06T23-15-10.049618_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-06T21-48-33.140421_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-06T21-48-33.140421_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-06T21-48-33.140421_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-06T23-12-04.655593_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-06/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-06T23-12-04.655593_00-00.json' -> './dataset\\datasets/alpha/2025-11-06/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-06T23-12-04.655593_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/318ATmp975Q5m7xJQiv9f3NmBVLYQNwGAcNitGJupump_2025-11-07T03-59-11.464680_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/318ATmp975Q5m7xJQiv9f3NmBVLYQNwGAcNitGJupump_2025-11-07T03-59-11.464680_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/318ATmp975Q5m7xJQiv9f3NmBVLYQNwGAcNitGJupump_2025-11-07T03-59-11.464680_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T06-27-39.756038_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T06-27-39.756038_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T06-27-39.756038_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T09-41-49.767907_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T09-41-49.767907_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T09-41-49.767907_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T13-28-38.361476_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T13-28-38.361476_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T13-28-38.361476_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T21-49-27.313760_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T21-49-27.313760_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T21-49-27.313760_00-00.json'\n",
      "Progress: 30/57 files processed...\n",
      "File 'datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T23-16-08.341527_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T23-16-08.341527_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-07T23-16-08.341527_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-07T21-56-05.319564_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-07T21-56-05.319564_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-07T21-56-05.319564_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-07T23-13-03.642771_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-07T23-13-03.642771_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-07T23-13-03.642771_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-07T04-33-17.158729_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-07T04-33-17.158729_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-07T04-33-17.158729_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-07T06-27-39.756079_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-07T06-27-39.756079_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-07T06-27-39.756079_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-07T09-41-49.767895_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-07T09-41-49.767895_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-07T09-41-49.767895_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T09-41-49.767919_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T09-41-49.767919_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T09-41-49.767919_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T13-28-38.361446_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T13-28-38.361446_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T13-28-38.361446_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T21-51-35.810776_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T21-51-35.810776_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T21-51-35.810776_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T23-08-48.951396_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T23-08-48.951396_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-07T23-08-48.951396_00-00.json'\n",
      "Progress: 40/57 files processed...\n",
      "File 'datasets/alpha/2025-11-07/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-07T13-28-38.361542_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-07T13-28-38.361542_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-07T13-28-38.361542_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-07T21-56-05.319345_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-07T21-56-05.319345_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-07T21-56-05.319345_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-07T23-13-03.642723_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-07T23-13-03.642723_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-07T23-13-03.642723_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-07T21-49-27.313750_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-07T21-49-27.313750_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-07T21-49-27.313750_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-07T23-08-48.951426_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-07T23-08-48.951426_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-07T23-08-48.951426_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-07T22-07-45.106745_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-07T22-07-45.106745_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-07T22-07-45.106745_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-07T23-14-25.085271_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-07T23-14-25.085271_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-07T23-14-25.085271_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-07T23-16-22.354436_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-07T23-16-22.354436_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-07T23-16-22.354436_00-00.json'\n",
      "File 'datasets/alpha/2025-11-07/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-07T21-51-35.810975_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-07/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-07T21-51-35.810975_00-00.json' -> './dataset\\datasets/alpha/2025-11-07/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-07T21-51-35.810975_00-00.json'\n",
      "File 'datasets/alpha/2025-11-08/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-08T01-40-44.393420_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-08/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-08T01-40-44.393420_00-00.json' -> './dataset\\datasets/alpha/2025-11-08/3GnuXqzqT1XYhSeWrHcvDKE4JyqYMhLecsKKsqjridev_2025-11-08T01-40-44.393420_00-00.json'\n",
      "Progress: 50/57 files processed...\n",
      "File 'datasets/alpha/2025-11-08/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-08T01-13-14.092942_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-08/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-08T01-13-14.092942_00-00.json' -> './dataset\\datasets/alpha/2025-11-08/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-08T01-13-14.092942_00-00.json'\n",
      "File 'datasets/alpha/2025-11-08/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-08T00-23-14.106297_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-08/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-08T00-23-14.106297_00-00.json' -> './dataset\\datasets/alpha/2025-11-08/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-08T00-23-14.106297_00-00.json'\n",
      "File 'datasets/alpha/2025-11-08/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-08T01-31-43.810498_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-08/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-08T01-31-43.810498_00-00.json' -> './dataset\\datasets/alpha/2025-11-08/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-08T01-31-43.810498_00-00.json'\n",
      "File 'datasets/alpha/2025-11-08/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-08T01-31-43.810530_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-08/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-08T01-31-43.810530_00-00.json' -> './dataset\\datasets/alpha/2025-11-08/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-08T01-31-43.810530_00-00.json'\n",
      "File 'datasets/alpha/2025-11-08/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-08T00-51-40.402067_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-08/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-08T00-51-40.402067_00-00.json' -> './dataset\\datasets/alpha/2025-11-08/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-08T00-51-40.402067_00-00.json'\n",
      "File 'datasets/alpha/2025-11-08/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-08T01-31-43.810364_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-08/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-08T01-31-43.810364_00-00.json' -> './dataset\\datasets/alpha/2025-11-08/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-08T01-31-43.810364_00-00.json'\n",
      "File 'datasets/alpha/2025-11-08/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-08T00-27-04.885931_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-08/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-08T00-27-04.885931_00-00.json' -> './dataset\\datasets/alpha/2025-11-08/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-08T00-27-04.885931_00-00.json'\n",
      "File 'datasets/alpha/2025-11-08/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-08T01-31-43.810457_00-00.json': File updated â€” new data loaded.\n",
      "Downloaded and saved 'datasets/alpha/2025-11-08/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-08T01-31-43.810457_00-00.json' -> './dataset\\datasets/alpha/2025-11-08/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-08T01-31-43.810457_00-00.json'\n",
      "\n",
      "âœ… Successfully processed 57 snapshots (0 errors)\n",
      "\n",
      "================================================================================\n",
      "DATAFRAME SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Shape: 57 rows Ã— 153 columns\n",
      "\n",
      "Columns: ['snapshot_id', 'generated_at_utc', 'input_signal_data_ts', 'input_signal_data_check_type', 'input_signal_data_security', 'input_signal_data_result_mint', 'input_signal_data_result_checked_at', 'input_signal_data_result_overlap_count', 'input_signal_data_result_overlap_percentage', 'input_signal_data_result_concentration', 'input_signal_data_result_weighted_concentration', 'input_signal_data_result_total_winner_wallets', 'input_signal_data_result_grade', 'input_signal_data_result_needs_monitoring', 'input_signal_data_result_skip_reason', 'input_security_rugcheck_passed', 'input_security_probation', 'input_security_probation_reason', 'input_rugcheck_rugged', 'input_rugcheck_top1_holder_pct', 'input_rugcheck_holder_count', 'input_rugcheck_has_authorities', 'input_rugcheck_creator_balance', 'input_rugcheck_freeze_authority', 'input_rugcheck_mint_authority', 'input_rugcheck_transfer_fee_pct', 'input_rugcheck_lp_locked_pct', 'input_rugcheck_total_liquidity_usd', 'input_dexscreener_current_price_usd', 'input_holiday_check_is_holiday', 'mint', 'signal_source', 'grade', 'checked_at_utc', 'checked_at_timestamp', 'time_of_day_utc', 'day_of_week_utc', 'is_weekend_utc', 'is_public_holiday_any', 'price_usd', 'fdv_usd', 'liquidity_usd', 'volume_h24_usd', 'price_change_h24_pct', 'pair_created_at_timestamp', 'rugcheck_risk_level', 'is_rugged', 'has_mint_authority', 'has_freeze_authority', 'creator_balance_pct', 'top_10_holders_pct', 'is_lp_locked_95_plus', 'total_lp_locked_usd', 'token_age_at_signal_seconds', 'token_age_hours_at_signal', 'is_new_token', 'finalize_window_hours', 'finalize_deadline', 'check_interval_minutes', 'next_check_at', 'check_count', 'finalization_status', 'claimed_by', 'claimed_at', 'finalized_at', 'label_mint', 'label_signal_type', 'label_symbol', 'label_name', 'label_entry_price', 'label_entry_time', 'label_token_age_hours', 'label_pair_created_at', 'label_tracking_interval_seconds', 'label_tracking_duration_hours', 'label_tracking_end_time', 'label_current_price', 'label_current_roi', 'label_ath_price', 'label_ath_roi', 'label_ath_time', 'label_status', 'label_hit_50_percent', 'label_hit_50_percent_time', 'label_time_to_ath_minutes', 'label_time_to_50_percent_minutes', 'label_last_price_check', 'label_last_successful_price', 'label_consecutive_failures', 'label_retry_start_time', 'label_final_price', 'label_final_roi', 'label_tracking_completed_at', 'label_win_pct', 'label_max_gain_pct', 'label_max_drawdown_pct', 'label_final_pnl_pct', 'label_duration_hours', 'rugcheck_score', 'rugcheck_score_normalized', 'risk_flag_count', 'has_critical_risk', 'has_warning_risk', 'number_of_markets', 'has_pump_fun_market', 'has_meteora_market', 'has_raydium_market', 'total_lp_providers', 'top_3_holders_pct', 'probation_top_n_pct', 'probation_threshold_pct', 'total_insider_networks', 'largest_insider_network_size', 'total_insider_token_amount', 'total_insider_active_accounts', 'transfer_network_count', 'trade_network_count', 'has_transfer_network', 'graph_insiders_detected', 'launchpad_platform', 'is_pump_fun_launch', 'token_supply', 'token_decimals', 'overlap_quality_score', 'winner_wallet_density', 'smart_money_concentration_ratio', 'insider_token_percentage', 'liquidity_per_holder', 'liquidity_to_mcap_ratio', 'volume_to_liquidity_ratio', 'turnover_ratio', 'whale_concentration_score', 'non_amm_top1_holder_pct', 'time_since_pair_creation_minutes', 'time_since_pair_creation_hours', 'token_age_category', 'is_launch_hour_1', 'is_launch_day_1', 'price_velocity', 'price_impact_score', 'authority_risk_score', 'creator_balance_high_risk', 'creator_dumped', 'suspicious_concentration', 'pump_dump_risk_score', 'sustainability_score', 'hour_category', 'is_crypto_prime_time', 'is_first_day_of_week', 'is_last_day_of_week', 'average_holding_size', 'primary_market_liquidity_pct', 'liquidity_fragmentation_index']\n",
      "\n",
      "Data types:\n",
      "snapshot_id                                   object\n",
      "generated_at_utc                 datetime64[ns, UTC]\n",
      "input_signal_data_ts                          object\n",
      "input_signal_data_check_type                  object\n",
      "input_signal_data_security                    object\n",
      "                                        ...         \n",
      "is_first_day_of_week                            bool\n",
      "is_last_day_of_week                             bool\n",
      "average_holding_size                         float64\n",
      "primary_market_liquidity_pct                 float64\n",
      "liquidity_fragmentation_index                float64\n",
      "Length: 153, dtype: object\n",
      "\n",
      "First few rows:\n",
      "\n",
      "================================================================================\n",
      "BASIC STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Unique tokens: 18\n",
      "Signal sources: {'alpha': 57}\n",
      "Grade distribution: {'LOW': 34, 'MEDIUM': 15, 'CRITICAL': 8}\n",
      "Label status distribution: {'loss': 29, 'win': 28}\n",
      "\n",
      "Date range:\n",
      "  Earliest: 2025-11-03 14:48:04.181771+00:00\n",
      "  Latest: 2025-11-08 01:40:44.393420+00:00\n",
      "\n",
      "Market metrics:\n",
      "       price_usd        fdv_usd  liquidity_usd  volume_h24_usd\n",
      "count  57.000000       2.000000      57.000000    2.000000e+00\n",
      "mean    0.000219   80879.000000   47555.227821    2.265652e+06\n",
      "std     0.000250   59200.393935   28133.049865    4.651782e+05\n",
      "min     0.000025   39018.000000   16569.519820    1.936721e+06\n",
      "25%     0.000053   59948.500000   26484.365929    2.101187e+06\n",
      "50%     0.000122   80879.000000   37930.626677    2.265652e+06\n",
      "75%     0.000255  101809.500000   57633.613929    2.430117e+06\n",
      "max     0.001213  122740.000000  124283.809616    2.594583e+06\n",
      "\n",
      "Discovery tokens: 0\n",
      "Alpha tokens: 57\n",
      "\n",
      "Daily token counts:\n",
      "date\n",
      "2025-11-03     2\n",
      "2025-11-05     1\n",
      "2025-11-06    21\n",
      "2025-11-07    24\n",
      "2025-11-08     9\n",
      "dtype: int64\n",
      "\n",
      "Labeled tokens: 57 (100.0%)\n",
      "Unlabeled tokens: 0 (0.0%)\n",
      "\n",
      "Win rate: 49.1%\n",
      "Average max gain: nan%\n",
      "Average final PnL: nan%\n"
     ]
    }
   ],
   "source": [
    "# Now let's load the data from Supabase into a DataFrame\n",
    "\n",
    "# %% Load all datasets (adjust parameters as needed)\n",
    "# Options:\n",
    "# - Load all: df = load_datasets_to_dataframe()\n",
    "# - Load discovery only: df = load_datasets_to_dataframe(pipeline='discovery')\n",
    "# - Load alpha only: df = load_datasets_to_dataframe(pipeline='alpha')\n",
    "# - Load specific date: df = load_datasets_to_dataframe(date_folder='2025-01-15')\n",
    "# - Load with limit: df = load_datasets_to_dataframe(limit=100)\n",
    "\n",
    "# Example: Load first 50 files from both pipelines\n",
    "df = load_datasets_to_dataframe() #limit=50)\n",
    "\n",
    "# Example: Load all discovery data\n",
    "# df = load_datasets_to_dataframe(pipeline='discovery')\n",
    "\n",
    "# Example: Load specific date from alpha\n",
    "# df = load_datasets_to_dataframe(pipeline='alpha', date_folder='2025-01-15')\n",
    "\n",
    "# %% Display DataFrame info\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATAFRAME SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nShape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()\n",
    "\n",
    "# %% Basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASIC STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"\\nUnique tokens: {df['mint'].nunique()}\")\n",
    "    print(f\"Signal sources: {df['signal_source'].value_counts().to_dict()}\")\n",
    "    print(f\"Grade distribution: {df['grade'].value_counts().to_dict()}\")\n",
    "    print(f\"Label status distribution: {df['label_status'].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"\\nDate range:\")\n",
    "    print(f\"  Earliest: {df['checked_at_utc'].min()}\")\n",
    "    print(f\"  Latest: {df['checked_at_utc'].max()}\")\n",
    "    \n",
    "    print(\"\\nMarket metrics:\")\n",
    "    print(df[['price_usd', 'fdv_usd', 'liquidity_usd', 'volume_h24_usd']].describe())\n",
    "\n",
    "# %% Export to CSV (optional)\n",
    "# df.to_csv('token_datasets.csv', index=False)\n",
    "# print(\"\\nâœ… Exported to token_datasets.csv\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Analysis Examples\n",
    "# \n",
    "# Here are some example analyses you can perform:\n",
    "\n",
    "# %% Example: Filter by pipeline\n",
    "if not df.empty:\n",
    "    discovery_df = df[df['signal_source'] == 'discovery']\n",
    "    alpha_df = df[df['signal_source'] == 'alpha']\n",
    "    \n",
    "    print(f\"\\nDiscovery tokens: {len(discovery_df)}\")\n",
    "    print(f\"Alpha tokens: {len(alpha_df)}\")\n",
    "\n",
    "# %% Example: Group by date\n",
    "if not df.empty and 'checked_at_utc' in df.columns:\n",
    "    df['date'] = df['checked_at_utc'].dt.date\n",
    "    daily_counts = df.groupby('date').size()\n",
    "    print(\"\\nDaily token counts:\")\n",
    "    print(daily_counts)\n",
    "\n",
    "# %% Example: Analyze labeled vs unlabeled\n",
    "if not df.empty:\n",
    "    labeled = df[df['label_status'].notna()]\n",
    "    unlabeled = df[df['label_status'].isna()]\n",
    "    \n",
    "    print(f\"\\nLabeled tokens: {len(labeled)} ({len(labeled)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Unlabeled tokens: {len(unlabeled)} ({len(unlabeled)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(labeled) > 0:\n",
    "        print(f\"\\nWin rate: {(labeled['label_status'] == 'win').sum() / len(labeled) * 100:.1f}%\")\n",
    "        print(f\"Average max gain: {labeled['label_max_gain_pct'].mean():.2f}%\")\n",
    "        print(f\"Average final PnL: {labeled['label_final_pnl_pct'].mean():.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2fa5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/token_datasets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c234b2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57 entries, 0 to 56\n",
      "Columns: 154 entries, snapshot_id to date\n",
      "dtypes: bool(27), datetime64[ns, UTC](4), float64(58), int64(23), object(42)\n",
      "memory usage: 58.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "degen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
