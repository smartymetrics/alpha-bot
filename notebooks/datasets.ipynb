{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d211a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSupabase Dataset Retrieval and DataFrame Creation Notebook\\n==========================================================\\nThis notebook retrieves token snapshot datasets from Supabase storage\\nand creates a pandas DataFrame with one row per token containing daily data.\\n\\nRequirements:\\npip install supabase pandas python-dotenv requests\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Supabase Dataset Retrieval and DataFrame Creation Notebook\n",
    "==========================================================\n",
    "This notebook retrieves token snapshot datasets from Supabase storage\n",
    "and creates a pandas DataFrame with one row per token containing daily data.\n",
    "\n",
    "Requirements:\n",
    "pip install supabase pandas python-dotenv requests\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# # Supabase Token Dataset Loader\n",
    "# \n",
    "# This notebook:\n",
    "# 1. Connects to Supabase storage\n",
    "# 2. Lists all dataset files from the analytics/snapshots folder\n",
    "# 3. Downloads and parses JSON snapshot files\n",
    "# 4. Creates a pandas DataFrame with one row per token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6833ce5",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f71ea3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf124fe",
   "metadata": {},
   "source": [
    "\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61e20568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to Supabase: https://ldraroaloinsesjoayxc.supabase.co\n"
     ]
    }
   ],
   "source": [
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "BUCKET_NAME = os.getenv(\"SUPABASE_BUCKET\", \"monitor-data\")\n",
    "DATASET_DIR_REMOTE = os.getenv(\"DATASET_DIR_REMOTE\", \"datasets\")\n",
    "\n",
    "if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "    raise RuntimeError(\"âŒ Missing SUPABASE_URL or SUPABASE_KEY in environment variables\")\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "print(f\"âœ… Connected to Supabase: {SUPABASE_URL}\")\n",
    "\n",
    "global _file_cache_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435f687",
   "metadata": {},
   "source": [
    "\n",
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b3fb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download function using signed URLs\n",
    "def download_json_from_supabase(remote_path: str, local_dataset_root: str = \"./dataset\") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Download and parse a JSON file from Supabase using a signed URL,\n",
    "    with conditional GET (ETag/If-Modified-Since) caching.\n",
    "    \n",
    "    Saves to and loads from a local 'dataset' folder to avoid refetching.\n",
    "\n",
    "    Args:\n",
    "        remote_path: Path to file in Supabase storage \n",
    "                     (e.g., 'mint_discovery/2025-01-15/token.json')\n",
    "        local_dataset_root: The local root directory to save/load cached files.\n",
    "    \n",
    "    Returns:\n",
    "        Parsed JSON as dict, or None if error.\n",
    "    \"\"\"\n",
    "    global _file_cache_headers\n",
    "\n",
    "    # 1. Determine local save path\n",
    "    save_path = os.path.join(local_dataset_root, remote_path)\n",
    "    file_content_bytes: Optional[bytes] = None\n",
    "\n",
    "    try:\n",
    "        # 2. Generate a signed URL\n",
    "        signed_url_response = supabase.storage.from_(BUCKET_NAME).create_signed_url(remote_path, 60)\n",
    "        signed_url = signed_url_response.get('signedURL')\n",
    "        \n",
    "        if not signed_url:\n",
    "            print(f\"âŒ Could not generate signed URL for '{remote_path}'\")\n",
    "            # If we can't get a URL, check the local cache as a fallback\n",
    "            if os.path.exists(save_path):\n",
    "                print(f\"Loading from local cache as fallback: '{save_path}'\")\n",
    "                with open(save_path, \"rb\") as f:\n",
    "                    file_content_bytes = f.read()\n",
    "            else:\n",
    "                return None  # No URL, no local file\n",
    "        \n",
    "        if not file_content_bytes:\n",
    "            # 3. Prepare headers for conditional GET\n",
    "            headers = {}\n",
    "            cached_headers = _file_cache_headers.get(remote_path, {})\n",
    "            if cached_headers.get('Last-Modified'):\n",
    "                headers['If-Modified-Since'] = cached_headers['Last-Modified']\n",
    "            if cached_headers.get('ETag'):\n",
    "                headers['If-None-Match'] = cached_headers['ETag']\n",
    "\n",
    "            # 4. Perform the HTTP request\n",
    "            response = requests.get(signed_url, headers=headers, timeout=15)\n",
    "\n",
    "            # 5. Handle the response\n",
    "            if response.status_code == 304:\n",
    "                # 304 Not Modified\n",
    "                print(f\"File '{remote_path}': No change detected (304 Not Modified).\")\n",
    "                if os.path.exists(save_path):\n",
    "                    print(f\"Loading from local cache: '{save_path}'\")\n",
    "                    with open(save_path, \"rb\") as f:\n",
    "                        file_content_bytes = f.read()\n",
    "                else:\n",
    "                    # File not modified, but local copy is missing. Force re-download.\n",
    "                    print(f\"File '{remote_path}' not modified, but local file '{save_path}' missing. Forcing re-download.\")\n",
    "                    headers.pop('If-Modified-Since', None)\n",
    "                    headers.pop('If-None-Match', None)\n",
    "                    _file_cache_headers.pop(remote_path, None)\n",
    "                    response = requests.get(signed_url, headers=headers, timeout=15)\n",
    "                    # Allow to fall through to 200 logic if it's 200 now\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                # 200 OK: File is new or has been updated\n",
    "                print(f\"File '{remote_path}': File updated â€” new data loaded.\")\n",
    "                file_content_bytes = response.content\n",
    "\n",
    "                # Update our cache with the new 'Last-Modified' and 'ETag' headers\n",
    "                new_last_modified = response.headers.get('Last-Modified')\n",
    "                new_etag = response.headers.get('ETag')\n",
    "                \n",
    "                new_headers_to_cache = {}\n",
    "                if new_last_modified:\n",
    "                    new_headers_to_cache['Last-Modified'] = new_last_modified\n",
    "                if new_etag:\n",
    "                    new_headers_to_cache['ETag'] = new_etag\n",
    "                    \n",
    "                if new_headers_to_cache:\n",
    "                    _file_cache_headers[remote_path] = new_headers_to_cache\n",
    "                    # print(f\"File '{remote_path}': Updated cache headers.\") # Optional: for more verbose logging\n",
    "\n",
    "                # Save the new file content locally\n",
    "                os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(file_content_bytes)\n",
    "                print(f\"Downloaded and saved '{remote_path}' -> '{save_path}'\")\n",
    "\n",
    "            elif response.status_code != 304: # if it wasn't 200 or 304\n",
    "                # Handle other errors (404 Not Found, 403 Forbidden, 500, etc.)\n",
    "                print(f\"File '{remote_path}': Error fetching file. Status: {response.status_code}, Response: {response.text[:100]}...\")\n",
    "                if os.path.exists(save_path):\n",
    "                    print(f\"Loading from local cache as fallback: '{save_path}'\")\n",
    "                    with open(save_path, \"rb\") as f:\n",
    "                        file_content_bytes = f.read()\n",
    "                else:\n",
    "                    return None # No file, no cache.\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"File '{remote_path}': Network or request error. {e}\")\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"Loading from local cache as fallback: '{save_path}'\")\n",
    "            with open(save_path, \"rb\") as f:\n",
    "                file_content_bytes = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"File '{remote_path}': An unexpected error occurred. {e}\")\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"Loading from local cache as fallback: '{save_path}'\")\n",
    "            with open(save_path, \"rb\") as f:\n",
    "                file_content_bytes = f.read()\n",
    "\n",
    "    # 6. If we have bytes (from download or cache), parse them\n",
    "    if file_content_bytes:\n",
    "        try:\n",
    "            return json.loads(file_content_bytes.decode('utf-8'))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ Failed to parse JSON from '{remote_path}' (local: '{save_path}'): {e}\")\n",
    "            return None\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"âŒ Failed to decode UTF-8 from '{remote_path}' (local: '{save_path}'): {e}\")\n",
    "            return None\n",
    "    \n",
    "    # If we reach here, all attempts failed\n",
    "    print(f\"âŒ Failed to retrieve or load '{remote_path}'\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a724590",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List files function\n",
    "def list_dataset_files(pipeline: str = None, date_folder: str = None, limit: int = 1000) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    List all dataset files in Supabase storage.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Filter by pipeline (e.g., 'mint_discovery', 'mint_alpha')\n",
    "        date_folder: Filter by date folder (e.g., '2025-01-15')\n",
    "        limit: Maximum number of files to return\n",
    "    \n",
    "    Returns:\n",
    "        List of file metadata dicts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pipeline and date_folder:\n",
    "            folder = f\"{DATASET_DIR_REMOTE}/{pipeline}/{date_folder}\"\n",
    "        elif pipeline:\n",
    "            folder = f\"{DATASET_DIR_REMOTE}/{pipeline}\"\n",
    "        else:\n",
    "            folder = DATASET_DIR_REMOTE\n",
    "        \n",
    "        files = supabase.storage.from_(BUCKET_NAME).list(\n",
    "            folder,\n",
    "            {\"limit\": limit, \"sortBy\": {\"column\": \"created_at\", \"order\": \"desc\"}}\n",
    "        )\n",
    "        \n",
    "        return files if files else []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error listing files in {folder}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e94b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_dataset_files(pipeline: str = None, date_folder: str = None, include_expired: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively list all dataset JSON files across all date folders.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Filter by pipeline ('discovery' or 'alpha'). If None, gets both.\n",
    "        date_folder: Filter by specific date (e.g., '2025-01-15'). If None, gets all dates.\n",
    "        include_expired: Whether to include files from 'expired_no_label' folder. Default False.\n",
    "    \n",
    "    Returns:\n",
    "        List of full file paths\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "    \n",
    "    try:\n",
    "        # Define pipelines: 'discovery' and 'alpha' folders\n",
    "        if pipeline:\n",
    "            pipelines = [pipeline]\n",
    "        else:\n",
    "            pipelines = ['discovery', 'alpha']\n",
    "        \n",
    "        print(f\"ğŸ“‚ Scanning pipelines: {pipelines}\")\n",
    "        \n",
    "        # For each pipeline, list date folders\n",
    "        for pipe in pipelines:\n",
    "            pipeline_path = f\"{DATASET_DIR_REMOTE}/{pipe}\"\n",
    "            \n",
    "            try:\n",
    "                # List all folders in this pipeline (these are the daily folders)\n",
    "                date_folders = supabase.storage.from_(BUCKET_NAME).list(pipeline_path)\n",
    "                \n",
    "                print(f\"\\n  ğŸ“… Pipeline '{pipe}' - Found {len(date_folders)} folders\")\n",
    "                \n",
    "                # Filter date folders if specified\n",
    "                if date_folder:\n",
    "                    date_folders = [f for f in date_folders if f.get('name') == date_folder]\n",
    "                    print(f\"     Filtered to date: {date_folder}\")\n",
    "                \n",
    "                for date_item in date_folders:\n",
    "                    folder_name = date_item.get('name')\n",
    "                    if not folder_name:\n",
    "                        continue\n",
    "                    \n",
    "                    # Skip 'expired_no_label' folder unless explicitly requested\n",
    "                    if folder_name == 'expired_no_label' and not include_expired:\n",
    "                        print(f\"    â­ï¸  Skipping: {folder_name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    date_path = f\"{pipeline_path}/{folder_name}\"\n",
    "                    \n",
    "                    try:\n",
    "                        # List JSON files in this date folder\n",
    "                        files = supabase.storage.from_(BUCKET_NAME).list(date_path)\n",
    "                        \n",
    "                        # Filter for JSON files only (files won't have 'id' field or will have metadata)\n",
    "                        json_files = []\n",
    "                        for f in files:\n",
    "                            fname = f.get('name', '')\n",
    "                            if fname.endswith('.json'):\n",
    "                                # This is a file, not a folder\n",
    "                                json_files.append(f)\n",
    "                        \n",
    "                        print(f\"    ğŸ“ {folder_name}: {len(json_files)} JSON files\")\n",
    "                        \n",
    "                        for file in json_files:\n",
    "                            full_path = f\"{date_path}/{file['name']}\"\n",
    "                            all_files.append(full_path)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"    âŒ Error listing files in '{date_path}': {e}\")\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error scanning pipeline '{pipe}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nâœ… Total files found: {len(all_files)}\")\n",
    "        \n",
    "        # Show sample of files found\n",
    "        if all_files:\n",
    "            print(\"\\nğŸ“„ Sample files:\")\n",
    "            for i, file_path in enumerate(all_files[:3]):\n",
    "                print(f\"  {i+1}. {file_path}\")\n",
    "            if len(all_files) > 3:\n",
    "                print(f\"  ... and {len(all_files) - 3} more\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸  No files found. Showing folder structure for debugging:\")\n",
    "            try:\n",
    "                # Try to list what's actually in the datasets folder\n",
    "                for pipe in pipelines:\n",
    "                    pipeline_path = f\"{DATASET_DIR_REMOTE}/{pipe}\"\n",
    "                    pipeline_contents = supabase.storage.from_(BUCKET_NAME).list(pipeline_path)\n",
    "                    print(f\"\\nğŸ“‚ '{pipeline_path}' contains {len(pipeline_contents)} items:\")\n",
    "                    for item in pipeline_contents[:10]:  # Show first 10\n",
    "                        item_name = item.get('name')\n",
    "                        print(f\"  - {item_name}\")\n",
    "                        # Try to peek inside each item\n",
    "                        try:\n",
    "                            folder_path = f\"{pipeline_path}/{item_name}\"\n",
    "                            folder_contents = supabase.storage.from_(BUCKET_NAME).list(folder_path)\n",
    "                            json_count = len([f for f in folder_contents if f.get('name', '').endswith('.json')])\n",
    "                            total_count = len(folder_contents)\n",
    "                            print(f\"    â†’ Contains {total_count} items ({json_count} JSON files)\")\n",
    "                            if json_count > 0:\n",
    "                                print(f\"    â†’ Sample: {folder_contents[0].get('name', 'N/A')}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"    â†’ Error: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Debug failed: {e}\")\n",
    "        \n",
    "        return all_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error listing dataset files: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fd098",
   "metadata": {},
   "source": [
    "\n",
    "#### Data Retrieval and Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e44b9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def flatten_snapshot_to_row(snapshot: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Flatten a snapshot JSON into a single row dict for DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        snapshot: Snapshot dict loaded from JSON\n",
    "    \n",
    "    Returns:\n",
    "        Flattened dict with all relevant fields\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    \n",
    "    # Snapshot metadata\n",
    "    row['snapshot_id'] = snapshot.get('snapshot_id')\n",
    "    row['generated_at_utc'] = snapshot.get('generated_at_utc')\n",
    "\n",
    "    # --- Inputs ---\n",
    "    # Define helper variables for nested input objects, using .get('key', {}) \n",
    "    # to safely chain .get() calls even if a parent object is missing.\n",
    "    inputs = snapshot.get('inputs', {})\n",
    "    signal_data = inputs.get('signal_data', {})\n",
    "    signal_data_result = signal_data.get('result', {})\n",
    "    security_data = signal_data_result.get('security', {})\n",
    "    rugcheck_data = security_data.get('rugcheck', {})\n",
    "    dexscreener_data = signal_data_result.get('dexscreener', {})\n",
    "    holiday_check = inputs.get('holiday_check', {})\n",
    "    \n",
    "    # Inputs - signal_data\n",
    "    row['input_signal_data_ts'] = signal_data.get('ts')\n",
    "    row['input_signal_data_check_type'] = signal_data.get('check_type')\n",
    "    row['input_signal_data_security'] = signal_data.get('security')\n",
    "\n",
    "    # Inputs - signal_data.result\n",
    "    row['input_signal_data_result_mint'] = signal_data_result.get('mint')\n",
    "    row['input_signal_data_result_checked_at'] = signal_data_result.get('checked_at')\n",
    "    row['input_signal_data_result_overlap_count'] = signal_data_result.get('overlap_count')\n",
    "    row['input_signal_data_result_overlap_percentage'] = signal_data_result.get('overlap_percentage')\n",
    "    row['input_signal_data_result_concentration'] = signal_data_result.get('concentration')\n",
    "    row['input_signal_data_result_weighted_concentration'] = signal_data_result.get('weighted_concentration')\n",
    "    row['input_signal_data_result_total_winner_wallets'] = signal_data_result.get('total_winner_wallets')\n",
    "    row['input_signal_data_result_grade'] = signal_data_result.get('grade')\n",
    "    row['input_signal_data_result_needs_monitoring'] = signal_data_result.get('needs_monitoring')\n",
    "    row['input_signal_data_result_skip_reason'] = signal_data_result.get('skip_reason')\n",
    "\n",
    "    # Inputs - signal_data.result.security\n",
    "    row['input_security_rugcheck_passed'] = security_data.get('rugcheck_passed')\n",
    "    row['input_security_probation'] = security_data.get('probation')\n",
    "    row['input_security_probation_reason'] = security_data.get('probation_reason')\n",
    "\n",
    "    # Inputs - signal_data.result.security.rugcheck\n",
    "    row['input_rugcheck_rugged'] = rugcheck_data.get('rugged')\n",
    "    row['input_rugcheck_top1_holder_pct'] = rugcheck_data.get('top1_holder_pct')\n",
    "    row['input_rugcheck_holder_count'] = rugcheck_data.get('holder_count')\n",
    "    row['input_rugcheck_has_authorities'] = rugcheck_data.get('has_authorities')\n",
    "    row['input_rugcheck_creator_balance'] = rugcheck_data.get('creator_balance')\n",
    "    row['input_rugcheck_freeze_authority'] = rugcheck_data.get('freeze_authority')\n",
    "    row['input_rugcheck_mint_authority'] = rugcheck_data.get('mint_authority')\n",
    "    row['input_rugcheck_transfer_fee_pct'] = rugcheck_data.get('transfer_fee_pct')\n",
    "    row['input_rugcheck_lp_locked_pct'] = rugcheck_data.get('lp_locked_pct')\n",
    "    row['input_rugcheck_total_liquidity_usd'] = rugcheck_data.get('total_liquidity_usd')\n",
    "\n",
    "    # Inputs - signal_data.result.dexscreener\n",
    "    row['input_dexscreener_current_price_usd'] = dexscreener_data.get('current_price_usd')\n",
    "\n",
    "    # Inputs - holiday_check\n",
    "    row['input_holiday_check_is_holiday'] = holiday_check.get('is_holiday')\n",
    "    # Note: Skipping 'countries' list as it's not a scalar value.\n",
    "\n",
    "    # --- Features ---\n",
    "    features = snapshot.get('features', {})\n",
    "    row['mint'] = features.get('mint')\n",
    "    row['signal_source'] = features.get('signal_source')\n",
    "    row['grade'] = features.get('grade')\n",
    "    row['checked_at_utc'] = features.get('checked_at_utc')\n",
    "    row['checked_at_timestamp'] = features.get('checked_at_timestamp')\n",
    "    \n",
    "    # Time features\n",
    "    row['time_of_day_utc'] = features.get('time_of_day_utc')\n",
    "    row['day_of_week_utc'] = features.get('day_of_week_utc')\n",
    "    row['is_weekend_utc'] = features.get('is_weekend_utc')\n",
    "    row['is_public_holiday_any'] = features.get('is_public_holiday_any')\n",
    "    \n",
    "    # Market features (from original function)\n",
    "    row['price_usd'] = features.get('price_usd')\n",
    "    row['fdv_usd'] = features.get('fdv_usd')\n",
    "    row['liquidity_usd'] = features.get('liquidity_usd')\n",
    "    row['volume_h24_usd'] = features.get('volume_h24_usd')\n",
    "    row['price_change_h24_pct'] = features.get('price_change_h24_pct')\n",
    "    row['pair_created_at_timestamp'] = features.get('pair_created_at_timestamp')\n",
    "    \n",
    "    # Security features\n",
    "    row['rugcheck_risk_level'] = features.get('rugcheck_risk_level')\n",
    "    row['is_rugged'] = features.get('is_rugged')\n",
    "    row['has_mint_authority'] = features.get('has_mint_authority')\n",
    "    row['has_freeze_authority'] = features.get('has_freeze_authority')\n",
    "    row['creator_balance_pct'] = features.get('creator_balance_pct')\n",
    "    row['top_10_holders_pct'] = features.get('top_10_holders_pct')\n",
    "    row['is_lp_locked_95_plus'] = features.get('is_lp_locked_95_plus')\n",
    "    row['total_lp_locked_usd'] = features.get('total_lp_locked_usd')\n",
    "    \n",
    "    # Derived features (from original function)\n",
    "    row['token_age_at_signal_seconds'] = features.get('token_age_at_signal_seconds')\n",
    "    \n",
    "    # --- Finalization ---\n",
    "    finalization = snapshot.get('finalization', {})\n",
    "    row['token_age_hours_at_signal'] = finalization.get('token_age_hours_at_signal')\n",
    "    row['is_new_token'] = finalization.get('is_new_token')\n",
    "    row['finalize_window_hours'] = finalization.get('finalize_window_hours')\n",
    "    row['finalize_deadline'] = finalization.get('finalize_deadline')\n",
    "    row['check_interval_minutes'] = finalization.get('check_interval_minutes')\n",
    "    row['next_check_at'] = finalization.get('next_check_at')\n",
    "    row['check_count'] = finalization.get('check_count')\n",
    "    row['finalization_status'] = finalization.get('finalization_status')\n",
    "    row['claimed_by'] = finalization.get('claimed_by')\n",
    "    row['claimed_at'] = finalization.get('claimed_at')\n",
    "    row['finalized_at'] = finalization.get('finalized_at')\n",
    "    \n",
    "    # --- Label ---\n",
    "    label = snapshot.get('label')\n",
    "    if label:\n",
    "        row['label_mint'] = label.get('mint')\n",
    "        row['label_signal_type'] = label.get('signal_type')\n",
    "        row['label_symbol'] = label.get('symbol')\n",
    "        row['label_name'] = label.get('name')\n",
    "        row['label_entry_price'] = label.get('entry_price')\n",
    "        row['label_entry_time'] = label.get('entry_time')\n",
    "        row['label_token_age_hours'] = label.get('token_age_hours')\n",
    "        row['label_pair_created_at'] = label.get('pair_created_at')\n",
    "        row['label_tracking_interval_seconds'] = label.get('tracking_interval_seconds')\n",
    "        row['label_tracking_duration_hours'] = label.get('tracking_duration_hours')\n",
    "        row['label_tracking_end_time'] = label.get('tracking_end_time')\n",
    "        row['label_current_price'] = label.get('current_price')\n",
    "        row['label_current_roi'] = label.get('current_roi')\n",
    "        row['label_ath_price'] = label.get('ath_price')\n",
    "        row['label_ath_roi'] = label.get('ath_roi')\n",
    "        row['label_ath_time'] = label.get('ath_time')\n",
    "        row['label_status'] = label.get('status')\n",
    "        row['label_hit_50_percent'] = label.get('hit_50_percent')\n",
    "        row['label_hit_50_percent_time'] = label.get('hit_50_percent_time')\n",
    "        row['label_time_to_ath_minutes'] = label.get('time_to_ath_minutes')\n",
    "        row['label_time_to_50_percent_minutes'] = label.get('time_to_50_percent_minutes')\n",
    "        row['label_last_price_check'] = label.get('last_price_check')\n",
    "        row['label_last_successful_price'] = label.get('last_successful_price')\n",
    "        row['label_consecutive_failures'] = label.get('consecutive_failures')\n",
    "        row['label_retry_start_time'] = label.get('retry_start_time')\n",
    "        row['label_final_price'] = label.get('final_price')\n",
    "        row['label_final_roi'] = label.get('final_roi')\n",
    "        row['label_tracking_completed_at'] = label.get('tracking_completed_at')\n",
    "        \n",
    "        # Fields from original function\n",
    "        row['label_win_pct'] = label.get('win_pct')\n",
    "        row['label_max_gain_pct'] = label.get('max_gain_pct')\n",
    "        row['label_max_drawdown_pct'] = label.get('max_drawdown_pct')\n",
    "        row['label_final_pnl_pct'] = label.get('final_pnl_pct')\n",
    "        row['label_duration_hours'] = label.get('duration_hours')\n",
    "    else:\n",
    "        # Set all label fields to None if label is missing\n",
    "        keys = [\n",
    "            'label_mint', 'label_signal_type', 'label_symbol', 'label_name',\n",
    "            'label_entry_price', 'label_entry_time', 'label_token_age_hours',\n",
    "            'label_pair_created_at', 'label_tracking_interval_seconds',\n",
    "            'label_tracking_duration_hours', 'label_tracking_end_time',\n",
    "            'label_current_price', 'label_current_roi', 'label_ath_price',\n",
    "            'label_ath_roi', 'label_ath_time', 'label_status',\n",
    "            'label_hit_50_percent', 'label_hit_50_percent_time',\n",
    "            'label_time_to_ath_minutes', 'label_time_to_50_percent_minutes',\n",
    "            'label_last_price_check', 'label_last_successful_price',\n",
    "            'label_consecutive_failures', 'label_retry_start_time',\n",
    "            'label_final_price', 'label_final_roi', 'label_tracking_completed_at',\n",
    "            'label_win_pct', 'label_max_gain_pct', 'label_max_drawdown_pct',\n",
    "            'label_final_pnl_pct', 'label_duration_hours'\n",
    "        ]\n",
    "        for key in keys:\n",
    "            row[key] = None\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14910c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load datasets into DataFrame\n",
    "def load_datasets_to_dataframe(pipeline: str = None, date_folder: str = None, limit: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all dataset files from Supabase and create a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Filter by pipeline ('discovery' or 'alpha'). None = both.\n",
    "        date_folder: Filter by specific date folder (e.g., '2025-01-15'). None = all dates.\n",
    "        limit: Maximum number of files to process (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with one row per token snapshot\n",
    "    \"\"\"\n",
    "    print(\"ğŸ” Listing dataset files...\")\n",
    "    file_paths = list_all_dataset_files(pipeline=pipeline, date_folder=date_folder)\n",
    "    \n",
    "    if limit:\n",
    "        file_paths = file_paths[:limit]\n",
    "        print(f\"âš ï¸ Limited to {limit} files\")\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(\"âŒ No dataset files found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nğŸ“¥ Downloading and processing {len(file_paths)} files...\")\n",
    "    \n",
    "    rows = []\n",
    "    errors = 0\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Progress: {i + 1}/{len(file_paths)} files processed...\")\n",
    "        \n",
    "        snapshot = download_json_from_supabase(file_path)\n",
    "        \n",
    "        if snapshot:\n",
    "            try:\n",
    "                row = flatten_snapshot_to_row(snapshot)\n",
    "                rows.append(row)\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error processing {file_path}: {e}\")\n",
    "                errors += 1\n",
    "        else:\n",
    "            errors += 1\n",
    "    \n",
    "    print(f\"\\nâœ… Successfully processed {len(rows)} snapshots ({errors} errors)\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Convert timestamps to datetime\n",
    "    if 'checked_at_utc' in df.columns:\n",
    "        df['checked_at_utc'] = pd.to_datetime(df['checked_at_utc'])\n",
    "    if 'generated_at_utc' in df.columns:\n",
    "        df['generated_at_utc'] = pd.to_datetime(df['generated_at_utc'])\n",
    "    if 'finalized_at' in df.columns:\n",
    "        df['finalized_at'] = pd.to_datetime(df['finalized_at'])\n",
    "    if 'label_tracking_completed_at' in df.columns:\n",
    "        df['label_tracking_completed_at'] = pd.to_datetime(df['label_tracking_completed_at'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac702739",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "662a8506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Listing dataset files...\n",
      "ğŸ“‚ Scanning pipelines: ['discovery', 'alpha']\n",
      "\n",
      "  ğŸ“… Pipeline 'discovery' - Found 1 folders\n",
      "    â­ï¸  Skipping: expired_no_label\n",
      "\n",
      "  ğŸ“… Pipeline 'alpha' - Found 6 folders\n",
      "    ğŸ“ 2025-11-03: 2 JSON files\n",
      "    ğŸ“ 2025-11-05: 1 JSON files\n",
      "    ğŸ“ 2025-11-06: 21 JSON files\n",
      "    ğŸ“ 2025-11-07: 24 JSON files\n",
      "    ğŸ“ 2025-11-08: 9 JSON files\n",
      "    â­ï¸  Skipping: expired_no_label\n",
      "\n",
      "âœ… Total files found: 57\n",
      "\n",
      "ğŸ“„ Sample files:\n",
      "  1. datasets/alpha/2025-11-03/2wTr7CbhVx85JeJjJ4aVsMFJKcvLwDJyfyK3SKd6pump_2025-11-03T14-48-04.181771_00-00.json\n",
      "  2. datasets/alpha/2025-11-03/5yVT7keCgESd6S523y6EwiVfJ3JoRj8f28JkZt2ipump_2025-11-03T15-03-11.343913_00-00.json\n",
      "  3. datasets/alpha/2025-11-05/4bcfWqNrGgvEy8NA3kWeCbZr46Qpd175hWbUXtkopump_2025-11-05T04-14-22.642681_00-00.json\n",
      "  ... and 54 more\n",
      "\n",
      "ğŸ“¥ Downloading and processing 57 files...\n",
      "File 'datasets/alpha/2025-11-03/2wTr7CbhVx85JeJjJ4aVsMFJKcvLwDJyfyK3SKd6pump_2025-11-03T14-48-04.181771_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-03/2wTr7CbhVx85JeJjJ4aVsMFJKcvLwDJyfyK3SKd6pump_2025-11-03T14-48-04.181771_00-00.json'\n",
      "File 'datasets/alpha/2025-11-03/5yVT7keCgESd6S523y6EwiVfJ3JoRj8f28JkZt2ipump_2025-11-03T15-03-11.343913_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-03/5yVT7keCgESd6S523y6EwiVfJ3JoRj8f28JkZt2ipump_2025-11-03T15-03-11.343913_00-00.json'\n",
      "File 'datasets/alpha/2025-11-05/4bcfWqNrGgvEy8NA3kWeCbZr46Qpd175hWbUXtkopump_2025-11-05T04-14-22.642681_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-05/4bcfWqNrGgvEy8NA3kWeCbZr46Qpd175hWbUXtkopump_2025-11-05T04-14-22.642681_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/3MHQYPkMKoCk5Zt2RDtN8siaRBKuH2CQroLYsFGNpump_2025-11-06T23-17-42.108451_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/3MHQYPkMKoCk5Zt2RDtN8siaRBKuH2CQroLYsFGNpump_2025-11-06T23-17-42.108451_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-06T21-48-33.225354_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-06T21-48-33.225354_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-06T23-17-07.525776_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/3NczXZWS3pLri4W7zHk8mCVcxReBwyvDWZh5HfgVpump_2025-11-06T23-17-07.525776_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/48Yd7G8UixJwBT4qF8C6TM9QgNHvcZPgG3dPn9fkpump_2025-11-06T23-09-42.479425_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/48Yd7G8UixJwBT4qF8C6TM9QgNHvcZPgG3dPn9fkpump_2025-11-06T23-09-42.479425_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-06T21-48-33.140534_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-06T21-48-33.140534_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-06T23-12-04.655367_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/6iUSp8fLPki1htYZmY5JtZgYjhUjAfxvy8K7tjK2pump_2025-11-06T23-12-04.655367_00-00.json'\n",
      "Progress: 10/57 files processed...\n",
      "File 'datasets/alpha/2025-11-06/7Mk8rfgdCZ2CF1tCoEcMy9uCFV3MkdfkdfyHmaU1pump_2025-11-06T06-12-19.581250_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/7Mk8rfgdCZ2CF1tCoEcMy9uCFV3MkdfkdfyHmaU1pump_2025-11-06T06-12-19.581250_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-06T23-16-25.354242_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/A1akZcGPb9h53EspAuFgmzUTQFZ48uszRWRvxikzjkzr_2025-11-06T23-16-25.354242_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/BuhyqqURetnUuouatgLXWikrbUG93ShcxCJk27a7pump_2025-11-06T20-28-56.300273_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/BuhyqqURetnUuouatgLXWikrbUG93ShcxCJk27a7pump_2025-11-06T20-28-56.300273_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/BuhyqqURetnUuouatgLXWikrbUG93ShcxCJk27a7pump_2025-11-06T23-11-17.030907_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/BuhyqqURetnUuouatgLXWikrbUG93ShcxCJk27a7pump_2025-11-06T23-11-17.030907_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/CaHrnvS8y4qjosyeArb8PYgVUkK8Y6UXUdYg4BhPpump_2025-11-06T20-36-04.428389_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/CaHrnvS8y4qjosyeArb8PYgVUkK8Y6UXUdYg4BhPpump_2025-11-06T20-36-04.428389_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-06T21-51-10.239309_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/E5UdugdLvgsccqe3pdq1ivNBTZhaxwthjvnE85Kzpump_2025-11-06T21-51-10.239309_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T20-38-45.964075_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T20-38-45.964075_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T21-49-44.525610_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T21-49-44.525610_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T23-12-04.654344_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/EMAGfmV5bMzYEtgda43ZmCYwmLL7SaMi2RVqaRPjpump_2025-11-06T23-12-04.654344_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-06T20-30-56.853399_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-06T20-30-56.853399_00-00.json'\n",
      "Progress: 20/57 files processed...\n",
      "File 'datasets/alpha/2025-11-06/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-06T23-11-17.037540_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/FN3iSWdie6sBPKePryuTCevDMxWRyZBDSPdebSYepump_2025-11-06T23-11-17.037540_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-06T21-51-10.239402_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-06T21-51-10.239402_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-06T23-15-10.049618_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/HohE9FsiB2Mxaa5jjVwtqxym7y17ac4jYTS48pRGxahq_2025-11-06T23-15-10.049618_00-00.json'\n",
      "File 'datasets/alpha/2025-11-06/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-06T21-48-33.140421_00-00.json': An unexpected error occurred. name '_file_cache_headers' is not defined\n",
      "âŒ Failed to retrieve or load 'datasets/alpha/2025-11-06/HsfJnaBfRhBUTQCzCpXdL5codokZw6nwwWFnkzeWpump_2025-11-06T21-48-33.140421_00-00.json'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Now let's load the data from Supabase into a DataFrame\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# %% Load all datasets (adjust parameters as needed)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Example: Load first 50 files from both pipelines\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_datasets_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#limit=50)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Example: Load all discovery data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# df = load_datasets_to_dataframe(pipeline='discovery')\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# %% Display DataFrame info\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 34\u001b[0m, in \u001b[0;36mload_datasets_to_dataframe\u001b[1;34m(pipeline, date_folder, limit)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProgress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(file_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files processed...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m snapshot \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_json_from_supabase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m snapshot:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m, in \u001b[0;36mdownload_json_from_supabase\u001b[1;34m(remote_path, local_dataset_root)\u001b[0m\n\u001b[0;32m     21\u001b[0m file_content_bytes: Optional[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# 2. Generate a signed URL\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     signed_url_response \u001b[38;5;241m=\u001b[39m \u001b[43msupabase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBUCKET_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_signed_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremote_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     signed_url \u001b[38;5;241m=\u001b[39m signed_url_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msignedURL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signed_url:\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\storage3\\_sync\\file_api.py:182\u001b[0m, in \u001b[0;36mSyncBucketActionsMixin.create_signed_url\u001b[1;34m(self, path, expires_in, options)\u001b[0m\n\u001b[0;32m    179\u001b[0m     json\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m: options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[0;32m    181\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_final_path(path)\n\u001b[1;32m--> 182\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/object/sign/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Prepare URL\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\storage3\\_sync\\file_api.py:50\u001b[0m, in \u001b[0;36mSyncBucketActionsMixin._request\u001b[1;34m(self, method, url, headers, json, files, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request\u001b[39m(\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     42\u001b[0m     method: RequestMethod,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     48\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m     51\u001b[0m             method, url, headers\u001b[38;5;241m=\u001b[39mheaders \u001b[38;5;129;01mor\u001b[39;00m {}, json\u001b[38;5;241m=\u001b[39mjson, files\u001b[38;5;241m=\u001b[39mfiles, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     52\u001b[0m         )\n\u001b[0;32m     53\u001b[0m         response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpx\\_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    824\u001b[0m )\n\u001b[1;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpcore\\_sync\\http2.py:187\u001b[0m, in \u001b[0;36mHTTP2Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# If h2 raises a protocol error in some other state then we\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# must somehow have made a protocol violation.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocalProtocolError(exc)  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpcore\\_sync\\http2.py:150\u001b[0m, in \u001b[0;36mHTTP2Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request_body(request\u001b[38;5;241m=\u001b[39mrequest, stream_id\u001b[38;5;241m=\u001b[39mstream_id)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    149\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m--> 150\u001b[0m     status, headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_id\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (status, headers)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    156\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m    157\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m     },\n\u001b[0;32m    164\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpcore\\_sync\\http2.py:294\u001b[0m, in \u001b[0;36mHTTP2Connection._receive_response\u001b[1;34m(self, request, stream_id)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03mReturn the response status code and headers for a given stream ID.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 294\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_stream_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h2\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mResponseReceived):\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpcore\\_sync\\http2.py:336\u001b[0m, in \u001b[0;36mHTTP2Connection._receive_stream_event\u001b[1;34m(self, request, stream_id)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03mReturn the next available event for a given stream ID.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mWill read more data from the network if required.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mget(stream_id):\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events[stream_id]\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h2\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mStreamReset):\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpcore\\_sync\\http2.py:364\u001b[0m, in \u001b[0;36mHTTP2Connection._receive_events\u001b[1;34m(self, request, stream_id)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# This conditional is a bit icky. We don't want to block reading if we've\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# actually got an event to return for a given stream. We need to do that\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# check *within* the atomic read lock. Though it also need to be optional,\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;66;03m# because when we call it from `_wait_for_outgoing_flow` we *do* want to\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# block until we've available flow control, event when we have events\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# pending for the stream ID we're attempting to send on.\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mget(stream_id):\n\u001b[1;32m--> 364\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_incoming_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h2\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39mRemoteSettingsChanged):\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpcore\\_sync\\http2.py:441\u001b[0m, in \u001b[0;36mHTTP2Connection._read_incoming_data\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_exception  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m RemoteProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer disconnected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP USER\\Documents\\Data Analyst\\degen smart\\degen\\lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\ssl.py:1259\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1258\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now let's load the data from Supabase into a DataFrame\n",
    "\n",
    "# %% Load all datasets (adjust parameters as needed)\n",
    "# Options:\n",
    "# - Load all: df = load_datasets_to_dataframe()\n",
    "# - Load discovery only: df = load_datasets_to_dataframe(pipeline='discovery')\n",
    "# - Load alpha only: df = load_datasets_to_dataframe(pipeline='alpha')\n",
    "# - Load specific date: df = load_datasets_to_dataframe(date_folder='2025-01-15')\n",
    "# - Load with limit: df = load_datasets_to_dataframe(limit=100)\n",
    "\n",
    "# Example: Load first 50 files from both pipelines\n",
    "df = load_datasets_to_dataframe() #limit=50)\n",
    "\n",
    "# Example: Load all discovery data\n",
    "# df = load_datasets_to_dataframe(pipeline='discovery')\n",
    "\n",
    "# Example: Load specific date from alpha\n",
    "# df = load_datasets_to_dataframe(pipeline='alpha', date_folder='2025-01-15')\n",
    "\n",
    "# %% Display DataFrame info\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATAFRAME SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nShape: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()\n",
    "\n",
    "# %% Basic statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASIC STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"\\nUnique tokens: {df['mint'].nunique()}\")\n",
    "    print(f\"Signal sources: {df['signal_source'].value_counts().to_dict()}\")\n",
    "    print(f\"Grade distribution: {df['grade'].value_counts().to_dict()}\")\n",
    "    print(f\"Label status distribution: {df['label_status'].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(\"\\nDate range:\")\n",
    "    print(f\"  Earliest: {df['checked_at_utc'].min()}\")\n",
    "    print(f\"  Latest: {df['checked_at_utc'].max()}\")\n",
    "    \n",
    "    print(\"\\nMarket metrics:\")\n",
    "    print(df[['price_usd', 'fdv_usd', 'liquidity_usd', 'volume_h24_usd']].describe())\n",
    "\n",
    "# %% Export to CSV (optional)\n",
    "# df.to_csv('token_datasets.csv', index=False)\n",
    "# print(\"\\nâœ… Exported to token_datasets.csv\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Analysis Examples\n",
    "# \n",
    "# Here are some example analyses you can perform:\n",
    "\n",
    "# %% Example: Filter by pipeline\n",
    "if not df.empty:\n",
    "    discovery_df = df[df['signal_source'] == 'discovery']\n",
    "    alpha_df = df[df['signal_source'] == 'alpha']\n",
    "    \n",
    "    print(f\"\\nDiscovery tokens: {len(discovery_df)}\")\n",
    "    print(f\"Alpha tokens: {len(alpha_df)}\")\n",
    "\n",
    "# %% Example: Group by date\n",
    "if not df.empty and 'checked_at_utc' in df.columns:\n",
    "    df['date'] = df['checked_at_utc'].dt.date\n",
    "    daily_counts = df.groupby('date').size()\n",
    "    print(\"\\nDaily token counts:\")\n",
    "    print(daily_counts)\n",
    "\n",
    "# %% Example: Analyze labeled vs unlabeled\n",
    "if not df.empty:\n",
    "    labeled = df[df['label_status'].notna()]\n",
    "    unlabeled = df[df['label_status'].isna()]\n",
    "    \n",
    "    print(f\"\\nLabeled tokens: {len(labeled)} ({len(labeled)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Unlabeled tokens: {len(unlabeled)} ({len(unlabeled)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(labeled) > 0:\n",
    "        print(f\"\\nWin rate: {(labeled['label_status'] == 'win').sum() / len(labeled) * 100:.1f}%\")\n",
    "        print(f\"Average max gain: {labeled['label_max_gain_pct'].mean():.2f}%\")\n",
    "        print(f\"Average final PnL: {labeled['label_final_pnl_pct'].mean():.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa5530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=0, step=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "degen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
