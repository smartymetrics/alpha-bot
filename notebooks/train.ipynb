# %% [markdown]
# # Production ML Training for Solana Token Bot
# 
# This notebook:
# 1. Uses ONLY features that exist in collector.py
# 2. Handles time-series data correctly
# 3. Trains XGBoost (better than logistic regression)
# 4. Saves everything needed for production deployment
# 5. Validates model performance thoroughly

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import joblib
import json

# ML libraries
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    roc_curve, precision_recall_curve, average_precision_score
)
import xgboost as xgb

import warnings
warnings.filterwarnings('ignore')

sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

print("âœ… Libraries imported successfully")

# %% [markdown]
# ## 1. Load Data

# %%
# Load your dataset
df = pd.read_csv('data/token_datasets.csv')

print(f"ğŸ“Š Dataset loaded: {df.shape[0]} rows Ã— {df.shape[1]} columns")
print(f"\nğŸ¯ Target distribution:")
print(df['label_status'].value_counts())
print(f"\nWin rate: {(df['label_status'] == 'win').mean() * 100:.2f}%")

# %% [markdown]
# ## 2. Feature Engineering - ONLY Collector.py Features
# 
# **CRITICAL**: We only use features that exist in your collector.py snapshot

# %%
# These are the ACTUAL features from your collector.py
# DO NOT add features that don't exist in the collector!

MARKET_FEATURES = [
    'price_usd',
    'fdv_usd', 
    'liquidity_usd',
    'volume_h24_usd',
    'price_change_h24_pct',
]

SECURITY_FEATURES = [
    'creator_balance_pct',
    'top_10_holders_pct',
    'total_lp_locked_usd',
    'has_mint_authority',
    'has_freeze_authority',
    'is_lp_locked_95_plus',
]

TIME_FEATURES = [
    'time_of_day_utc',
    'day_of_week_utc',
    'is_weekend_utc',
    'is_public_holiday_any',
]

SIGNAL_FEATURES = [
    'signal_source',  # discovery or alpha
    'grade',  # CRITICAL, HIGH, MEDIUM, LOW
]

TOKEN_AGE_FEATURES = [
    'token_age_at_signal_seconds',
]

# Combine all features
FEATURE_COLS = (
    MARKET_FEATURES + 
    SECURITY_FEATURES + 
    TIME_FEATURES + 
    SIGNAL_FEATURES + 
    TOKEN_AGE_FEATURES
)

print(f"âœ… Using {len(FEATURE_COLS)} features from collector.py")
print("\nFeatures:")
for f in FEATURE_COLS:
    print(f"  - {f}")

# %%
# Check which features actually exist in your data
missing_features = [f for f in FEATURE_COLS if f not in df.columns]

if missing_features:
    print(f"âš ï¸  WARNING: Missing features in dataset:")
    for f in missing_features:
        print(f"  - {f}")
    
    # Remove missing features
    FEATURE_COLS = [f for f in FEATURE_COLS if f in df.columns]
    print(f"\nâœ… Using {len(FEATURE_COLS)} available features")

# %% [markdown]
# ## 3. Data Preprocessing

# %%
# Create a clean copy
df_clean = df.copy()

# Handle missing values
print("ğŸ” Checking for missing values...")
missing = df_clean[FEATURE_COLS].isnull().sum()
missing = missing[missing > 0]

if len(missing) > 0:
    print(f"\nâš ï¸  Found missing values:")
    print(missing)
    
    # Fill numeric with 0
    numeric_cols = df_clean[FEATURE_COLS].select_dtypes(include=[np.number]).columns
    df_clean[numeric_cols] = df_clean[numeric_cols].fillna(0)
    
    # Fill categorical with 'UNKNOWN'
    cat_cols = df_clean[FEATURE_COLS].select_dtypes(include=['object']).columns
    df_clean[cat_cols] = df_clean[cat_cols].fillna('UNKNOWN')
    
    print("âœ… Missing values handled")
else:
    print("âœ… No missing values")

# %%
# Encode categorical features
print("\nğŸ·ï¸  Encoding categorical features...")

label_encoders = {}
df_encoded = df_clean.copy()

for col in FEATURE_COLS:
    if df_encoded[col].dtype == 'object':
        print(f"  Encoding: {col}")
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))
        label_encoders[col] = le

print(f"âœ… Encoded {len(label_encoders)} categorical features")

# %%
# Encode target
print("\nğŸ¯ Encoding target variable...")
le_target = LabelEncoder()
df_encoded['target'] = le_target.fit_transform(df_clean['label_status'])
label_encoders['target'] = le_target

print(f"  0 = {le_target.classes_[0]}")
print(f"  1 = {le_target.classes_[1]}")

# %%
# Create derived features (these are computed from existing features)
print("\nğŸ”¬ Creating derived features...")

df_encoded['volume_to_liquidity_ratio'] = np.where(
    df_encoded['liquidity_usd'] > 0,
    df_encoded['volume_h24_usd'] / df_encoded['liquidity_usd'],
    0
)

df_encoded['fdv_to_liquidity_ratio'] = np.where(
    df_encoded['liquidity_usd'] > 0,
    df_encoded['fdv_usd'] / df_encoded['liquidity_usd'],
    0
)

df_encoded['is_new_token'] = (df_encoded['token_age_at_signal_seconds'] < 43200).astype(int)  # <12 hours

# Add derived features to feature list
FEATURE_COLS.extend([
    'volume_to_liquidity_ratio',
    'fdv_to_liquidity_ratio', 
    'is_new_token'
])

print(f"âœ… Total features: {len(FEATURE_COLS)}")

# %% [markdown]
# ## 4. Time-Series Train-Test Split
# 
# **CRITICAL**: We split by TIME, not randomly

# %%
# Sort by timestamp if available
if 'checked_at_timestamp' in df_encoded.columns:
    print("ğŸ“… Sorting by timestamp...")
    df_encoded = df_encoded.sort_values('checked_at_timestamp')
    print(f"  Date range: {df_encoded['checked_at_timestamp'].min()} to {df_encoded['checked_at_timestamp'].max()}")
else:
    print("âš ï¸  No timestamp found - using order as-is")

# %%
# Time-series split: 80% train, 20% test
# Test set is the MOST RECENT data
split_idx = int(len(df_encoded) * 0.8)

train_df = df_encoded.iloc[:split_idx]
test_df = df_encoded.iloc[split_idx:]

print(f"\nâœ‚ï¸  Train-Test Split:")
print(f"  Training samples: {len(train_df)} ({len(train_df)/len(df_encoded)*100:.1f}%)")
print(f"  Test samples: {len(test_df)} ({len(test_df)/len(df_encoded)*100:.1f}%)")

# Check distribution
print(f"\nğŸ“Š Distribution:")
print(f"  Training win rate: {train_df['target'].mean()*100:.2f}%")
print(f"  Test win rate: {test_df['target'].mean()*100:.2f}%")

# %%
# Prepare X and y
X_train = train_df[FEATURE_COLS].copy()
y_train = train_df['target'].copy()

X_test = test_df[FEATURE_COLS].copy()
y_test = test_df['target'].copy()

print(f"\nâœ… Data prepared:")
print(f"  X_train: {X_train.shape}")
print(f"  X_test: {X_test.shape}")

# %% [markdown]
# ## 5. Feature Analysis

# %%
# Correlation with target
print("ğŸ” Top 15 Features Correlated with Target:")
print("-" * 60)

correlations = train_df[FEATURE_COLS + ['target']].corr()['target'].drop('target').sort_values(ascending=False)

for i, (feature, corr) in enumerate(correlations.head(15).items(), 1):
    print(f"{i:2}. {feature:40} | r = {corr:6.3f}")

# Plot
plt.figure(figsize=(12, 8))
correlations.head(20).plot(kind='barh')
plt.xlabel('Correlation with Target')
plt.title('Top 20 Features Correlated with Win/Loss')
plt.tight_layout()
plt.savefig('outputs/feature_correlations.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nâœ… Feature correlation analysis complete")

# %% [markdown]
# ## 6. Train XGBoost Model

# %%
print("ğŸš€ Training XGBoost Model...")
print("-" * 60)

# Calculate scale_pos_weight for class imbalance
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
print(f"Class imbalance ratio: {scale_pos_weight:.2f}")

# Initialize XGBoost
model = xgb.XGBClassifier(
    max_depth=6,
    learning_rate=0.1,
    n_estimators=200,
    objective='binary:logistic',
    eval_metric='auc',
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=scale_pos_weight,  # Handle imbalance
    random_state=42,
    early_stopping_rounds=20,
    n_jobs=-1
)

# %%
# Train with validation set
eval_set = [(X_train, y_train), (X_test, y_test)]

model.fit(
    X_train, y_train,
    eval_set=eval_set,
    verbose=True
)

print("\nâœ… Model training complete!")

# %% [markdown]
# ## 7. Cross-Validation

# %%
print("ğŸ”„ Running 5-Fold Time-Series Cross-Validation...")

# Use TimeSeriesSplit for proper time-series CV
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
cv_scores = []

for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):
    X_fold_train = X_train.iloc[train_idx]
    y_fold_train = y_train.iloc[train_idx]
    X_fold_val = X_train.iloc[val_idx]
    y_fold_val = y_train.iloc[val_idx]
    
    # Train
    fold_model = xgb.XGBClassifier(
        max_depth=6, learning_rate=0.1, n_estimators=200,
        scale_pos_weight=scale_pos_weight, random_state=42
    )
    fold_model.fit(X_fold_train, y_fold_train, verbose=False)
    
    # Evaluate
    score = roc_auc_score(y_fold_val, fold_model.predict_proba(X_fold_val)[:, 1])
    cv_scores.append(score)
    print(f"  Fold {fold}: AUC = {score:.4f}")

print(f"\nğŸ“Š Cross-Validation Results:")
print(f"  Mean AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")

# %% [markdown]
# ## 8. Model Evaluation

# %%
print("="*60)
print("ğŸ“Š MODEL EVALUATION")
print("="*60)

# Predictions
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

y_train_proba = model.predict_proba(X_train)[:, 1]
y_test_proba = model.predict_proba(X_test)[:, 1]

# Accuracy
train_acc = (y_train_pred == y_train).mean()
test_acc = (y_test_pred == y_test).mean()

print(f"\nğŸ“ˆ Accuracy:")
print(f"  Training: {train_acc:.4f} ({train_acc*100:.2f}%)")
print(f"  Test:     {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"  Gap:      {abs(train_acc - test_acc):.4f}")

# AUC
train_auc = roc_auc_score(y_train, y_train_proba)
test_auc = roc_auc_score(y_test, y_test_proba)

print(f"\nğŸ“ˆ ROC AUC:")
print(f"  Training: {train_auc:.4f}")
print(f"  Test:     {test_auc:.4f}")

# %%
# Confusion Matrix
print("\n" + "="*60)
print("ğŸ“‹ CONFUSION MATRIX")
print("="*60)

cm_train = confusion_matrix(y_train, y_train_pred)
cm_test = confusion_matrix(y_test, y_test_pred)

print("\nTraining Set:")
print(cm_train)
print(f"  TN: {cm_train[0][0]}, FP: {cm_train[0][1]}")
print(f"  FN: {cm_train[1][0]}, TP: {cm_train[1][1]}")

print("\nTest Set:")
print(cm_test)
print(f"  TN: {cm_test[0][0]}, FP: {cm_test[0][1]}")
print(f"  FN: {cm_test[1][0]}, TP: {cm_test[1][1]}")

# %%
# Classification Report
print("\n" + "="*60)
print("ğŸ“Š CLASSIFICATION REPORT")
print("="*60)

print("\nTest Set:")
print(classification_report(
    y_test, y_test_pred,
    target_names=['Loss', 'Win'],
    digits=4
))

# %%
# ROC Curve
print("\nğŸ“ˆ Plotting ROC Curve...")

fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)

plt.figure(figsize=(10, 8))
plt.plot(fpr_train, tpr_train, linewidth=2, 
         label=f'Training (AUC = {train_auc:.4f})', color='blue')
plt.plot(fpr_test, tpr_test, linewidth=2,
         label=f'Test (AUC = {test_auc:.4f})', color='green')
plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')

plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve - XGBoost Model', fontsize=14, fontweight='bold')
plt.legend(loc='lower right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('outputs/roc_curve.png', dpi=150, bbox_inches='tight')
plt.show()

# %%
# Precision-Recall Curve
print("\nğŸ“ˆ Plotting Precision-Recall Curve...")

precision_train, recall_train, _ = precision_recall_curve(y_train, y_train_proba)
precision_test, recall_test, _ = precision_recall_curve(y_test, y_test_proba)

ap_train = average_precision_score(y_train, y_train_proba)
ap_test = average_precision_score(y_test, y_test_proba)

plt.figure(figsize=(10, 8))
plt.plot(recall_train, precision_train, linewidth=2,
         label=f'Training (AP = {ap_train:.4f})', color='blue')
plt.plot(recall_test, precision_test, linewidth=2,
         label=f'Test (AP = {ap_test:.4f})', color='green')

plt.xlabel('Recall', fontsize=12)
plt.ylabel('Precision', fontsize=12)
plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('outputs/precision_recall_curve.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"\nâœ… Average Precision:")
print(f"  Training: {ap_train:.4f}")
print(f"  Test:     {ap_test:.4f}")

# %% [markdown]
# ## 9. Feature Importance

# %%
print("="*60)
print("ğŸ” FEATURE IMPORTANCE")
print("="*60)

# Get feature importance
feature_importance = pd.DataFrame({
    'feature': FEATURE_COLS,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 20 Most Important Features:")
print("-" * 60)
for i, row in feature_importance.head(20).iterrows():
    print(f"{row['feature']:40} | {row['importance']:.4f}")

# %%
# Plot feature importance
plt.figure(figsize=(12, 10))
top_features = feature_importance.head(20)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Importance')
plt.title('Top 20 Feature Importances - XGBoost', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('outputs/feature_importance.png', dpi=150, bbox_inches='tight')
plt.show()

# %% [markdown]
# ## 10. Prediction Analysis

# %%
print("="*60)
print("ğŸ” PREDICTION ANALYSIS")
print("="*60)

# Analyze predictions by confidence
test_df_pred = test_df.copy()
test_df_pred['predicted_proba'] = y_test_proba
test_df_pred['predicted_class'] = y_test_pred
test_df_pred['correct'] = (y_test_pred == y_test).astype(int)

# Group by confidence bins
bins = [0, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
test_df_pred['confidence_bin'] = pd.cut(test_df_pred['predicted_proba'], bins=bins)

print("\nğŸ“Š Performance by Confidence Level:")
print("-" * 60)
conf_analysis = test_df_pred.groupby('confidence_bin').agg({
    'correct': ['count', 'sum', 'mean'],
    'target': 'mean'
})
print(conf_analysis)

# %%
# High confidence predictions
high_conf = test_df_pred[test_df_pred['predicted_proba'] >= 0.7]
print(f"\nğŸ¯ High Confidence Predictions (>=0.7):")
print(f"  Count: {len(high_conf)}")
print(f"  Accuracy: {high_conf['correct'].mean():.4f}")
print(f"  Win rate: {high_conf['target'].mean():.4f}")

# %%
# Misclassifications
misclassified = test_df_pred[test_df_pred['correct'] == 0]
print(f"\nâŒ Misclassified Samples: {len(misclassified)}")

if len(misclassified) > 0:
    print("\nMisclassification Analysis:")
    print(f"  False Positives (predicted win, actual loss): {((misclassified['predicted_class'] == 1) & (misclassified['target'] == 0)).sum()}")
    print(f"  False Negatives (predicted loss, actual win): {((misclassified['predicted_class'] == 0) & (misclassified['target'] == 1)).sum()}")

# %% [markdown]
# ## 11. Save Model for Production

# %%
print("="*60)
print("ğŸ’¾ SAVING MODEL FOR PRODUCTION")
print("="*60)

import os
os.makedirs('models', exist_ok=True)

# Save model
model_path = 'models/xgboost_signal_classifier.pkl'
joblib.dump(model, model_path)
print(f"âœ… Model saved: {model_path}")

# Save label encoders
encoders_path = 'models/label_encoders.pkl'
joblib.dump(label_encoders, encoders_path)
print(f"âœ… Label encoders saved: {encoders_path}")

# Save feature names
features_path = 'models/feature_names.json'
with open(features_path, 'w') as f:
    json.dump(FEATURE_COLS, f, indent=2)
print(f"âœ… Feature names saved: {features_path}")

# Save metadata
metadata = {
    'model_type': 'XGBClassifier',
    'trained_at': datetime.now().isoformat(),
    'training_samples': len(X_train),
    'test_samples': len(X_test),
    'features': FEATURE_COLS,
    'performance': {
        'train_accuracy': float(train_acc),
        'test_accuracy': float(test_acc),
        'train_auc': float(train_auc),
        'test_auc': float(test_auc),
        'cv_auc_mean': float(np.mean(cv_scores)),
        'cv_auc_std': float(np.std(cv_scores))
    },
    'hyperparameters': {
        'max_depth': 6,
        'learning_rate': 0.1,
        'n_estimators': 200,
        'scale_pos_weight': float(scale_pos_weight)
    }
}

metadata_path = 'models/model_metadata.json'
with open(metadata_path, 'w') as f:
    json.dump(metadata, f, indent=2)
print(f"âœ… Metadata saved: {metadata_path}")

print("\n" + "="*60)
print("âœ… MODEL READY FOR PRODUCTION DEPLOYMENT!")
print("="*60)

# %% [markdown]
# ## 12. Production Integration Code

# %%
print("""
To integrate this model into your collector.py:

1. Copy the MLSignalFilter class from the integration guide
2. Set in your .env:
   ML_MODEL_PATH=models/xgboost_signal_classifier.pkl
   ML_MIN_CONFIDENCE=0.7

3. The filter will automatically:
   - Load the model
   - Extract features from snapshots
   - Predict win probability
   - Reject low-confidence signals

4. Monitor performance:
   - Track ML-approved vs rejected signals
   - Compare win rates
   - Retrain weekly with new data
""")

# %% [markdown]
# ## Final Summary

# %%
print("\n" + "="*80)
print("ğŸ‰ MODEL TRAINING COMPLETE - SUMMARY")
print("="*80)

print(f"\nğŸ“Š Dataset:")
print(f"  Total samples: {len(df)}")
print(f"  Training: {len(X_train)}")
print(f"  Test: {len(X_test)}")
print(f"  Features: {len(FEATURE_COLS)}")

print(f"\nğŸ¯ Performance:")
print(f"  Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"  Test AUC: {test_auc:.4f}")
print(f"  CV AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")

print(f"\nâœ… Model Status:")
if test_auc >= 0.70:
    print(f"  ğŸŸ¢ EXCELLENT - Ready for production!")
elif test_auc >= 0.65:
    print(f"  ğŸŸ¡ GOOD - Deploy with monitoring")
else:
    print(f"  ğŸ”´ NEEDS IMPROVEMENT - Collect more data")

print(f"\nğŸ“ Saved Files:")
print(f"  - {model_path}")
print(f"  - {encoders_path}")
print(f"  - {features_path}")
print(f"  - {metadata_path}")

print("\n" + "="*80)